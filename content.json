{"meta":{"title":"Yukino256のBlog","subtitle":"————我永远喜欢雪之下雪乃","description":"private records of CS study","author":"Yukino256","url":"https://Yukino256.github.io","root":"/"},"pages":[{"title":"搜索","date":"2023-04-20T08:12:14.000Z","updated":"2024-03-31T00:48:51.002Z","comments":true,"path":"search/index.html","permalink":"https://yukino256.github.io/search/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-04-20T08:10:01.000Z","updated":"2024-03-31T00:49:02.777Z","comments":true,"path":"tags/index.html","permalink":"https://yukino256.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-04-20T08:08:57.000Z","updated":"2024-03-31T00:48:37.781Z","comments":true,"path":"categories/index.html","permalink":"https://yukino256.github.io/categories/index.html","excerpt":"","text":""},{"title":"友链","date":"2024-03-28T09:28:20.000Z","updated":"2024-03-31T00:48:21.701Z","comments":true,"path":"link/index.html","permalink":"https://yukino256.github.io/link/index.html","excerpt":"","text":""},{"title":"","date":"2024-03-28T09:51:20.593Z","updated":"2024-03-28T09:51:20.593Z","comments":true,"path":"css/modify.css","permalink":"https://yukino256.github.io/css/modify.css","excerpt":"","text":"#page-header { background: transparent !important; } #page-header.post-bg, #page-header.not-home-page { height: 280px !important; } #page-header #post-info { bottom: 40px !important; text-align: center; } #page-header #page-site-info { top: 140px !important; } @media screen and (max-width: 768px) { #page-header.not-home-page { height: 200px !important; } #page-header #post-info { bottom: 10px !important; } #page-header #page-site-info { top: 100px !important; } } .top-img { height: 250px; margin: -50px -40px 50px; border-top-left-radius: inherit; border-top-right-radius: inherit; background-position: center center; background-size: cover; -webkit-transition: all 0.3s; -moz-transition: all 0.3s; -o-transition: all 0.3s; -ms-transition: all 0.3s; transition: all 0.3s; } @media screen and (max-width: 768px) { .top-img { height: 230px; margin: -36px -14px 36px; } } [data-theme='dark'] .top-img { filter: brightness(0.8); } #footer:before { background-color: rgba(255,255,255,0.5); } [data-theme='dark'] #footer:before { background-color: rgba(0,0,0,0.5); } #footer-wrap, #footer-wrap a { color: #111; -webkit-transition: unset; -moz-transition: unset; -o-transition: unset; -ms-transition: unset; transition: unset; } [data-theme='dark'] #footer-wrap, [data-theme='dark'] #footer-wrap a { color: var(--light-grey); }"}],"posts":[{"title":"girls band cry","slug":"girls-band-cry","date":"2025-09-17T15:44:35.000Z","updated":"2025-09-17T17:17:58.027Z","comments":true,"path":"2025/09/17/girls-band-cry/","link":"","permalink":"https://yukino256.github.io/2025/09/17/girls-band-cry/","excerpt":"","text":"Girls Band Cry现在是2025年9月17日的深夜，23点46分，我刚刚看完gbc的第二集。 我清楚的记得，gbc的第一集，是在去年的六月底，在黄庆龙的江汉油田隐居地看的。当时看到的感觉，也只是平平无奇，普通的感叹一下制作似乎很精良，再没有看下去的心思。大概是被找实习之类的思绪牵绊住了吧，那时的我并没有看什么番剧的欲望，基本只想看一些乐子番。这种心态是什么时候开始的呢？我在大二往后就很少看番了，高中时候那种追番的热情逐渐消磨，更是没有完整的追完一个番剧。话题扯远了，这种思绪后续再慢慢回顾梳理。 近几年火热的乐队番有很多，孤独摇滚、、gbc、mujika，影响似乎经久不衰，在上海的各个二次元场所总能看到这些番剧的周边。或许是耳濡目染之下，随波逐流的我渐渐起了一点兴趣，因而有了全补一遍的心思。第一个补的是孤独摇滚，不强求的慢慢看，大概两三周看完了。孤独摇滚是一个萌萌的好番剧，芳文社是有进步的，相比于之前纯卖萌的题材来说，孤独摇滚在可爱的同时又能整活儿，波奇大概引起了很多人在某种程度的共鸣，总体而言讲了一个好故事。记得在我最能看番的年纪，芳文社的点兔、迷路贴、new game我都是追完了的，但这些番剧都是不温不火，乃至我在二次元店看见周边都很惊喜，一下子把我拉回了八九年前，颇有感悟。 总而言之，孤独摇滚平淡而又不平淡的结束了，看完确实希望有第二季，但是没有很强烈的情感需要宣泄。在b站孤独摇滚的右侧推荐栏，第一个就是少女乐队的呐喊，没什么特别的想法，也许是看到一些周边很潇洒的“不登校”，亦或者“竖起中指”，于是选择它作为下一步看的番剧。 第一集，少女芹泽仁菜孤身一人来到东京，人生地不熟之下，第一晚没有能够成功入住出租屋，但机缘巧合下幸运的遇见了自己的偶像歌手河原木桃香，与之相识并闲谈，得知她要放弃音乐。桃香收留了仁菜一晚，并且第二天在离开东京时将吉他送给仁菜。仁菜百感交集，最终决定劝阻桃香，鼓励其继续梦想。他们在雨夜东京的街头合奏表演了《空の箱》这一仁菜最爱的歌。 第一集看完之后，让我感受最深的其实是动画中展现出来的东京街景与房屋，又勾起了一点我在东京旅游的见闻。仁菜的呐喊，以及最后的合奏，反而让我稍微有点尴尬的感觉。 然而刚刚看完的第二集，则完全逆转了我的感受。桃香决定继续之后，和仁菜合住了几天。剧集的开头，桃香清晨酒醒翻身，倒下床，因而去卫生间醒酒催吐。仁菜随之醒来，跟过去查看情况。可以看见仁菜的房间： 大概一人大小，也许是0.5m * 1.8m的床 床尾没多远的地方是一个壁挂衣柜，小小的，两扇开门，只比床宽一点；衣柜上面挂了两件衣服，一件红色外套是仁菜的，后面一件好像是蓝色的，也许是桃香的 睡觉的地方，这个床+衣柜已经基本把长度打满了；在宽度上，床右侧有一点空间，大概两个床那么大？反正也没多大 一个小圆桌，放在床边远一点的地方 床头接的是推拉落地门窗和窗帘。 床对面的墙上还有个小窗 睡觉的地方从门出去，窄窄的走道直通玄关，玄关也很小；门的右边立刻是一个冰箱 走道的中间右侧是个灶台；可以做点无火的饭 走到的中间左侧，就是卫生间。只看见了马桶和洗手台，没看见浴室在哪个方位。 真的是触景生情。在日本旅游的时候，在东京住在K桑家中，虽然比这大了点，布局也略有差异，但是总体上大差不差。仁菜住的地方，是东京边缘，5w日元一个月。这个屋子大概只有20平米？仁菜住的不好，我一下子又想起了桃香住在仁菜附近。仁菜住的是单身公寓，桃香住的就是日式一户建同租，第一集给了用力推拉老门的特写，很艰难，也体现出住的不怎么好。 生活的困难一下子扑面而来了，这种真实感尤其强烈。又想到仁菜隔壁的一家三口新婚夫妇，或许也是住这个20平的屋子；与动画里体现出的看上去挺幸福的夫妻俩形成对比，金钱的窒息感压得我喘不过气。 起床后，仁菜去报学塾，桃香陪着。过程中，仁菜说起了报学塾的金钱压力大不能一直上，说 她需要考上好大学，因为高中辍学未来会很艰难，桃香深有同感。 桃香提起仁菜的乐队安排，因为她以为仁菜要和她一起组乐队；但是发现她误会了，仁菜只是想让她不要放弃梦想，仁菜自己要努力考大学。 她们买了菜回去，用电锅煮了火锅吃，这也是桃香住的最后一晚，明天她就要搬回原来住的地方。仁菜虽然在合住的时候觉得有点拥挤有点烦，但是真走了又还是有些不舍。——我懂这种感觉仁菜第一次独居，发现有很多东西要买的没考虑到，桃香说每个人都是这么过来的。哎，深有感触，几乎要落泪了。我去年三月在北京初次实习时候住的还没桃香好：隔断的客厅房间，大概只有十平米出头，合租的卫生间也特别小。 第二天，桃香搬走了，仁菜回到了孤身一人的情况。那天晚上，她在昏黄的台灯下忍不住哭了。她每天独自上下学，无人交谈，分外孤独。她的卧室的灯从一开始来就没有，一周过去了，也没有买（虽然桃香提醒过她）。我想，这是初次独居的深深迷茫导致的。 晚上，仁菜想找桃香说说话，但是怕打扰她，犹豫之下没有主动发信息。所幸桃香从安和昴那里拿到了一个旧的灯，于是仁菜要不要。仁菜很激动，因为可以合理接触了，一直一个人实在不好受，于是快速的去桃香哪儿拿灯。、昴正好也在，桃香和昴说起过仁菜，昴表现的对仁菜很友好，好奇。桃香说已经开始打工了，没那么拮据，今晚请仁菜一起吃个饭。也许仁菜知道17岁独自出来生活的难受感受，想要为仁菜搭把手。 她们吃饭的过程中，阳角的昴让仁菜想起了被霸凌的校园生活。因为霸凌+得不到家里人支持，仁菜住院，乃至不得已退学，乃至到东京讨生活。她一方面感谢昴对于她的热情和表现出来的关照，一方面又讨厌阳角，觉得阳角高高在上。在这个情况下，她是别扭的。这种别扭我也有过，很多人也有过。仁菜的别扭让她和关心她的桃香吵了一架，她中途离场回家了。回家的路上，她很后悔，这是别扭的后遗症，她放生大哭，大声发泄。回到家，她蜷缩在地板上，默默流泪。但是幸好，桃香和昴理解她的感受，到她家来关心她，可喜可贺，她们决定一起组乐队。 大火都被生活所折磨，所以抱团取暖显得尤为珍惜、可贵、感人。 思来想去，尤其感叹生活的无力与困难，在动画中刻画的相当真实，让人感同身受。 我为什么现在有这种强烈的感受？也许是现在工作的疲劳、住的老破小；也许是东京旅游的见闻和与k桑多次的聊天谈心？我暂时说不清楚。记得china joy的两个姑娘组成的乐队iris mondo，是否也是抱团取暖呢？ 希望gbc接下来能继续吸引我。虽说感触有些痛苦，但心灵上好像略微宁静了下来。","categories":[],"tags":[]},{"title":"DPO, PPO, GRPO","slug":"DPO-PPO-GRPO","date":"2025-02-23T06:06:17.000Z","updated":"2025-02-24T16:55:48.488Z","comments":true,"path":"2025/02/23/DPO-PPO-GRPO/","link":"","permalink":"https://yukino256.github.io/2025/02/23/DPO-PPO-GRPO/","excerpt":"","text":"在小米实习的第二个任务，是对VLM进行R1类似路线的复现，查看其能否实现VLM的推理能力增强。因此，在此记录一下我在尝试理解RL、DPO、PPO、GRPO的路径 RLHF参考强化学习小白入门与概念辨析PPO源码理解PPO中GAE的推导RLHF的流程与公式代码参考公式难以理解的话，看看代码还挺有用的，感觉代码对我来说更好理解一点 dpo局限性样本全面性假设参考该分析文章评论中的一段话： 1论文附录第一个证明里面reward用policy表达有个暗含的前提是对y求期望也就是要把y的样本采集全了，然后配分和赋予reward才是有效的* 这个直接影响了数学公式转换中等价条件的成立， 因而尤其重要但实际情况下，这个假设成立比较困难，又回到了数据收集的困难上来。当然，数据不太全面，有时候问题也不大，只是会导致训练效果差一点。（这就是黑箱模型训练的玄学吗，全凭实践！） 此外，与PPO算法相比: 1如果两个方法的训练数据都是足够多的，那么他们能训练到的最优的模型是一样的。但是从 rl 的角度，dpo 是离线方法，ppo 是在线方法，在线方法类似于生产了更多的数据。 RL方法中的并行策略与显存分配跑DPO和PPO、GRPO这种强化学习方法的时候，容易遇到OOM的问题。在Pretrain和SFT阶段，我脑海中能很轻易地理解DP、PP、TP等一些概念。但涉及到多个模型，我就捉襟见肘了。究竟是模型分布在不同的卡组上呢，还是一张卡上有混合的模型部分呢？Deepseek对此的答案是： 123456789101112131415161718192021222324在强化学习（如DPO、PPO、GRPO）中，处理多个模型时容易遇到OOM问题，主要原因是这些方法通常需要同时加载多个模型（如策略模型、参考模型、价值模型等），显存需求较大。以下是一些关键点：### 1. 模型分布方式- **单卡混合模型**：在一张卡上同时加载多个模型的部分组件，显存需求高，容易OOM。- **多卡分布模型**：将不同模型分布到不同卡上，减少单卡显存压力，但需要跨卡通信。### 2. 并行策略- **数据并行（DP）**：每张卡加载完整模型，处理不同数据批次，适用于单模型场景。- **模型并行（MP）**：将单个模型拆分到多张卡上，适用于大模型。- **流水线并行（PP）**：将模型按层拆分到不同卡上，减少单卡显存占用。- **张量并行（TP）**：将模型参数拆分到多张卡上，适用于大模型。### 3. 解决OOM的方法- **混合并行**：结合DP、MP、PP、TP，优化显存使用。- **梯度累积**：通过累积小批次梯度模拟大批次训练，减少显存需求。- **卸载技术**：将部分计算或存储移至CPU或磁盘，减少显存占用。- **分布式训练**：使用多节点分布式训练，进一步扩展资源。### 4. 具体实施- **多卡分布模型**：将策略模型、参考模型、价值模型分布到不同卡上，减少单卡显存压力。- **混合并行**：对单个大模型使用MP、PP、TP，多个模型使用DP。### 总结通过合理分布模型和混合并行策略，可以有效缓解OOM问题。具体选择哪种策略需根据模型大小和硬件条件调整。 仍然没有解决我的疑问。 我想，可能是accelerator、deepspeed的并行方法我没有理解过，对我来说是一个黑箱。 GRPO中的reward如何写openR1的官方参考","categories":[{"name":"大模型， 强化学习","slug":"大模型，-强化学习","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/tags/python/"},{"name":"LLM， RL","slug":"LLM，-RL","permalink":"https://yukino256.github.io/tags/LLM%EF%BC%8C-RL/"}]},{"title":"Mi老鼠：小米实习期间摸鱼看其他知识","slug":"Mi老鼠：小米实习期间摸鱼看其他知识","date":"2025-02-22T06:41:49.000Z","updated":"2025-02-24T17:00:45.172Z","comments":true,"path":"2025/02/22/Mi老鼠：小米实习期间摸鱼看其他知识/","link":"","permalink":"https://yukino256.github.io/2025/02/22/Mi%E8%80%81%E9%BC%A0%EF%BC%9A%E5%B0%8F%E7%B1%B3%E5%AE%9E%E4%B9%A0%E6%9C%9F%E9%97%B4%E6%91%B8%E9%B1%BC%E7%9C%8B%E5%85%B6%E4%BB%96%E7%9F%A5%E8%AF%86/","excerpt":"","text":"大模型基本功ybq说的挺好的 神中神代码gpt-fast面面俱到，我只能说写的太好了！甚至包含tensor parallel但需要注意的是，transformer部分写法似乎只支持推理，因为强制生成kv_cache的占位空间才能正常运行，在训练中这是不必要的。但改起来似乎也很方便，理论上把这行注释掉就行？ 采样解析（greedy， beamsearch， topK， topP， temperature， 联合策略）https://hengsblog.top/2023/11/01/decoder/温度只控制图像的陡峭程度，不改变值的相对大小（当然由于精度问题，一些值被截断后可能会和其他值相等）温度会使得绝对大小的差值改变。那么，在最后输出的备选词A B C中选择一个，由于温度改变了原有的概率绝对大小，在备选词进行概率重新归一化然后选择这一步，选择token的概率完全改变了因为最后选择的代码大概是这么个思想： 1234567891011probs = &#123; &#x27;A&#x27;: 0.5, &#x27;B&#x27;: 0.4, &#x27;C&#x27;: 0.1 &#125;tgt_token = Nonefor token in probs: if random.rand(0, 1) &lt;= probs[token]: tgt_token = tokenif not tgt_token: tgt_token = probs.keys()[-1]5 激活函数relu 12def relu(x): return np.maximum(0, x) swish&#x2F;SiLU 12def swish(x, beta): return x / (1 + np.exp(-beta*x)) SwiGLU$$\\text{SwigLU}(x) &#x3D; \\text{Swish}(x) \\cdot \\text{GLU}(x)$$可以参见：https://blog.csdn.net/yjw123456/article/details/138441972SwiGLU通常与FFN联合起来用，因为FFN有up proj和down proj的操作 123456789101112131415161718import torchfrom torch import nnimport torch.nn.functional as Fclass FeedForward(nn.Module): def __init__(self, hidden_size: int, intermediate_size: int) -&gt; None: super().__init__() self.w1 = nn.Linear(hidden_size, intermediate_size, bias=False) self.w2 = nn.Linear(intermediate_size, hidden_size, bias=False) self.w3 = nn.Linear(hidden_size, intermediate_size, bias=False) def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # x: (batch_size, seq_len, hidden_size) # w1(x) -&gt; (batch_size, seq_len, intermediate_size) # w3(x) -&gt; (batch_size, seq_len, intermediate_size) # w2(*) -&gt; (batch_size, seq_len, hidden_size) return self.w2(F.silu(self.w1(x)) * self.w3(x)) 为什么这里使用的是silu激活函数？ 因为SiLU其实就是beta为1时的Swish激活函数w1和w3都是up proj，实际上实现的路径是：x通过w1升维——通过silu转化成门控概率——x通过w3升维——和门控概率相乘这个路径得到的是通常ffn中，up proj之后直接用relu之类的结果之后通过w2降维，这和relu的ffn差不多 SVM核函数中，高斯核&#x2F;RBF核为何可以将原样本映射到无限的高维空间中去？https://www.cnblogs.com/jiading/p/11695870.html 核函数：直接求两点在某个高维空间的距离，而无需先执行映射函数将点映射成高维点、再通过距离算法计算高维点的距离 径向基核函数（Radial Basis Function, RBF）是一种广泛用于机器学习和支持向量机（SVM）的核函数。RBF 核函数之所以能够映射到无限维度的空间，主要原因如下： 特征映射的本质： RBF 核函数是通过计算两个数据点在高维空间中的相似性来工作的。对于任意两个输入向量 ( x ) 和 ( x’ )，RBF 核函数的形式为： $$ K(x, x’) &#x3D; \\exp\\left(-\\gamma | x - x’ |^2 \\right) $$ 其中 ( \\gamma ) 是一个参数，控制 RBF 核的宽度。这个核函数实际上是用无穷多的高维特征来表示输入向量之间的相似性。 无穷维度的特征空间： RBF 核函数的特征映射可以被认为是将输入数据投影到一个无穷维的特征空间中。这意味着通过这种映射，两个原始空间中的点在高维空间中的内积将变得更加可分。这种高维映射增加了数据点的线性可分性，尽管原始数据在低维空间中可能是线性不可分的。 实际上，径向基核函数只是在求两个点的距离为什么SVM用核函数呢？因为它是要做到一批点可以分界，那么这个边界怎么确定？就是用已有的点来做。距离与hinge lossok我们得到一个边界点，及其确立的一个边界&#x2F;超平面然后，这个&#x2F;几个点就作为RBF的两个向量中固定的一个，来给其他点、以及后来所有点做分类 感觉有点类似于聚类了这样说……. 为什么说将原样本映射到无限的高维空间呢，因为RBF核只是计算最终的高维映射的距离，而不关心高维映射的结果。这个距离函数尽管表现的很简单，但是实际上可能是由点原本特征的各种组合决定的。点的特征、点的数量都会影响这个映射维度，从而反映到映射距离上。 Qwen2-VL训练策略在推理阶段，所有图和视频被打包为一个序列模型结构：ViT + LLMViT是将图片分成patchs，然后铺平，然后经过MLP映射为Token，然后加入位置编码 首先只训练ViT，然后ViT+LLM，然后最后只训练LLM延续 Qwen-VL，Qwen2-VL 也采用了 3-stage 的训练过程：ViT训练——全参数训练——LLM 指令微调。 LLM使用Qwen2初始化Vision encoder使用基于DFN数据集训练的ViT模型的encoder 同样是optimizer，deepspeed自定义的似乎比trainer的占用更多显存记录适配Qwen2VL TRL PPO时候的一些BUGgeneration config这个设置，我在inference_test.py这个文件中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from transformers import Qwen2VLForConditionalGeneration, GenerationConfig, AutoProcessorfrom qwen_vl_utils import process_vision_infoimport torchmodel_dir = &#x27;./Qwen2VL/Qwen2-VL-2B-Instruct&#x27;model = Qwen2VLForConditionalGeneration.from_pretrained( model_dir, torch_dtype=torch.bfloat16, attn_implementation=&quot;flash_attention_2&quot;, device_map=&quot;cuda&quot;, # 这里改成cuda，flash attn会报错)processor = AutoProcessor.from_pretrained(model_dir)messages = [ &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [ &#123;&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: &quot;./Qwen2VL/data/test1.jpg&quot;&#125;, &#123;&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: &quot;./Qwen2VL/data/test2.jpg&quot;&#125;, &#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;网友第一次按摩，问这是不是正规的操作？[允悲]&quot;&#125;, &#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;之前提供的图文的内容是一个微博博文，请分析这条博文的内容、情感等信息&quot;&#125; ], &#125;]text = processor.apply_chat_template( messages, tokenize=False, add_generation_prompt=True)image_inputs, video_inputs = process_vision_info(messages)inputs = processor( text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=&quot;pt&quot;,)inputs = inputs.to(&quot;cuda&quot;)generation_config = GenerationConfig( max_new_tokens=512, temperature=1.0, top_k=0.0, top_p=1.0, do_sample=True, return_dict_in_generate=True, output_scores=True, output_attentions = False, output_hidden_states = False )print(inputs)generated_ids = model.generate(**inputs, generation_config=generation_config)generated_ids_trimmed = [ out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids.sequences)]output_text = processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)print(output_text) 这样设置，可以输出正常的decode结果。但是在trainer内部，定义这个同样的config，进行batch decode的时候，却遇到了decode一直到max token才终止的问题。也就是说，generate没有正确地运行。直接查看Qwen2VL的generation config： 1234567891011121314&#123; &quot;bos_token_id&quot;: 151643, &quot;pad_token_id&quot;: 151643, &quot;do_sample&quot;: true, &quot;eos_token_id&quot;: [ 151645, 151643 ], &quot;repetition_penalty&quot;: 1.0, &quot;temperature&quot;: 0.01, &quot;top_p&quot;: 0.001, &quot;top_k&quot;: 1, &quot;transformers_version&quot;: &quot;4.37.0&quot; &#125; 此处指定了终止符、pad符等一系列generate的设置直接将这个设置中的参数加入到trainer内config的里面，运行生成，这下结果正常了。 原因尚未知晓。目前看来，两处代码的主要区别在于： inference_test.py文件导入了qwen-vl-utils这个官方包 合理怀疑，是导入的过程中，这个package有一份generation config，自动初始化了？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586def generate( lm_backbone: torch.nn.Module, one_inputs, pad_token_id: int, generation_config: GenerationConfig) -&gt; tuple[torch.Tensor, torch.Tensor]: for key in one_inputs: if key == &#x27;input_ids&#x27; or key == &#x27;image_grid_thw&#x27; or key == &#x27;attention_mask&#x27;: one_inputs[key] = torch.LongTensor(one_inputs[key]).to(&#x27;cuda&#x27;) else: one_inputs[key] = torch.Tensor(one_inputs[key]).to(&#x27;cuda&#x27;) input_shape = one_inputs[&#x27;input_ids&#x27;].shape context_length = input_shape[1] attention_mask = one_inputs[&#x27;input_ids&#x27;] != pad_token_id one_inputs[&#x27;input_ids&#x27;] = torch.masked_fill(one_inputs[&#x27;input_ids&#x27;], ~attention_mask, 0) output = lm_backbone.generate( **one_inputs, generation_config=generation_config, ) logits = torch.stack(output.scores, 1) return torch.cat((one_inputs[&#x27;input_ids&#x27;], output.sequences[:, context_length:]), dim=1), logits, context_lengthdef pad(tensors: list[torch.Tensor], padding_value: int = 151643, padding_side: str = &quot;right&quot;) -&gt; torch.Tensor: output_shape = np.max([t.shape for t in tensors], 0).tolist() output = torch.full((len(tensors), *output_shape), padding_value, dtype=tensors[0].dtype, device=tensors[0].device) for i, t in enumerate(tensors): if padding_side == &quot;left&quot;: seq_slice = slice(output_shape[0] - t.shape[0], output_shape[0]) elif padding_side == &quot;right&quot;: seq_slice = slice(0, t.shape[0]) else: raise ValueError(&quot;padding_side must be &#x27;left&#x27; or &#x27;right&#x27;&quot;) slices = (seq_slice,) + tuple(slice(0, s) for s in t.shape[1:]) output[i][slices] = t return output@torch.no_grad()def batch_generation( model: torch.nn.Module, inputs: torch.Tensor, local_rollout_forward_batch_size: int, pad_token_id: int, generation_config: GenerationConfig,): query_responses = [] logitss = [] prompt_length = [] batch_size = len(inputs) for i in range(0, batch_size): micro_inputs = inputs[i] query_response, logits, context_length = generate( model, micro_inputs, pad_token_id, generation_config, ) query_responses.append(query_response) logitss.append(logits) prompt_length.append(context_length) padded_query_responses = pad(query_responses, padding_value=pad_token_id, padding_side=&quot;right&quot;) padded_logitss = pad(logitss, padding_value=0, padding_side=&quot;right&quot;) tmp_1 = padded_query_responses.shape tmp_2 = padded_logitss.shape padded_query_responses = padded_query_responses.view(-1, padded_query_responses.shape[-1])[:batch_size] padded_logitss = padded_logitss.view(-1, *padded_logitss.shape[2:])[:batch_size] return padded_query_responses, padded_logitss, prompt_lengthwith unwrap_model_for_generation(model, self.accelerator) as unwrapped_model: # print query_responses, logitss, prompt_length = batch_generation( unwrapped_model.policy, data, args.local_rollout_forward_batch_size, pad_token_id=151643, generation_config=generation_config, )for i in range(0, len(data)): context_length = prompt_length[i] query_response = query_responses[i] response = query_response[context_length:] logits = logitss[i] all_logprob = F.log_softmax(logits, dim=-1) logprob = torch.gather(all_logprob, 1, response.unsqueeze(-1)).squeeze(-1) attention mask 输出的shape是1 * seq len，在输入到模型之后，会被一个方法转化成1 * seq len * seq len同时，一个样本输入到模型中，经过qkv的计算，得到的attn weight的shape是1 * seq len * seq len正好加上mask，然后 * V得到attn output的shape则是1 * seq len * hidden dim也就是说，一个样本样本确实需要用到seq len * seq len的attn mask","categories":[{"name":"大模型， 基础知识","slug":"大模型，-基础知识","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/tags/python/"},{"name":"LLM， 基础","slug":"LLM，-基础","permalink":"https://yukino256.github.io/tags/LLM%EF%BC%8C-%E5%9F%BA%E7%A1%80/"}]},{"title":"LLM中的并行策略————如何理解与理解路径记录","slug":"train-parallelism-understanding","date":"2025-01-10T06:05:43.000Z","updated":"2025-02-24T17:00:17.689Z","comments":true,"path":"2025/01/10/train-parallelism-understanding/","link":"","permalink":"https://yukino256.github.io/2025/01/10/train-parallelism-understanding/","excerpt":"","text":"在小米实习接到的第一个工作，是对MiLM进行长文本SFT。具体而言，部门内部的SFT代码之前并没有适配TP（tensor parallelism）， 13B、30B模型训练单卡最多防线22K个token，长文本训练使用PoSE来增强，实际上还是不完全的阉割版。在我来的时候，TP的框架已经被前一个实习生适配的差不多了，因而我基于此开始进行后续的训练，以及代码理解、代码调整与PR等TP的概念图还比较容易理解，但是具体代码，影响到方方面面，需要在多处进行修改，我还是花了很大的功夫去进行理解，在此记录自己的路径与一些思考 TP与SP是什么，如何实现？张量并行与序列并行细节分析 TP+SP流程：参考：Xtuner 32k数据——序列并行环境初始化（GPU分组，哪些GPU共同处理一个32k）——seq_len切分——embedding层——修改本来的mask、position id， label， input id——切分后的数据输入到attn中——q,k,v proj——all to all算子，使得qkv从seq切分转化为head切分——attn weight（qk）、softmax、attn_output计算（可能要用到flash，则定义lockmask_flash_attention_forward方法）——all to all，将原来的attn output从head切分转化成seq_len切分——attn output的transpose、reshape、o_proj——传入MiFeedForward，因为这个矩阵乘法结果等价，所有feedforward的序列并行也顺带着进行了——巴拉巴拉——进入下一个模块 （为什么关于切分之后norm的问题，因为norm是针对hidden dim的，所以每次pre norm时候由于都处于seq切分状态，所以norm不影响） 修改模型还需要注意细节，即dataloader加载序列并行数据的逻辑和原本的逻辑是完全不一样的根源在于dataloader的_inner_training_loop这个函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from typing import Iterator, Optionalimport mathimport torchfrom torch.utils.data import Sampler, Datasetfrom .utils import ( get_group_num, get_group_index,)class ParallelGroupSampler(Sampler): def __init__(self, dataset: Dataset, group_num: Optional[int] = None, group_index: Optional[int] = None, shuffle: bool = True, seed: int = 42, drop_last: bool = False) -&gt; None: super().__init__(dataset) if group_num is None: group_num = get_group_num() if group_index is None: group_index = get_group_index() self.dataset = dataset self.group_num = group_num #### 此处！！！！！！！！！！ self.group_index = group_index #### 此处！！！！！！！！！！ self.epoch = 0 self.drop_last = drop_last if self.drop_last and len(self.dataset) % self.group_num != 0: # type: ignore[arg-type] self.num_samples = len(self.dataset) // self.group_num # type: ignore[arg-type] else: self.num_samples = math.ceil(len(self.dataset) / self.group_num) # type: ignore[arg-type] self.total_size = self.num_samples * self.group_num self.shuffle = shuffle self.seed = seed def __iter__(self) -&gt; Iterator: if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) indices = torch.randperm(len(self.dataset), generator=g).tolist() # type: ignore[arg-type] else: indices = list(range(len(self.dataset))) # type: ignore[arg-type] if not self.drop_last: # add extra samples to make it evenly divisible padding_size = self.total_size - len(indices) if padding_size &lt;= len(indices): indices += indices[:padding_size] else: indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size] else: # remove tail of data to make it evenly divisible. indices = indices[:self.total_size] assert len(indices) == self.total_size # subsample indices = indices[self.group_index:self.total_size:self.group_num] #### 此处！！！！！！！！！！ assert len(indices) == self.num_samples return iter(indices) def __len__(self) -&gt; int: return self.num_samples def set_epoch(self, epoch: int) -&gt; None: self.epoch = epoch 理解这个为什么要这么写，需要顺序参考以下开源代码：Sampler1Sampler2distributed env 然后，这个代码替换的是transformer源代码中trainer的一部分： Deepspeed-Zero入门官方文档 也可稍微看一下知乎解析 并行的不止模型本身 解决了模型状态，再来看剩余状态，也就是激活值（activation）、临时缓冲区（buffer）以及显存碎片（fragmentation）。 激活值同样使用分片方法，并且配合checkpointing 模型训练过程中经常会创建一些大小不等的临时缓冲区，比如对梯度进行AllReduce啥的，解决办法就是预先创建一个固定的缓冲区，训练过程中不再动态创建，如果要传输的数据较小，则多组数据bucket后再一次性传输，提高效率 显存出现碎片的一大原因是时候gradient checkpointing后，不断地创建和销毁那些不保存的激活值，解决方法是预先分配一块连续的显存，将常驻显存的模型状态和checkpointed activation存在里面，剩余显存用于动态创建和销毁discarded activation Deepspeed Zero3（最~~~重要的一集）DeepSpeed ZeroOffload 12345678910111213141516171819202122232425262728293031323334353637383940414243for_parallel rank in range(world_size): initialize_layers() for batch in dataset: x = forward(batch) compute_loss(x,batch).backward() backward(x.grad) step()def _is_owner(i): return True if rank owns i else Falsedef initialize_layers(): for i in range(num_layers): l = layers[i] allocate_on_gpu l.param_fp16 if _is_owner(i): allocate_on_cpu l.param_fp32 allocate_on_cpu l.optim_states_fp32 allocate_on_cpu l.cpu_graddef forward(x): for i in range(num_layers): x = layers[i].forward(x) return xdef backward(dx): for i in range(num_layers, 0, -1): dx=layers[i].backward(dx) reduce(layers[i].grad, dest_rank = _owner_rank(i)) if _is_owner(i) l.cpu_grad.copy(l.grad) else pass del layers[i].graddef step(): for i in range(num_layers): l=layers[i] if _is_owner(i): update_in_cpu(l.optim_states_fp32, l.cpu_grad, l.param_fp32) l.param_fp16.copy(l.param_fp32) BROADCAST(l.param_fp16, src=_owner_rank(i)) 想要理解为什么能够将optimizer states从GPU传到CPU上计算 反向传播过程，会获得当前layer的参数的梯度，最简单的参数更新来说，直接原参数 - lr*grad即可更新然后，当前获得的梯度，也会传递到前一层，在前一层反向传播的过程中作为链式法则的一部分那么，为什么offload中可以直接删除GPU上的grad呢？前一层不也要用吗？删除了之后如何传递？似乎，是在backward中： 12for i in range(num_layers, 0, -1): dx=layers[i].backward(dx) 此处即完成了梯度传递给前一层。实际上，获取上一层、计算当前层、传递给下一层，都集成在了这一个里面。","categories":[{"name":"大模型， 并行策略","slug":"大模型，-并行策略","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C-%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"}],"tags":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/tags/python/"},{"name":"LLM， parallelism","slug":"LLM，-parallelism","permalink":"https://yukino256.github.io/tags/LLM%EF%BC%8C-parallelism/"}]},{"title":"Mi孝子：实习期间“偷”到的一些小知识","slug":"Mi孝子：实习期间“偷”到的一些小知识","date":"2024-11-26T13:06:32.000Z","updated":"2024-11-26T13:10:45.245Z","comments":true,"path":"2024/11/26/Mi孝子：实习期间“偷”到的一些小知识/","link":"","permalink":"https://yukino256.github.io/2024/11/26/Mi%E5%AD%9D%E5%AD%90%EF%BC%9A%E5%AE%9E%E4%B9%A0%E6%9C%9F%E9%97%B4%E2%80%9C%E5%81%B7%E2%80%9D%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E7%9F%A5%E8%AF%86/","excerpt":"","text":"近期获得&#x2F;加深的一些知识理解GPT结构、训练与推理分析图解GPT理解prefill阶段是生成kv cache以及用最后一个token对之前的kv进行计算得到一个概率，后续token同理。vllm就是对各个模型进行推理上的重构，而本身的modeling_qwen2.py这类其实是训练用的文件。推理阶段还需要重写。实际上公司内部模型部署vllm就是转化成qwen格式然后调用vllm的qwen接口…… bash脚本文件语法写sh脚本的时候，’&#x3D;’前后不能有空格，与本人python编程习惯背道而驰….. python小知识点——import使用 12import sys sys.path.insert(0, sys.path[0] + &quot;/../&quot;) 进行导入的时候，需要注意的是，这个添加的路径是当前工作目录的相对路径，而不是这个文件的相对路径。也就是说，如果一个处于该文件父目录的文件调用了该文件，那么该文件的这个代码就会被解释成“将父目录的父目录加入path”，与预期的“将父目录加入path”不符合导入模块应该采用以下写法，即获取当前文件的父目录，然后操作，如sys.path.insert(0, parent_dir) 1234567891011121314151617import osimport sysimport importlib.util# 获取当前文件的目录current_dir = os.path.dirname(os.path.abspath(__file__))parent_dir = os.path.dirname(current_dir)# 构造tokenization_milm3.py的绝对路径#module_path = os.path.join(parent_dir, &quot;test.py&quot;)# 获取模块名和spec#module_name = &quot;test&quot;#module_spec = importlib.util.spec_from_file_location(module_name, module_path)# 加载并执行模块#module = importlib.util.module_from_spec(module_spec)#module_spec.loader.exec_module(module) 这样写，相对导入是确定了的。关于import，其中’.’要不要加，在实际使用的时候还要斟酌，一般来说module内部的相互import应该加，而module外部import module内部的内容不应该加。也就是说，只有在该项目&#x2F;文件夹 被导入时，项目&#x2F;文件夹内部使用.来相对导入，才不会报错。否则，如果在内部直接使用.或..来import，则跑不通。通常来说，module内部文件组织大可以直接import，只是在最后对外的整合接口里面，需要用相对导入？needlebench的代码例子中，对于每种length，生成三类数据集，最终该长度的数据集对外的总接口文件如下： 12345678910from .needlebench_multi_reasoning_4k import get_datasets as get_datasets_mul_reasoningfrom .needlebench_single_4k import get_datasets as get_datasets_single_needlefrom .needlebench_multi_retrieval_4k import get_datasets as get_datasets_mul_needlesdef get_datasets(tokenizer_path): res = [] res.extend(get_datasets_single_needle(tokenizer_path)) res.extend(get_datasets_mul_needles(tokenizer_path)) res.extend(get_datasets_mul_reasoning(tokenizer_path)) return res 这里相对导入就是必须的，不然父目录import会报错 qwen2的代码里面没有yarn，是否表面qwen2只是在rope和dynamic rope的基础上实现了128k？qwen2的代码，有32层，前28层是full attention， 后4层是4096窗长的sliding window attention，最后一层的感受场大小为4 * 4096，也就是16k？实际上，代码中是，如果设置使用swa，那么就在28层之后使用，否则就是全部层都全注意力。当然，这只是transformer里面的基础设置。qwen2.5各个模型有各自的config，都是不同的。观看2.5-32b的config，SWA基本上已经弃用了。qwen2.5-32b本身最大位置编码已经到了131072，即4096*32， 然后rope的base也乘100变成了1000000，正好和苏神博客中提到的，NTK-scaling与yarn对做大规模预训练的公司来说没啥区别，都可以获得更好的初始模型长文本效果，而且前者更方便实现。 vllm推理加速或许可以参见https://github.com/vllm-project/vllm/blob/main/vllm/attention/backends/xformers.py#L392通过对prefill阶段进行并行计算来加速TTFT。这个阶段类似于训练过程，prompt加入attn mask然后进行attn的计算操作，在模型的每一层，都会存储对应的kv；和训练过程计算流程一致，除了没有LM head（因为LH head是在最上层CasualPretrainedModel加入的，其下逐层是：DecoderLayer-Decoder-Attention,训练过程中与输入shape相同的N*d维度的tensor最终会经过LM Head映射为N * V的tensor，V是词表大小，之后根据进行BCE loss的计算），这就是为什么prefill阶段是计算密集型的原因。当然不是说显存占用不大，只是和decode阶段比起来，计算量是尤其巨大的。（感觉还有点不明白的地方，如具体计算的细节，但后面再说吧） 序列并行代码（序列并行的概念在不同框架里面可能并不一致）xtuner里面的序列并行（sequence parallel），好像就是megatron里面的 Context Parallel并行；megatron里面的sequence parallel似乎没对attention进行切分并行，只在CP里面做了更进一步的实现关于megatron的细节，这个博客中有一系列的文章，或许可供参考 知乎干货up主 真中合欢爱好者(发布的文章目前看来都挺有价值，LLM相关回答也不错) CUDA算子理解本人是NLP搬砖工，并不专门研究CUDA算子。我看了几个回答没看见特别通俗易懂的，所以我想尝试以一个外行人的身份给其他外行人解释。所以我主要注重懂，而不是准。在CUDA生态的最低层就是硬件，也就是显卡里面的计算单元、显存、缓存、总线、控制器等等。计算单元负责数值和逻辑运算，缓存负责存储计算前的数据和计算结果，显存和缓存的作用相同，但是更大更慢。总线负责在各个部件之间传递数据，控制器负责调度。在硬件之上的是驱动。显卡插在电脑上，你的电脑应该如何访问显卡，如何传输数据，怎么获取显卡的结果是由驱动定义的。驱动再之上是算子，程序员通常会以c++语言来写一些计算程序，比如一个数加上另一个数，一个矩阵加上另一个矩阵等等，这些都是具体的程序。这些程序在程序员写好后，会被编译软件编译成汇编代码，然后再编译成机器码，保存下来。在cpu编程里，这样的一个逻辑简单、能完成某一种特定运算的功能通常被称为函数或简单程序，在gpu编程中称为算子。比如完成两个矩阵相加功能的称为加法算子，完成乘法的叫乘法算子。当然还有执行非常复杂功能的算子，比如flash attention算子，实现了整个self attention功能。当你需要某个功能而去运行某个算子时，实际就是将算子的机器码、待运算的数据通过驱动传递给显卡，结果再通过驱动返回。算子再往上就是机器学习框架，比如torch、tensorflow，或者其他框架。它们对一系列算子进行整合，实现“数学中函数级运算”的功能。最简单的矩阵加减乘除就不说了，比如要计算一个多元一次方程，只需要调用torch框架中的linear函数:y&#x3D;torch.nn.Linear(40,1)(torch.randn(1,40)) 上面这一行torch代码就是计算一个40元一次方程。这背后涉及了矩阵的乘法和加法，需要调用乘法和加法算子，torch这个框架在背后帮我们调用了。那么在整个过程中cuda体现在哪里呢？首先我们用c++写的这个程序，就叫cuda程序，通常是以.cu结尾。然后把这个程序编译为机器码的过程要用到cuda编译器。为什么我们要用cuda算子完成一个功能，而不是cpu程序呢？这通常是为了利用gpu高并发的特性。在cpu编程里，大部分情况我们写的都是单线程程序，如果涉及到高性能计算或IO，一般会并发多线程，这个线程数量少的2-3个，多的也就30-40个，撑死了也就100，一般和cpu的核心数一个量级，因为每个cpu核心同一时间只能处理一个线程，线程太多了相互抢占反而造成性能下降。就算有超线程，也不会超太多。但是在gpu里，我们为了完成一次计算，会启动上万甚至百万个线程。那么能实现如此高的并发，gpu肯定要设计一套不同于一般cpu的特殊架构和硬件，使用特别设计的计算单元、缓存、显存、总线、控制器，完成大量线程并发调度、大规模数据通信、大规模简单运算。这一套架构也称为cuda架构。所以总的来说，cuda就是英伟达为了大规模计算设计的一整套软硬件体系。也就是其他答主所解释的“平台”这一个概念。 拒绝采样 ……..剩下的不放这儿了 ybq——真中合欢爱好者的同公司饭搭子 训练模型小trick 只用打榜数据退火，用大量数据微调，模型效果会变差 “小初始化、大权重衰减、适中学习率” 的组合能更好学习推理解，因为更可能促使模型通过低复杂度的优化路径逐步逼近目标函数，从而更好地理解数据中的组合性结构; (Paper Link:Implicit Regularization of Dropout) 同一作者的 tpami 讨论了 dropout 对凝聚的作用：“研究发现Dropout可以在全训练过程中促使神经网络趋于参数凝聚，并且不要求小初始化，因此，在保持好泛化性的同时也不会遭受由于小初始化带来的训练慢的问题。其次，该研究发现并验证了使用Dropout训练的神经网络与标准梯度下降训练相比，具有更平坦的最小值，而该团队发现的隐式正则化正是训练可以找到平坦解的关键。” 对单一任务数据集的选择，通常认为为避免过拟合的影响应当选择第一个高点，但是从实验来看，选择步数较深也有一定的价值。 任务重复度的关系： 4b-24l的实验表明，平均时不同ckpt对应的训练数据集的任务交叉度越低，4b平均上的效果越好。但是这可能和4b作为小模型，参数之间的耦合度比大模型更高有关。 但是总之结论是需要在平均时增强数据集任务的分类独立度和数据集本身的质量。这与不同类别的任务推理与模型不同结构&#x2F;参数有关的自然假设符合，会带来更可预期的平均结果。","categories":[{"name":"大模型","slug":"大模型","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/tags/python/"},{"name":"bash","slug":"bash","permalink":"https://yukino256.github.io/tags/bash/"},{"name":"LLM","slug":"LLM","permalink":"https://yukino256.github.io/tags/LLM/"}]},{"title":"adamW优化器：思想、算法公式与代码","slug":"adamW优化器：思想、算法公式与代码","date":"2024-11-26T13:03:48.000Z","updated":"2024-11-26T13:08:23.201Z","comments":true,"path":"2024/11/26/adamW优化器：思想、算法公式与代码/","link":"","permalink":"https://yukino256.github.io/2024/11/26/adamW%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%9A%E6%80%9D%E6%83%B3%E3%80%81%E7%AE%97%E6%B3%95%E5%85%AC%E5%BC%8F%E4%B8%8E%E4%BB%A3%E7%A0%81/","excerpt":"","text":"首先分析一下adam的思想： 获取梯度&#x2F;导数&#x2F;一阶动量的滑动平均——m1&#x2F;滑动平均后的梯度g 获取二阶导数&#x2F;二阶动量的滑动平均——m2 其中，β1和β2就是控制一阶动量和二阶动量滑动平均的参数。通过滑动平均，可以使得m1和m2都变成前面所有时间步结果的一个调和平均之后： 根据β1和β2，以及当前的时间步t，计算一个系数，调整学习率，得到L_t 梯度下降，公式变成了： 参数 &#x3D; 参数 - L_t * m1 &#x2F; (sqrt(m2) + eps) 最后梯度下降的公式，实际上，看着花里胡哨的，但最终，m1之外的其他部分都可以看成是对学习率的调整，本质上只是让学习率更好而已。 而在原始论文及其最初的衍生实现中，还将L2正则加入到了这个优化器中，具体表现为，在每一步t开始计算优化前，将当前时间步t梯度g_t变成: 1g_t = g_t + weight_decay * L2项 然后后面就使用g_t进行m1、m2等的计算。但是，这样的问题是，L2正则直接加在梯度g_t上面，后面的一阶动量和二阶动量都使用了g_t，最后又有 **m1 &#x2F; (sqrt(m2) + eps)**这个东西，L2正则项被大大削减了。因而，L2正则就不生效了。 后来，这个问题被发现，adamW应运而生，同时adam的代码也被更改了，不再预先把L2加到梯度上。 adamW相对于adam的不同就是，其真正实现了weight decay&#x2F;L2 Norm。adam和adamW的大致算法步骤与公式参见下面这个博客：【深度学习基础】第十九课：Adam优化算法从公式中可以看出，只是在最后计算梯度更新的时候加了权重衰减。实际实现中，也很简单，adam和adamW的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243class my_opt(): def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2): self.params = params self.lr = lr self.b1 = betas[0] self.b2 = betas[1] self.eps = eps self.wd = weight_decay self.m = 0 self.v = 0 self.b1t = 1.0 self.b2t = 1.0 # 模拟AdamW的step def stepW(self): for name, param in self.params.named_parameters(): if param.grad is None: continue g = param.grad self.m = self.b1 * self.m + (1 - self.b1) * g self.v = self.b2 * self.v + (1 - self.b2) * g * g self.b1t *= self.b1 self.b2t *= self.b2 m = self.m / (1 - self.b1t) v = self.v / (1 - self.b2t) n = 1.0 param.data -= n * (self.lr * (m / (v.sqrt() + self.eps) + self.wd * param.data)) # 模拟Adam的step def step(self): for name, param in self.params.named_parameters(): if param.grad is None: continue g = param.grad self.m = self.b1 * self.m + (1 - self.b1) * g self.v = self.b2 * self.v + (1 - self.b2) * g * g self.b1t *= self.b1 self.b2t *= self.b2 m = self.m / (1 - self.b1t) v = self.v / (1 - self.b2t) n = 1.0 param.data -= n * (self.lr * m / (v.sqrt() + self.eps)) 可以看到，代码里面adamW确实也只是在最后加了衰减。那么，为什么加了这个参数本身的衰减，就是L2正则生效了呢？L2正则不是加在损失函数上面的吗？这里，首先理解L2正则是什么，怎么加上去。那么，实际上，L2正则加在损失函数上，在反向传播的时候，经过公式推导，就是直接减去参数本身的形式。 L2正则反向传播公式推导或许可以参见：L2 Normalization（L2归一化）反向传播推导","categories":[{"name":"大模型","slug":"大模型","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"adam","slug":"adam","permalink":"https://yukino256.github.io/tags/adam/"},{"name":"adamW","slug":"adamW","permalink":"https://yukino256.github.io/tags/adamW/"},{"name":"优化器","slug":"优化器","permalink":"https://yukino256.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"}]},{"title":"长文本增强的便捷技术：YaRN与PoSE代码解析","slug":"长文本增强的便捷技术：YaRN与PoSE代码解析","date":"2024-11-26T13:01:14.000Z","updated":"2024-11-26T13:10:08.399Z","comments":true,"path":"2024/11/26/长文本增强的便捷技术：YaRN与PoSE代码解析/","link":"","permalink":"https://yukino256.github.io/2024/11/26/%E9%95%BF%E6%96%87%E6%9C%AC%E5%A2%9E%E5%BC%BA%E7%9A%84%E4%BE%BF%E6%8D%B7%E6%8A%80%E6%9C%AF%EF%BC%9AYaRN%E4%B8%8EPoSE%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"首先放上苏神的两个博客： Transformer升级之路：2、博采众长的旋转式位置编码 Transformer升级之路：16、“复盘”长度外推技术 本人在进行代码解析之前，首先尽量去理解了这个理论基础，之后在解析代码的时候和论文、博客相互映照，才最终得到一个比较好的理解。其中，第二个是重中之重，需要从头到尾地理解透彻。第一个是阐述了RoPE的数学思路、设计原理。单看这个可能不太能理解RoPE，可以参照其他一些讲解RoPE的博客去理解，使用Google或者bing可以搜到一些程序员自己搭建的博客页面，而非csdn之类的，这些感觉质量都还不错。此外，一定要理解RoPE的原生代码总的来说，笔者在上面这些准备工作花不少时间，之后在理解yarn的代码上又花费了大概一天的时间，才终于产出了这篇解析源代码的博客。所获得的收获是很大的，不单单是理解了一种方法，更是对位置编码有了更深的理解。当然，这些都还是一些浅显的东西，位置编码及其本质的探讨，苏神后来也有一些博客，有兴趣的可以去看一看。 1234567891011121314151617181920212223242526272829303132333435# Inverse dim formula to find dim based on number of rotationsdef _yarn_find_correction_dim(num_rotations, dim, base=10000, max_position_embeddings=2048): return (dim * math.log(max_position_embeddings/(num_rotations * 2 * math.pi)))/(2 * math.log(base))# 从函数名来看，应该是找到yarn应用时所需要的正确的dim，这个函数的需求是num_rotations，即θi在训练窗长内所占的圈数；返回结果是一个固定公式计算得出的数值。在yarn类中，最终会传入这个函数的是beta和alpha，通常是32和1。# 函数应该是，基于hidden dim的维度，如0， 1， 2， ....511，计算这个维度需要的？# 从dim乘以一个系数的角度考虑，那么这个函数是再确定应用[不变、线性插值、缩放]的dim范围！确实是应该有两个分界点的！# Find dim range bounds based on rotationsdef _yarn_find_correction_range(low_rot, high_rot, dim, base=10000, max_position_embeddings=2048): low = math.floor(_yarn_find_correction_dim( low_rot, dim, base, max_position_embeddings)) high = math.ceil(_yarn_find_correction_dim( high_rot, dim, base, max_position_embeddings)) return max(low, 0), min(high, dim-1) # Clamp values just in case# 上面的函数计算出来的数值，用在此处计算出了low和high，两个float返回值作为该函数的结果。low的值域应该是[0, min(high, dim-1)], high的值域应该是high和dim-1两者之间的更小值。这个值很有意思，因为在确定当前的θi是要走[不变、线性插值、缩放]三个条件中哪一个时判断的依据，就是。# 由此，上第一个函数，应该是计算训练过程中转的圈数γi？其公式正好是θi * L_train / 2 * PI。拿这个公式带入最上面的函数，则：# 分子：dim * math.log(max_position_embeddings/(num_rotations * 2 * math.pi))# 分母：2 * math.log(base)# 从苏神的博客中来看，唯一用到log的地方就是scale因子的确定上。def _yarn_linear_ramp_mask(min, max, dim): if min == max: max += 0.001 # Prevent singularity linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min) ramp_func = torch.clamp(linear_func, 0, 1) return ramp_func# 上一个函数确定的范围，在此处得到了应用。linear_func先arange一个形如[0， 1， 2....， dim - 1]的tensor，shape为[1, dim],然后将其最大最小归一化。最后，将其截断在全部clamp（阶段）在min和max之间。def _yarn_get_mscale(scale=1): if scale &lt;= 1: return 1.0 return 0.1 * math.log(scale) + 1.0# 根据sacle得到缩放系数sqrt(t)，这个公式是固定的 以上是yarn需要用到的一些函数。逐个地顺序解析，大概知道了函数输入。输出是什么。至于为什么要这些输入输出，则需要到下面的主体class去看一下详细的操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235# 如果让我基于yarn的理论写代码，那么，我应该会：原生rope的θi计算。然后对计算每个hidden position的缩放系数，然后相乘。然后apply rotrayclass LlamaYaRNScaledRotaryEmbedding(torch.nn.Module): def __init__( self, dim, max_position_embeddings=2048, base=10000, scale=1, original_max_position_embeddings=2048, extrapolation_factor=1, attn_factor=1, beta_fast=32, beta_slow=1, finetuned=False, device=None, ): super().__init__() self.dim = dim self.max_position_embeddings = max_position_embeddings self.base = base self.scale = scale self.original_max_position_embeddings = original_max_position_embeddings self.extrapolation_factor = extrapolation_factor self.attn_factor = attn_factor self.beta_fast = beta_fast self.beta_slow = beta_slow self.yarn(device) # 查看下面的yarn函数解析。此函数运行后可以得到两个参数：缩放系数根号t，以及θnew # Build here to make `torch.jit.trace` work. self.max_seq_len_cached = max_position_embeddings # 在扩展模型窗长的时候，scale变化，max_position_embeddings也应该乘以相应倍数？ t = torch.arange( self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype, ) # 下面的没啥好说了，和RoPE一样不变，因为yarn只是改变θ freqs = torch.einsum(&quot;i,j-&gt;ij&quot;, t, self.inv_freq) # Different from paper, but it uses a different permutation in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1) dtype = torch.get_default_dtype() self.register_buffer( &quot;cos_cached&quot;, (emb.cos() * self.mscale)[None, None, :, :].to(dtype), persistent=False, ) self.register_buffer( &quot;sin_cached&quot;, (emb.sin() * self.mscale)[None, None, :, :].to(dtype), persistent=False, ) def forward(self, x, seq_len=None): # x: [bs, num_attention_heads, seq_len, head_size] # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case. if seq_len &gt; self.max_seq_len_cached: self.max_seq_len_cached = seq_len t = torch.arange( self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype ) freqs = torch.einsum(&quot;i,j-&gt;ij&quot;, t, self.inv_freq) # Different from paper, but it uses a different permutation in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1).to(x.device) self.register_buffer( &quot;cos_cached&quot;, (emb.cos() * self.mscale)[None, None, :, :].to(x.dtype), persistent=False, ) self.register_buffer( &quot;sin_cached&quot;, (emb.sin() * self.mscale)[None, None, :, :].to(x.dtype), persistent=False, ) # return ( # self.cos_cached[:seq_len].to(dtype=x.dtype), # self.sin_cached[:seq_len].to(dtype=x.dtype), # ) return ( self.cos_cached[:, :, :, ...].to(dtype=x.dtype), self.sin_cached[:, :, :, ...].to(dtype=x.dtype), ) def yarn(self, device): pos_freqs = self.base ** ( torch.arange(0, self.dim, 2).float().to(device) / self.dim ) inv_freq_extrapolation = 1.0 / pos_freqs # 外推 inv_freq_interpolation = 1.0 / (self.scale * pos_freqs) # 内推/内插 low, high = _yarn_find_correction_range( self.beta_fast, self.beta_slow, self.dim, self.base, self.original_max_position_embeddings, ) inv_freq_mask = ( 1 - _yarn_linear_ramp_mask(low, high, self.dim // 2).float().to(device) # 获得的是一个递减的list[1, 1, 0.9, 0.8 ..... 0], 这个其实是系数，决定你当前这个位置的position是否外推/内插/不变 ) * self.extrapolation_factor # Get n-d rotational scaling corrected for extrapolation # 此处的extrapolation_factor，可以看到默认值，是1，这个是一个总体的缩放系数，一般不需要更改 inv_freq = ( inv_freq_interpolation * (1 - inv_freq_mask) + inv_freq_extrapolation * inv_freq_mask ) # 此处的逻辑，是将任何位置的rope都看成是内插 + 外推的线性组合。看论文中的公式，这一步很好理解。如果是看苏神的博客的话。那么外推inv_freq_extrapolation就是θi，内插inv_freq_interpolation则是θi * L_train / L_test。此处用scale来直接代表，scale其实就是位置编码总体上扩展的大小。如果是8.0，则是将原来的4k窗长扩展到32k，当然后来还需要进行相应的ft以适应这种变化。 # 总的来说，这一步得到了inv_freq，也就是所有的Θnew被确定下来了 # # self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False) self.mscale = float( _yarn_get_mscale(self.scale) * self.attn_factor ) # Get n-d magnitude scaling corrected for interpolation # 此处attn总体的缩放系数和之前的extrapolation_factor一样，也默认为1，且通常不需要更改。 # 为什么要有这两类系数，也许是做实验时候进行网格搜索之类的，结果发现不变更好；也有可能是其他方法对attention进行缩放的话，如L2norm还是啥约束attn的大小，从而使得参数更容易稀疏化，就可以在这一步进行# 此外，yarn还有revise版本，其没有分开计算外推和内插：# inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))# inv_freq = inv_freq / ((1-inv_freq_mask)*self.scale + inv_freq_mask)# 以具体数值可能更好理解：[1, 1, 0.9,...0, 0] -&gt; [0, 0, 0.1, ...., 1, 1] -&gt; [0, 0, 0.8, ....8, 8] -&gt; [1, 1, 1.7, .....8, 8]# 具体的原因好像在于底层算子未按照预期运行？具体issue讨论在yarn下：https://github.com/jquesnelle/yarn/issues/24。此外，就算正确运行，这个revise似乎也有概率效果更好，但不保证奥class LlamaDynamicYaRNScaledRotaryEmbedding(torch.nn.Module): def __init__( self, dim, max_position_embeddings=2048, base=10000, original_max_position_embeddings=2048, extrapolation_factor=1, attn_factor=1, beta_fast=32, beta_slow=1, finetuned=False, device=None, ): super().__init__() self.dim = dim self.max_position_embeddings = max_position_embeddings self.base = base self.original_max_position_embeddings = original_max_position_embeddings self.extrapolation_factor = extrapolation_factor self.attn_factor = attn_factor self.beta_fast = beta_fast self.beta_slow = beta_slow # 只是在这里加了判断，如果处于fintune阶段，则重新以yarn计算rope，否则原生rope # 这个类的本质，是将yarn集成到RoPE中，支持rope原生训练以及微调时方便地使用yarn if finetuned: self.yarn( self.max_position_embeddings / self.original_max_position_embeddings, device, ) else: inv_freq = 1.0 / ( base ** (torch.arange(0, dim, 2).float().to(device) / dim) ) self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False) self.mscale = 1 # Build here to make `torch.jit.trace` work. self.max_seq_len_cached = max_position_embeddings t = torch.arange( self.max_seq_len_cached, device=self.inv_freq.device, dtype=torch.float32 ) freqs = torch.einsum(&quot;i,j-&gt;ij&quot;, t, self.inv_freq) # Different from paper, but it uses a different permutation in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1) dtype = torch.get_default_dtype() self.register_buffer( &quot;cos_cached&quot;, (emb.cos() * self.mscale).to(dtype), persistent=False ) self.register_buffer( &quot;sin_cached&quot;, (emb.sin() * self.mscale).to(dtype), persistent=False ) def forward(self, x, seq_len=None): # x: [bs, num_attention_heads, seq_len, head_size] # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case. if seq_len &gt; self.max_seq_len_cached: self.max_seq_len_cached = seq_len self.yarn(seq_len / self.max_position_embeddings, x.device) t = torch.arange( self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype ) freqs = torch.einsum(&quot;i,j-&gt;ij&quot;, t, self.inv_freq) # Different from paper, but it uses a different permutation in order to obtain the same calculation emb = torch.cat((freqs, freqs), dim=-1).to(x.device) self.register_buffer( &quot;cos_cached&quot;, (emb.cos() * self.mscale).to(x.dtype), persistent=False ) self.register_buffer( &quot;sin_cached&quot;, (emb.sin() * self.mscale).to(x.dtype), persistent=False ) return ( self.cos_cached[:seq_len].to(dtype=x.dtype), self.sin_cached[:seq_len].to(dtype=x.dtype), ) def yarn(self, scale, device): pos_freqs = self.base ** ( torch.arange(0, self.dim, 2).float().to(device) / self.dim ) inv_freq_extrapolation = 1.0 / pos_freqs inv_freq_interpolation = 1.0 / (scale * pos_freqs) low, high = _yarn_find_correction_range( self.beta_fast, self.beta_slow, self.dim, self.base, self.original_max_position_embeddings, ) inv_freq_mask = ( 1 - _yarn_linear_ramp_mask(low, high, self.dim // 2).float().to(device) ) * self.extrapolation_factor # Get n-d rotational scaling corrected for extrapolation inv_freq = ( inv_freq_interpolation * (1 - inv_freq_mask) + inv_freq_extrapolation * inv_freq_mask ) self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent=False) self.mscale = float( _yarn_get_mscale(scale) * self.attn_factor ) # Get n-d magnitude scaling corrected for interpolation 以上是yarn positional embedding的类代码由此，yarn代码解析完毕。什么嘛！做完之后还是挺通畅简单的嘛！ PoSE代码解析pose代码如下，这是核心代码，实际上不需要在modeling里面修改，只需要在tokenize之后、传入model之前进行操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293def train_preprocess_function_randomized(examples, tokenizer, scaled_max_position_embeddings, model_max_position_embeddings): inputs = examples[&quot;text&quot;] # 获取输入的query/src/text model_inputs = tokenizer(inputs, padding=False, truncation=True, max_length=model_max_position_embeddings) # tokenize，返回token list，也有可能是多个句子的token list position_ids = [torch.arange(len(ids), dtype=torch.long) for ids in model_inputs[&quot;input_ids&quot;]] # 对于多个句子， ids即一个句子tokenize后的id list，position_ids返回的是句子长度的[1, 2, 3, .. , seq_len]， for pos_ids in position_ids: len_pos_ids = len(pos_ids) # 获取的seq_len? tot_pos_list = list(range(scaled_max_position_embeddings)) # 扩展后的scaled_max_position_embeddings， 代码里显示scaled_max_position_embeddings=int(training_args.model_max_position_embeddings * training_args.rope_scaling_factor) new_pos_list = random.sample(tot_pos_list, len_pos_ids) # 从扩展后的pose id里面，随机筛选出当前seq_len的个数 new_pos_list.sort() # 排序后，应该是一个打乱的非连续的pos_id，本身token 位置对应的1， 2， 3...这类顺序pose不存在了 pos_ids[:] = torch.tensor(new_pos_list, dtype=torch.long) # tensor化 model_inputs[&quot;position_ids&quot;] = position_ids # 更新 model_inputs[&quot;labels&quot;] = model_inputs[&quot;input_ids&quot;] # token的label不变，即embedding不变， 但这一步有必要吗？ return model_inputs# 上面这个纯随机在实际中效果不好，因而不用；默认使用的是下面的有选择截断拼接def train_preprocess_function_pose(examples, tokenizer, scaled_max_position_embeddings, model_max_position_embeddings): inputs = examples[&quot;text&quot;] raw_model_inputs = tokenizer(inputs, padding=False, truncation=True, max_length=model_max_position_embeddings*5) # max_length变成五倍，意思是可以接受更长文本的输入。至于为什么是5，可能的一个原因是，单张80g的卡最多同时放下10000个token。pose再高单张卡放不下了。 input_ids = [] position_ids = [] for ids in raw_model_inputs[&quot;input_ids&quot;]: # ids 即为一个句子tokenize之后的id list len_chunk = min(len(ids), model_max_position_embeddings) len_input = len(ids) # 下面将ids切成两片 lt1 = 0 rt1 = random.randint(1, (len_chunk+1)//2) # 第一个切片，起始点是0， 终止点最多在：如果句子长，则在原来模型的窗长的一半内，通常是2048 // 2； 如果句子短，则是句子长度一半以内 rt2 = random.randint(lt1+len_chunk, len_input) lt2 = rt2 - (len_chunk - (rt1-lt1)) # 第二个切片， 长度是len_chunk - 第一个切片的长度。终止点一定是超过了len_chunk的，起始点未必 chunked_ids = ids[lt1:rt1] + ids[lt2:rt2] # 拼接得到新的chunk id，也就是token id的list input_ids.append(chunked_ids) # 把新得到的这一段切片加到input ids里 pos_ids = torch.arange(len(chunked_ids), dtype=torch.long) # tensor化 len_pos_ids = len(pos_ids) # lt = random.randint(0, scaled_max_position_embeddings-len_pos_ids) lt = 0 # this revision makes the coverage possiblity more uniform for large relative positions rt = random.randint(lt, scaled_max_position_embeddings-len_pos_ids) # pos_ids[:rt1-lt1] += lt # 第一个切片，不变 + 0 pos_ids[rt1-lt1:] += rt # 第二个切片后面加上一个随机整数 position_ids.append(pos_ids) # 处理完毕 # 这一步的操作，是将过长的文本进行切分与拼接，这个切分与拼接包含token id以及对应的rope id，之后会返回新的拼接后的结果。 # 也就是说，中间的那一部分被扔掉了。 # 本质上，这个方法是：当输入文本没超过之前的窗长，则文本不变，pose由本来的[1, 2, ....seq_len]切成两部分，第一部分不变（代码中+0），第二部分加一个扩展后窗长（如0 - 2048*5）内的随机数，这样会让文本的位置编码变成两部分的拼接。虽然比不上直接用全数据，但有时候没有长数据可以用这种方法来将之前的文本长度扩展，实际上还是妥协的方法 # 当文本超过了之前模型的窗长， 则将其限制在之前模型能处理的窗长内，具体为随机选取两段拼接。中间的部分也扔掉。至于位置编码，则和原来的位置也关系不大，第二个切片的位置编码随机加了整数，和超过窗长的文本本来的位置也对不上 # 回顾上面的lt1、rt1、lt2、rt2，如果len_input没有超过最大窗长（如2048），实际上inpput ids是没变的，如果超过，则选取一部分。关于位置编码的部分，第二部分其实都是[50,51 52.. 100]这种list每个元素加了一个随机数。和原来的没关系 # 总感觉在超出窗长文本位置编码的处理上有更好的方法？可不可以直接保留原来的位置呢？ model_inputs = &#123;&quot;input_ids&quot;: input_ids, &quot;position_ids&quot;: position_ids, &quot;labels&quot;: input_ids&#125; return model_inputsdef train_preprocess_function_pi(examples, tokenizer, scaled_max_position_embeddings, model_max_position_embeddings): inputs = examples[&quot;text&quot;] model_inputs = tokenizer(inputs, padding=False, truncation=True, max_length=scaled_max_position_embeddings) position_ids = [torch.arange(len(ids), dtype=torch.long) for ids in model_inputs[&quot;input_ids&quot;]] model_inputs[&quot;position_ids&quot;] = position_ids model_inputs[&quot;labels&quot;] = model_inputs[&quot;input_ids&quot;] return model_inputsdef test_preprocess_function(examples, tokenizer, inference_length): inputs = examples[&quot;text&quot;] model_inputs = tokenizer(inputs, padding=False, truncation=True, max_length=inference_length) position_ids = [torch.arange(len(ids), dtype=torch.long) for ids in model_inputs[&quot;input_ids&quot;]] model_inputs[&quot;position_ids&quot;] = position_ids model_inputs[&quot;labels&quot;] = model_inputs[&quot;input_ids&quot;] return model_inputs","categories":[{"name":"大模型","slug":"大模型","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"长文本","slug":"长文本","permalink":"https://yukino256.github.io/tags/%E9%95%BF%E6%96%87%E6%9C%AC/"},{"name":"YaRN","slug":"YaRN","permalink":"https://yukino256.github.io/tags/YaRN/"},{"name":"PoSE","slug":"PoSE","permalink":"https://yukino256.github.io/tags/PoSE/"}]},{"title":"补习HMM，对其的一些理解与思考","slug":"补习HMM，对其的一些理解与思考","date":"2024-09-17T06:04:19.000Z","updated":"2024-09-17T12:11:03.130Z","comments":true,"path":"2024/09/17/补习HMM，对其的一些理解与思考/","link":"","permalink":"https://yukino256.github.io/2024/09/17/%E8%A1%A5%E4%B9%A0HMM%EF%BC%8C%E5%AF%B9%E5%85%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3%E4%B8%8E%E6%80%9D%E8%80%83/","excerpt":"","text":"为什么要去研究HMM？首先，HMM这个概率模型，在NLP领域算是一个比较基础的，但很有价值的模型。它的主要思想是：概率分布及其能导向什么结果。关于HMM的理解，在具体的数据场景下是比较容易的，网上也有不少介绍的blog、视频，因而这里不再赘述了。 之前，记得在自然语言处理的课程上，是有提到过HMM模型的，依稀记得是用在句法分析还是词性标注上面。但是由于老师死气沉沉讲的不好，我们院系设置的又不是必选，因而听的也不甚上心，最后也果然没考到，HMM自此埋下伏笔，成为了我的一个知识盲区。后来，应用场景大多是生成文本、或是对文本的内容进行分析，已经没有用到HMM的必要了，像是jieba使用HMM模型作为它的分词器，我只需要懒人使用即可，重点还是后面的内容理解与分析上，因而再也没有接触过这个东西。 实际上，最近，由于准备秋招，深感自身基础不足，在看牛客上的面经，会需要手撕一些基础的分类、聚类、transformer等一些算法。说实话我在这方面是没有准备过、相当薄弱的，因而需要将这些手撕都写一下，我也因此在GitHub上开了一个仓库专门做这个事情。 HMM，本来是不在我的计划范围之内的。但是和朋友聊天的过程中，说到他正在开发的输入法所面临的算法问题，需要用到HMM，感到眼熟的情况下，正好手撕代码也遇到了一些瓶颈，需要再去看一下东西，因而走马观花地浏览了一下HMM，本来还以为会像之前好几次一样看完不懂就扔，没想到这次一下栽了进去出不来了。 为什么呢？主要是因为看到了一份代码，是写了HMM的模型，支持根据观测序列推断HMM三元组。一个class，大概几百行，也不冗余，很快就搞定了。这不也是机器学习算法的一种吗？根据代码来看，fit与predict，何其熟悉的两个函数！因而来了兴致，花了几天去钻研一下子。但是这个数学公式实在是不太熟悉难以看懂，因而是有些痛苦的。 所幸的是，今天，终于理解了一部分，虽然EM算法这个使用拉格朗日乘子与KKF来求解HMM三元组的极大似然的方法仍然是不懂，但至少，对于输入法使用HMM的这个方面，是理解的差不多了。 如何理解HMM？在这里，需要明晰的一个概念就是：HMM只是一个，根据初始概率分布的List——Pi、每个状态在任意时刻下到所有其他状态的状态转移概率矩阵——A、以及每个状态在任何时刻下所能观测到的的观测概率矩阵——B，来计算观测序列的概率分布的模型。如果说，有M个状态、每个状态有N种概率，那么Pi的size就是1 * M， A的size就是M * M， B的size就是M * N。如果看这个shape不能直观理解的花，可以去搜一下HMM的例子，网上有很多。这里需要注意的是，HMM使用是基于两个独立性假设的。 （本来用md想写公式的，看了看有点烦，暂时搁置了，教程网址点这里） 关于HMM，在已知三元组的情况下，求出某个观测序列产生的概率，可以用前向算法或后向算法解决。总之都是硬算。具体公式解析看这篇博客，写的挺好的。代码实现则可以参照这个博客,d但公式解析就不要看了，公式没写好不直观。 前向算法是简单的动态规划即可解决了，就是一层层套娃比较麻烦，而在将概率矩阵用dict保存的这种稀疏矩阵场景下尤甚。至于后向算法，由于在计算过程中，不仅需要已知三元组，还需要用到前向算法的结果，还需要利用两大假设对公式进行化简、相当不容易直观理解，因而必须在前向算法结束之后才能使用。在这个解决问题的场景下，显得有些幽默。但后向算法的意义似乎不在于此，而在于EM算法评估参数的过程。（EM算法实际上就是极大似然估计法，只是这个算法出现的比极大似然估计早，总不能老子改成儿子的名字吧！）EM算法还是看不太懂公式，但是不要紧！分词和句法分析这类用不到！ Viterbi算法————前向算法，只不过：计算的是最有可能的路径，且你需要记录你走过的路径，以及当前这个状态最大的状态值 在输入法的这个场景下，由于概率连乘会导致精度消失问题，通常用log转化为加法。那么，可以很好地构想出Viterbi算法的意义了：找到从起始节点到终止节点，sum最小的路径长度。只不过在乘法视角下有些不太直观 那么，在HMM的场景下，我们需要记录什么？需要记录每一步能够到达的状态及其开销（概率）。直到最后一步。同时，在每个时间步记录我们当前时间步为止的最有可能的状态序列；然后，对于我们记录下来的最终的这个状态序列，逐个去查每个状态最有可能的观测值，得到的和状态序列长度相同的观测值序列，就是维比特算法的结果。 输入法场景下，我们会根据输入的字母组合，如“asdasd”这种字符串，去查拼音表进行一个字符串匹配的贪心切分，然后会返回一个或多个切分的结果。根据这个切分的结果，我们将字母decode成汉字。在decode的过程中使用的Viterbi算法。转移概率矩阵及其观测概率矩阵是不同的。如，对于切片结果[‘a’, ‘b’, ‘c’], 初始分布概率Pi是’a’这个字母对应的字的出现频率；到了转移矩阵，则是第二个时间步，也就是开始“decode” b对应的字的时候，这个概率转移向量，就是所有“a”能decode出来的字， 后面跟的字以“b”为拼音开头所出现的频率。 也就是说，对于我们去获得HMM的三元组的语料库。我们需要做以下几件事： 对于初始化Pi矩阵：需要获取所有字的拼音，按首字母、全拼音长度两个（或许你也可以统计残缺拼音的频率，但应该会对精确度造成损失。），对拼音对应的字的频数&#x2F;频率进行统计。以dict的形式保存起来。然后在输入法decode的阶段，查这个dict作为初始Pi矩阵 对于转移概率矩阵，则需要统计，语料库从头到尾，前一个字之后的当前字的拼音及其对应字同时出现的频数&#x2F;频次（考虑到首字母、全拼音的组合，这个应该是1 * 2种可能？）然后，以前一个字作为key，后一个字{拼音： [字 and 频， 字 and 频，字 and 频，字 and 频]}这种形式保存起来，作为后续的状态转移概率矩阵。 对于观测概率矩阵，则是统计拼音对应的字的概率。实际上也是Pi矩阵。这个矩阵只会在上面转移概率发现没有对应的下一个词的统计结果，而只能将目前的拼音与前面割裂开来、当成一个独立的来decode时候，才会使用。实际上也是新一轮的HMM初始化？ 因而，输入法的维比特算法，与一般维比特算法的区别在于：转移概率部分替代观测概率。 此外，输入法还需要整合记录其他的路径的内容和概率以便做排序，这个还得再想想…… EM算法推导还是看不懂，公式和代码死记硬背吧！是《统计学习方法》——李牧里面的。感兴趣可以去看看原书第10章。","categories":[],"tags":[{"name":"概率","slug":"概率","permalink":"https://yukino256.github.io/tags/%E6%A6%82%E7%8E%87/"},{"name":"机器学习","slug":"机器学习","permalink":"https://yukino256.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"输入法算法","slug":"输入法算法","permalink":"https://yukino256.github.io/tags/%E8%BE%93%E5%85%A5%E6%B3%95%E7%AE%97%E6%B3%95/"}]},{"title":"秋招大失利，深夜破防记录","slug":"秋招大失利，深夜破防记录","date":"2024-09-12T15:56:26.000Z","updated":"2024-11-26T13:07:42.610Z","comments":true,"path":"2024/09/12/秋招大失利，深夜破防记录/","link":"","permalink":"https://yukino256.github.io/2024/09/12/%E7%A7%8B%E6%8B%9B%E5%A4%A7%E5%A4%B1%E5%88%A9%EF%BC%8C%E6%B7%B1%E5%A4%9C%E7%A0%B4%E9%98%B2%E8%AE%B0%E5%BD%95/","excerpt":"","text":"首先，梳理一下逃离北京之后，我大概做了什么 去LF那里躺了一周，之后一直躺家里。期间投、面了一些暑期实习，结果均以早早失败告终。究其原因，感觉在于八股、代码不熟悉，很菜。因而，这段时间主要又刷了leetcode的hot 100，大概2-3遍。由于在家有吃有喝没人逼迫，全凭自觉，因而刷的慢；到了八月，总算有了危机意识，开始着手准备八股了，却对于秋招的提前批也没有多投。是懒惰？抑或是自卑、畏惧？无论如何，是要开学回学校了，正式批，瞄准中、大的互联网厂，基本上都投了一遍；后面又投了几批。 那么，投递的结果怎么样呢？基本是全挂，笔试A的不够多，选择题之类的大概又都又一半是模模糊糊印象不深刻只能嗯猜，因而笔试最终结果肯定不是很好。但是让我去提升，一时半会也无法速成。像是各种排序算法的流程细节、每一轮的迭代结果；像是分类、聚类基本算法的一些八股；像是C++&#x2F;C的老旧的、大学课程式的输出结果判断；像是机器学习各种方法的优缺点、适用场景等……————都是一些零零散散，但是不深入理解一下的话毫无作用的东西。然而相对于真正的重中之重：NLP和LLM方面的代码手撕与细节八股，重要程度却又更低一点，因而暂时无法抽身。我深陷囹圄，终会被埋没。这是对我之前半吊子的惩罚吧……也有可能是我的脑子只能支持我走到这里了。还有单薄的简历，经历实在是单薄，无法编造下去了……只能听天由命罢了，这个时候除了拜神，我还有什么安慰自己的手段呢？ 迄今为止，一共拿到了三场面试。北京联想，一面秒挂、发感谢信；北京百度，一面秒挂、共享中，估计已经似了，毕竟感觉这个一面也是被人家捞上来的；今天下午刚刚面完滴滴的三面，这个流程感觉还不错，一气到底直抒胸臆，然而三面完之后，出不来结果，又要经过煎熬的等待；三面的面试官加我微信，问我要不要去实习，我惊讶之余又有些明悟，毕竟他在面试的时候已经试探了两三次。实习当然是可以去的，顾虑的无非就是，担心他在骗我，这个实习跟校招不是平行而互不影响，而是他想要三面挂我，然后拉我过去做一个没有转正的实习、亦或者看我不行就把本来打算给我的校招名额剔除……我无法相信一个陌生人，只能提心吊胆，但是能怎么办呢，选择权不在我。 刷了刷牛客，刷到了一些同届校招生分享的面经。印象很深的是两人，一个是西交的本硕、一个是清华的本硕。两人投递秋招，基本上都是能进面的话就从头面到尾，然后拿offer，是收割机；清华的尤其突出，给我的震撼是拼多多的笔试题全A了，这是我根本不敢奢望的。对我的世界观造成的冲击是，我一直以为没学过竞赛、根本不用考虑笔试全A这种东西；而这位清华爷应该搞的不是竞赛，毕竟有三四段实习，面经里面也没提到有竞赛经历，因而这是纯纯的聪明。我感受到我的脑子、能力远远不足；同时看他的面经，每一轮都感觉答不上来……我真的适合在这一行干下去吗？我真的能找到心仪的工作吗？我是否将自己看的过高、而应该脱下我的长衫、从台阶上走下来？算法！算法！唉…… 我为什么要去追求好的工作、好的学历？实际上，就我个人而言，是无所谓而得过且过的，活下去就可以。但也许是责任心太重、也许是虚荣，总之出于家里亲朋的期许，我是需要成为一个他们眼里“成功”的人。 上面的也许是借口，还有一种可能是，我只是心中苦闷，而尝试将这个归因于外部，实现一种压力转移与逃避。无论怎么说，那种想法对我心态的调整无益……最近一个月都是在忙忙碌碌、发愁，一点游戏都没碰过，提不起兴趣来……最基本的生存需求感觉在未来已经无法满足了……现在最重要的是活命…… 总而言之还是梳理一下接下来要准备的东西吧……起码可以自欺欺人：准备到春招、期间再搞个实习，未来就会光明起来……唉，我是一个很悲观的人。 手撕LLM的相关代码、包括模型结构、损失函数、优化器、位置编码；继续看NLP和LLM的八股（从牛客面经上偷） 手撕机器学习的经典算法，回顾机器学习的经典八股 数据结构、排序算法、代码题 暂时想不起来了…… 我将会走向何方呢？ 秋招寄录更新滴滴的实习过了，下周四入职……北京的租房好贵……不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡不想去北京😭不得不去😡 再次更新此时是2024年11月底，这个记录是10月底记录的，那之后再无进展，心终于彻底似了。开摆了嘻嘻🤗🤗🤗🤗 投了约30+公司。算法岗，NLP&#x2F;LLM为主，有个别搜广推。 联想一面挂 百度一面挂 度小满一面挂 滴滴三面挂 拼多多三面挂 剩下的没进面","categories":[{"name":"日常","slug":"日常","permalink":"https://yukino256.github.io/categories/%E6%97%A5%E5%B8%B8/"}],"tags":[{"name":"日记","slug":"日记","permalink":"https://yukino256.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"随想","slug":"随想","permalink":"https://yukino256.github.io/tags/%E9%9A%8F%E6%83%B3/"},{"name":"心情","slug":"心情","permalink":"https://yukino256.github.io/tags/%E5%BF%83%E6%83%85/"}]},{"title":"DailyRecord-June","slug":"DailyRecord-June","date":"2024-06-03T01:28:33.000Z","updated":"2024-09-13T04:51:18.665Z","comments":true,"path":"2024/06/03/DailyRecord-June/","link":"","permalink":"https://yukino256.github.io/2024/06/03/DailyRecord-June/","excerpt":"","text":"6.1-6.2周六，中午和叶阿姨和她的女儿一起在中关村附近一个饭店吃饭，看上去还挺高档的。吃了北京烤鸭、麻辣牛蛙、豆腐煲、宫保鸡丁……味道还可以。和叶阿姨、她女儿聊天，聊了很多。在我的朦朦胧胧的印象里（也许不是印象，而是我脑部的期许？毕竟那时候我才3、4岁），叶姐姐是一个温柔而活泼的大姐姐。时过境迁，到了现在，温柔仍然是温柔，已经不再活泼。从她的面庞中、眼神里，似乎感觉到她也有些感慨、或是悲伤，隐隐可见泪光。叶阿姨的女儿是高三，马上高考。但是她似乎有压力但不大？或许是我没能看出她笑容下面隐藏了什么吧。总之，是一个活泼可爱的小姑娘，穿搭、皮肤质量之类的与我们当时完全不同。记得当时整个高中生涯似乎都是瘦巴巴、苦哈哈的，哈哈！从姑娘的身上，我隐隐也看见了过去自己的影子：无需考虑生活，有着许多不切实际而有趣的理想、兴趣，也许会实现、也许不会实现。例如深山老林生存式隐居、周游世界……当然，北京小姑娘的条件肯定比我小镇做题家好很多，也许都能实现也说不定？更加感叹人与人的不同，小姑娘目前为止已经去过不少地方了，去年好像还独自去旅游过……吃完饭，小姑娘带着我们去打桌游，一个叫什么法师的桌游。玩了大概一两个小时，挺开心的。叶阿姨叶很有参与度。总之玩到了三点，然后我要去圆明园了。本来打算坐地铁，但是叶阿姨让出租车绕路送了我一下。和ZZK逛圆明园。园内道路错综复杂，我和他也没有做攻略，走到哪儿看到哪儿，只在有地图告示牌的时候稍微看一下自己的位置。前天，北京的强对流天气很猛，我在公司里没感觉到，但刷视频的时候看见了国贸大厦的空中飞人，以及路口被大树砸扁的汽车，还有圆明园里面倒塌的大树。而现在，在圆明园里面逛，真是处处可见被吹断的大树。有的已经被切段、挪走、堆积了，而有的还是原地躺在那里，似乎在向游人说：好哇！我都这样了，你们还开开心心的逛公园！在园里逛了约莫3个小时，看看树、石头、湖水、小土堆、湖中的鸭子和水鸟，说到底圆明园里面就剩这些东西。最有印象的，还是历史书上的圆明园火烧遗址，那个拱门就那样孤零零的站在那里，和十年前在历史书上看到的一模一样。以这儿为背景，拍了几张照片，照片上的我笑意盈盈地竖起了大拇指，心里想的是，总算在现实中看见了历史。结果照片拍下来发给家人和朋友，却被哥们儿指出在这个场景下多少有点不正确🤣🤣🤣到了六点多，其实才逛了圆明园的约莫三分之一。但是已经买好了8：20的电影票，因而得准备出园，去吃饭然后看电影。我们逛的不多，但是逛的很深入，因而离出口的门很远。走了大概四五十分钟，才从圆明园里面出去。还没有逛完！下次还来！电影院在上地，上次和NJ来过这里。这次，我和ZZK在这里，点了喜茶、然后去吃萨莉亚（捏妈！意大利沙县小吃！）。借了沙比怪兽充电宝血亏10R，买了冰淇淋，带到了电影院里面吃。看的是 《哆啦A梦：大雄的地球交响乐》， 我是蓝胖子的忠实粉丝！除了3D版之外，其他剧场版可以说是一集不漏吧！今年的剧场版也不错，比去年的沙比强行煽情讲道理要好太多！此外，mika也是非常非常非常可爱！就是她妹妹这个伏笔感觉有点突兀，而且回收的不是很好，美中不足吧！看完电影，十点多了，急匆匆赶地铁，结果还是以两分钟之差错过了最后一班地铁，唉！花了28R打车回到了出租屋。周日，躺了。还是有经典的大乱斗环节。 6.3 - 6.4这两天的工作，是改多路召回项目。主要是后端API的调试与修改。我还是要说，没有项目文档和注释的代码真是史！这两天又塞了新的需求过来，额外地加、改。我的评价是算法转测开。 周一的晚上，终于下定决心跑路了。因为感觉在这学不到东西，在公司名头不大的情况下，继续呆下去只会在舒适区里面溺死，结局就是秋招找不到好工作。在这边上班拿钱固然很好，但是没法刷题、面试。下定了跑路的决心，于是昭告天下，跑路！周二上午，向这边沈老师的师姐说了离职的事情。然后向leader、人事报告。下午就去办了一下预离职的程序，拿了三个表回来，下周的时候交。晚上，NJ和我聊了聊申请澳洲博士的事情，给了我很多建议。总而言之，下一阶段的任务是： 刷leetcode hot 100，三遍，然后刷牛客的I&#x2F;O模式 写英文简历，向澳洲那边广撒网 唉…… 6.5 - 6.7没离职之前还有不断的需求要去改……总的来说是改改需求，调调API。没什么值得说道的。可能主要是新来那个后端实习生干的，总之问答对库的分级完成了，知识库删除功能也调整好了。学到的比较有价值的东西，是session这个数据库操作对象。知道了这个是先定义好连接然后进行操作。 偷偷刷了两道题吧 6.8 - 6.10没有调休，好耶！周六中午，坐地铁去王府井，和ZZK汇合，在王府井逛了逛，只能说很失望，就是一条步行街感觉。路倒是挺宽敞的。在王府井买了稻香村的一盒糕点寄回家去，-200；当场快递，-42；真佛了这么贵的快递。之后，在KFC吃了午饭。又在王府井周围逛了逛。看见了教堂，还不错。之后，买了杯喜茶，坐地铁去天坛公园了。走西门进的天坛公园，与上次进来的门不同，因而都是新风景。看见了路边一颗颗的古树，有百年的小树，也有六百年的大树。天哪！六百年！多么大的一个数字，多么漫长的一段岁月！看着古树，我想想五百年前、四百年前，也有什么人从它边上走过，一种历史的厚重感扑面而来……之后，去看了祈年殿、圜丘、回音壁。倒是没什么好看的，排队的人很多，但最后看到的结果不如想象中那么宏伟壮观。之后，坐在长椅上歇了一会儿，晚上到商场吃了焖锅。喝了星巴克、椰汁……劳累但是充实快乐的一天。周日，躺。周一，端午节。和SL、NJ、LT一起吃了湘菜。吃完之后，在附近的公园，席地而坐，打了两小时的掼蛋。回想上次，似乎就只是高二的某个夜里。之后就再也没有像这样聚齐打牌过。记得大二的时候，112宿舍的六人还集齐过，在XY中学对面电竞宾馆打游戏，但是似乎没打牌。吃完饭，LT跟我回出租屋。我们将要一起对付一周这样。多了一个人，虽然很挤，但有了生气、有了一点归属感。 6.11晚上没睡好，醒了几次，感觉睡得浅。可能是怕LT睡觉不老实对我搂搂抱抱乃至撅我，因而小心翼翼的感觉（不是上午，交接了一下多路召回的东西。感觉可以准备润了。刷了不短时间的手机…… 6.12今天交表跑路噜 6.13以后这一段的事情总算是告终了，然而新的事情必须开始。总之，在即将离开北京的这几个日子里，和在北京的朋友们聚了聚： 和LT一起挤在出租屋的小床上，一块儿睡了几天。他也离我而去之后的第二天，我也离开了北京。 和LT、ZZK一起，逛了故宫、人民大会堂，天安门广场。从上午9点多集合，一直走到下午3点多，才勉强逛完，去餐厅吃迟了很久的午饭。吃完之后，也很累了，回去出租屋躺了，也没打游戏。 临走的前一天夜里，和被调去北七家那边的同事见面，大概九点了。吃的烤鱼。因为之前在做拼多多的笔试题目。聊了聊一些东西，瞎聊罢了。 周五还是周六的下午16.00，面了北京字节。拷打qwen的细节答得很不好，手撕也不太行，总之面完是秒挂了。周一的晚上，赶火车，差点迟到。跑了一路总算勉强赶上了。呼吸道似乎因此出了点问题。火车上睡了一夜，睡得不太好，还是比较累。周二早上，和LF汇合了。吃了火车站边上的热干面当早点。一般。然后去华科逛了逛，中午吃的华科自助餐，挺不错。下午赶火车去潜江，这次没赶上。改签了。晚上到了LF住的地方。偏僻而清净。有不少的感触，想了想不知从何写起，似乎也没必要说。在这或许能有一些宁静吧，我不知道。 总的来说，日记应该到此为止了。写这几个月的日记，是为了减轻我自己的迷茫，帮助看清自己。现在已经结束实习、离开北京。或许下一段实习的时候还会写吧，怎么样呢，我不知道。","categories":[{"name":"记录","slug":"记录","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/"},{"name":"大模型","slug":"记录/大模型","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"实习","slug":"记录/大模型/实习","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%AE%9E%E4%B9%A0/"}],"tags":[{"name":"日记","slug":"日记","permalink":"https://yukino256.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"随想","slug":"随想","permalink":"https://yukino256.github.io/tags/%E9%9A%8F%E6%83%B3/"},{"name":"心情","slug":"心情","permalink":"https://yukino256.github.io/tags/%E5%BF%83%E6%83%85/"}]},{"title":"DailyRecord-May","slug":"DailyRecord-May","date":"2024-05-02T12:40:09.000Z","updated":"2024-05-31T09:44:14.238Z","comments":true,"path":"2024/05/02/DailyRecord-May/","link":"","permalink":"https://yukino256.github.io/2024/05/02/DailyRecord-May/","excerpt":"","text":"5.1 —— 5.55.1 五一假期的第一天。再说一遍：我恨调休！我恨调休！昨天晚上21：46的火车，早上6：35火车到站。只有站票，我和LT带了马扎上去，结果发现能放马扎的地方也很少。在4月中旬的时候，我和LT说一起回家，聊到站票买马扎的事情，我说我买两个，他不用买直接过来就行，结果他忘了自己也买了一个。于是多出了个小马扎。我是14车，LT是15车，他在15车帮我占了个位置，结果发现是厕所门口，而这个厕所的门还坏了，乘务员过来修反而把门直接给搞下来了；中途，LT试图将多余的马扎以15R推销出去，结果失败了🤣🤣🤣🤣。每节车厢都带个厕所，因而站票大抵是都要在厕所附近窝着的我是不想当老八的，正好听闻说餐车那边有空位，因而我拎着我的两个小马扎过去找位子。到那儿发现餐车坐满了，估计都是站票过来买饭送座位的。餐车的尽头比较空旷，只有一个哥们儿在地上铺了个纸躺着。我在这儿蹲了下来，LT怕被赶走，让我看看情况，半小时没人赶就叫他。确实没人赶，然后他就过来了，过来一合计，吃点东西，然后去问有没有空位。乘务员腾了两个相邻的位子给我们，然后96r，没得选择，两人吃鱼香肉丝饭。我吃了一碗半，LT吃了两碗半。吃完后，可以名正言顺的占用餐桌。由于跨月，因而碧蓝航线要刷一下大世界的剩余体力，玩手机到1点，LT默默陪着我一起玩，我放下手机趴下他也趴下了。说实在话，虽然有坐，但是不安静，乘务员嘈杂，他们不关心你是不是在睡觉；尤其是2点多乘务员集体早餐，我们那个位置需要暂时让一下给乘务员吃饭，吃完饭一般立刻就走了，但是有不走的留在桌子上、东百口音的男的在吹牛逼，不压低声音。总而言之，到4、5点的时候，我们也只睡了质量很低的断断续续的2小时左右。然后到了开早饭的时候，被乘务员撵走了，我们到了餐车末尾的地方蹲了一小时，又被撵走了。总之，6.35到车站，然后LT他妈妈开车来带我们，把我送到家。妈妈在家等我，我洗了个澡，然后七点多开始睡觉，一下睡到中午12.30。然后去门市吃午饭，牛肉啥的，家常菜。啊！家的味道！下午，LSH叫我出去上网，我们把LT叫上了。线上叫了道哥，以及我的研究生同门SH，凑了五黑，大大乱斗。下午两点打到8点，十几把输了两把，爽赢！网吧有茶水售卖，喝了红茶。然后，我骑的自行车留在了网吧门口，LT开车带我、LSH、SXZ一起去步行街吃龙虾。喝啤酒、吃下酒菜，倾诉一下，聊聊在北京一个月的一些经历和感想。啤酒喝晕了，他们驾着我打车去浙江商城那儿的KTV唱歌。我真不知道自己是怎么过去的，只知道自己被人驾着。到那儿，躺到了0：30，醒了，唱歌到2点，走路回家睡觉。和朋友们喝酒是开心的、唱歌是开心的。5.2 五一假期第二天。中午去我爹那儿吃午饭。我的大爷、我爸的新对象和那个女儿也来了，喝了点啤酒，聊了聊一些烦恼。但是在这里接收到的只有“努力！拼搏！”一类的大道理、大男子&#x2F;大家长主义的说教。挫败，再次决定不应该向这里倾诉一些东西了，在母亲那儿更放松一点。但这个家族氛围就是这样，总体上还是和睦的、较好的，一些东西应该学会去忍受。下午，依旧昨天的配置、昨天的阵容，今天点了枸杞菊花茶，因为感觉这个好像是明目的，最近感觉视力有所下降。一开始连跪，沉默不语，然后连胜了，最后打到18点多，光荣下播！骑着自行车到门市吃饭捏，家的味道！*2🥰🥰5.3 五一假期的第三天。中午去我爹那儿吃饭，下午还是和他们一起去网吧。晚上去门市吃饭，之后去了舅爹舅奶家。看了看菜地、菜地边上舅爹晒太阳的破沙发。我妈看生菜长得好要了点回去。之后，在地下室的屋子里面坐着聊天，临走时，我把装在信封里面的、从银行取出来的新钱2k给了舅爹舅奶，实习工资的一部分，很有纪念意义。感到舅爹舅奶有点手足无措、又有点失落。5.4 五一假期第四天。中午去门市吃饭，然后在家里躺了一会儿。下雨了，NZY告诉我他在初中部那儿和初中老师吃饭。LT开车去带了他，还带来了HL。我们一起在网吧打了一两盘，他们就都走了。HL感觉没啥精气神儿了，这班上的。NZY也好像有点心事，感觉他不像以前那样开心了。晚上，到门市吃饭，和妈妈一起回家。5.5 今天回百京。下雨，早起坐上去宿迁的车，大概9点到高铁站。13点多到百京，点了个东北铁盒饭外卖，到出租屋的时候吃。下午和SH打了大乱斗，一直输。在出租屋没事干，混日子呗。 家里真舒服啊！真舒服啊！ 5.6复工。查看五一期间跑的训练怎么样了。发现训练没跑完，然后vscode远程sshA800也连不上了，但是bash可以连。百思不得其解。终于发现原因是保存的checkpoint文件太多、把硬盘填满了（本来剩下几百G）。测试checkpoint的效果，正常问答和指令问答效果都很差。看样子需要重新斟酌。需要仔细选取数据，进行调整。或者直接放弃改底层模型，因为这导致训好的模型被利用的很差。 此外，加载模型时候有这个提示： 123456789Some weights of the model checkpoint at output_qwen/checkpoint-2000 were not used when initializing QWenLMHeadModel: [&#x27;transformer.h.0.attn.bias&#x27;, &#x27;transformer.h.1.attn.bias&#x27;, &#x27;transformer.h.10.attn.bias&#x27;, &#x27;transformer.h.11.attn.bias&#x27;, &#x27;transformer.h.12.attn.bias&#x27;, &#x27;transformer.h.13.attn.bias&#x27;, &#x27;transformer.h.14.attn.bias&#x27;, &#x27;transformer.h.15.attn.bias&#x27;, &#x27;transformer.h.16.attn.bias&#x27;, &#x27;transformer.h.17.attn.bias&#x27;, &#x27;transformer.h.18.attn.bias&#x27;, &#x27;transformer.h.19.attn.bias&#x27;, &#x27;transformer.h.2.attn.bias&#x27;, &#x27;transformer.h.20.attn.bias&#x27;, &#x27;transformer.h.21.attn.bias&#x27;, &#x27;transformer.h.22.attn.bias&#x27;, &#x27;transformer.h.23.attn.bias&#x27;, &#x27;transformer.h.24.attn.bias&#x27;, &#x27;transformer.h.25.attn.bias&#x27;, &#x27;transformer.h.26.attn.bias&#x27;, &#x27;transformer.h.27.attn.bias&#x27;, &#x27;transformer.h.28.attn.bias&#x27;, &#x27;transformer.h.29.attn.bias&#x27;, &#x27;transformer.h.3.attn.bias&#x27;, &#x27;transformer.h.30.attn.bias&#x27;, &#x27;transformer.h.31.attn.bias&#x27;, &#x27;transformer.h.4.attn.bias&#x27;, &#x27;transformer.h.5.attn.bias&#x27;, &#x27;transformer.h.6.attn.bias&#x27;, &#x27;transformer.h.7.attn.bias&#x27;, &#x27;transformer.h.8.attn.bias&#x27;, &#x27;transformer.h.9.attn.bias&#x27;]- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of QWenLMHeadModel were not initialized from the model checkpoint at output_qwen/checkpoint-2000 and are newly initialized: [&#x27;transformer.h.0.attn.beta&#x27;, &#x27;transformer.h.1.attn.beta&#x27;, &#x27;transformer.h.10.attn.beta&#x27;, &#x27;transformer.h.11.attn.beta&#x27;, &#x27;transformer.h.12.attn.beta&#x27;, &#x27;transformer.h.13.attn.beta&#x27;, &#x27;transformer.h.14.attn.beta&#x27;, &#x27;transformer.h.15.attn.beta&#x27;, &#x27;transformer.h.16.attn.beta&#x27;, &#x27;transformer.h.17.attn.beta&#x27;, &#x27;transformer.h.18.attn.beta&#x27;, &#x27;transformer.h.19.attn.beta&#x27;, &#x27;transformer.h.2.attn.beta&#x27;, &#x27;transformer.h.20.attn.beta&#x27;, &#x27;transformer.h.21.attn.beta&#x27;, &#x27;transformer.h.22.attn.beta&#x27;, &#x27;transformer.h.23.attn.beta&#x27;, &#x27;transformer.h.24.attn.beta&#x27;, &#x27;transformer.h.25.attn.beta&#x27;, &#x27;transformer.h.26.attn.beta&#x27;, &#x27;transformer.h.27.attn.beta&#x27;, &#x27;transformer.h.28.attn.beta&#x27;, &#x27;transformer.h.29.attn.beta&#x27;, &#x27;transformer.h.3.attn.beta&#x27;, &#x27;transformer.h.30.attn.beta&#x27;, &#x27;transformer.h.31.attn.beta&#x27;, &#x27;transformer.h.4.attn.beta&#x27;, &#x27;transformer.h.5.attn.beta&#x27;, &#x27;transformer.h.6.attn.beta&#x27;, &#x27;transformer.h.7.attn.beta&#x27;, &#x27;transformer.h.8.attn.beta&#x27;, &#x27;transformer.h.9.attn.beta&#x27;]You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. attention的bias和我后面新加的beta参数，没有从模型中加载到。为什么？bias这个好像不影响。但是beta是核心参数，没了直接乱说一气。 开发大哥那边需求连接服务器，需要服务器接上wifi，同时能够连接A800。本以为是个改路由的大工程，但是没想到直接接入网络就行了。也许是接入网络先后顺序的问题？搞不懂。 改了infini里面的结构，本来是融合后在projection，我把projection放前面了。因为我不想动这个projection的参数，只想训练融合的参数信息。不知道结果会怎样。如果后融合的话，projection的参数也要变，模型本身能力直接寄。 晚上，开了例会。前辈哥给出了问题的可能解决方法，明天需要测试一下： 检查checkpoint目录下的config.json、modeling_qwen.py文件是否与预期一致 super().init(config) 移到最后，检查是否是继承类的初始化问题 此外，对于c_proj是否需要调整位置的问题，建议还是先不调整试试，一般影响不大。 晚上，和LT、SH一起大乱斗，赢！ 明日任务：测试解决方案是否可行 5.7测试昨晚商讨的解决方案。无效果。尝试打印模型结构和参数，结果显示attn已经修改，且模型参数里面有beta：没办法，上不去下不来，卡在这儿了😅看了一下相关的issue，似乎是加载模型的问题，好像要改from_pretrained。 来自ZYR的场外支援！开了个腾讯会议，排查了两小时，确定是from_pretrain的问题！我到这不知道该怎么办，他给出了另一种加载模型的方式： 123456789101112from transformers import AutoConfigfrom safetensors.torch import load_modelconfig = AutoConfig.from_pretrained(&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem&quot;)model = AutoModelForCausalLM.from_config(config)ckpt_file_1 = &quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00001-of-00004.ckpt&quot;model.load_state_dict(torch.load(ckpt_file_1), strict=False)ckpt_file_2 = &quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00002-of-00004.ckpt&quot;model.load_state_dict(torch.load(ckpt_file_2), strict=False)ckpt_file_3 = &quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00003-of-00004.ckpt&quot;model.load_state_dict(torch.load(ckpt_file_3), strict=False)ckpt_file_4 = &quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00004-of-00004.ckpt&quot;model.load_state_dict(torch.load(ckpt_file_4), strict=False) 在此之前，需要将safetensor逐个转化为ckpt文件： 12345678910111213from safetensors.torch import load_fileimport torchimport sysimport ospath = sys.argv[1]if not os.path.exists(path): print(&#x27;path not exists&#x27;) sys.exit(0)device = &#x27;cpu&#x27;weights = load_file(path, device=device)weights[&quot;state_dict&quot;] = weightstorch.save(weights, os.path.splitext(path)[0] + &#x27;.ckpt&#x27;) 将safetensor转化成ckpt，然后加载。加载的比from_pretrain慢好多！但是问题解决了！我必须立刻去研究一下这个加载方式的原理！真得给我朱哥磕一个吧，帮我太多了！safetensor是安全的，也就是说为了确保安全性、不包含恶意代码之类的，就要限制其中的东西，因而不让改动，也可以理解。 我定位到了问题，但没能解决的原因，在于对权重文件及其加载了解不深入，实际上这对我是傻瓜式的操作、黑箱。晚上没有战斗，混日子捏……感觉不到啥拼搏的动力，浑浑噩噩月光族也是活着…… 明日任务：测试训练好模型的效果 5.8昨天的代码，默认是在cpu上加载模型的，需要改成如下格式 123device = torch.device(&quot;cuda&quot;)torch.load(ckpt_name, map_location=device) model.to(device) # 这一步不知道究竟是否需要 对话效果烂了。我决定照着原论文的结构来复现，再训练一遍，看看到底怎么回事。看了过程中tensor的shape，似乎之前的代码shape不对？也许效果不好的原因就在于此。改了之后再跑训练看看效果叭！趁着训练的时候，查看ckpt和safetensor的区别。ckpt这种文件似乎是torch的官方保存方式，以dict形式存储torch.nn.module的模型结构及其对应的参数，在加载的时候使用torch.load加载。这个过程是很直观简单的。（正常训练结束的文件，好像应该是.pt或者.pth？）以下内容来自网络： .ckpt文件是Pytorch Lightning框架中使用的模型文件格式之一。Pytorch Lightning是一个基于Pytorch的轻量级深度学习框架，它提供了更简单和更高层次的API，用于训练和管理深度学习模型。.ckpt文件保存了模型的参数和优化器的状态，并且通常还包含训练的元数据信息。 .pth文件是Pytorch中最常见的模型文件格式之一。它是一个二进制文件，包含了模型的参数和状态。.pth文件保存了模型的权重和各层的参数，可以方便地用于加载和恢复模型。通过保存模型为.pth文件，我们可以在需要时重新加载模型，并使用它进行预测或继续训练。 也就是说，pth是base，ckpt是进阶，safetensor是再进阶？知乎上这篇解释safetensor还不错。看了一些讲解，了解到，不同类型的模型、训练方法会自定义save和load的方法，文件名之类的也有差别，这种要具体情况具体分析。 修改模型之后，重新训练，结果还是像以前那样，只能回答第一句话，后面的话无了。其原因在哪儿？需要print以下tensor的情况是否存在之类的。目前的猜想是：首先，不可能是训练数据只有单轮对话的原因，因为这无非是输入长短的问题，不应该是一点输出都没有。在多轮对话中，一点结果都不返回的话，说明第二轮及以后的decode都是&lt;!i’m end&gt;这种东西；这个问题的原因，可能在于魔改模型烂了，M和z对后续输出造成了很坏的负面应用，这个方法不通；也有可能是ft的方法、量级不对。亦或者，一个原因是M和z使用、更新的顺序出问题，导致后续decode无法进行之类的。因而，目前首先需要print所有decode的结果，而非对话输入之后，print出make_context中raw_context的结果，发现decode的结果是直接无了。思考这个现象出现的原因，应该是魔改模型+ft导致模型本身的能力被破坏殆尽；训练数据中都是单轮对话的数据，因而无法decode出多轮对话的结果，它输入进去的都只是一个system+user的对话，过拟合导致只能输出一轮对话的结果，后面直接decode成end了。这个猜想的可能性是很大的。目前，不能确定自己魔改的代码在思路上是没有问题的，因而： 可能需要再仔细斟酌一下代码修改，需要查看这个decode过程中的tensor(句子、M、z)； 也有可能代码没问题，纯因为这个方法需就是烂，水出来的； 也有可能是需要大量数据喂进去才能看到效果。 也有可能，是safetensor向ckpt转化过程中损失了参数；因为似乎ckpt to safetensor是可以无损的，而反过来不行，因为safetensor是一种压缩、精简的格式； 查看M和z在forward过程中的数值更新：查看M和z的具体数字后，发现其过大，没收敛在一个固定范围内😡。但是、似乎、好像、Attn的结果，是M和z除；因而Attn不会受到太大影响才对？感觉脑子不够用了，一天能认真思考的时间好短捏。思考多了头晕。（tmd不是因为我节食了营养这一块儿没人给我补吧？）好吧，确定一下思路：修正M和z的更新公式，使得其稳定，需要参考另一个论文实现的开源项目，这个项目适配了llama和Gemma，应该是好的。 明日任务：修正M和z的更新公式，力求将其限定在一定范围内波动，避免数值溢出。 5.9查看上面说的开源项目llama版本的改动，发现其在初始化阶段只初始化了beta，且beta是一个和MHA的shape相匹配的tensor而非单独的数值。这样，每个头有一个参数，似乎更加符合论文中的做法： 1self.gate = nn.Parameter(torch.full((1, self.num_heads, 1, 1), 0.0)) modeling的代码中有past_kv，查看这个tensor是否存在、shape是否合理；按理来说这个应该是自增的。确实是随着token的逐步decode递增的，第二个维度是query_len，输入+prompt大差不差是这个数。又回顾了M和z的具体数值，发现主要是z过大，那么最简单的方法就是对z做norm，但是这个操作没出现在论文中，最好还是再分析一下代码是否有问题。再去看一下论文：原来z只是归一化的分母，它就应该是大的，无所谓；但是M不应该是大的，这个问题出现的原因，可能是代码实现中没有关注 “element-wise” 这个操作修改完之后，再次训练，预计下班后能训好。这次训练查看效果，还是重点关注两方面的内容： 对话是否正常 如果对话不正常，则查看M和z，尤其是M，需要控制其在一定范围内 M为什么会突发性的过大？这个M的公式上看，增量更新应该没问题，也许是代码写的不对？查看代码，感觉没什么问题，再仔细看一会儿，真没问题那就没办法了。训练中断，加载checkpoints-1000的参数看看能否多轮对话，似乎有多轮对话的能力；但是没有看M和z的具体数值。又改了代码。把beta改成了shape为[1, 1, self.num_heads, 1]的tensor，再次训练，看看效果。预计明天看看今天训的两个模型的效果和M、z的具体数值。 明日任务：查看两次修改的ft效果，明确是否是数据量不足的原因；思考限制M和z范围的方法； 5.10昨晚跑一半服务器断了，今早发现连不上服务器了，ssh也不行。静观其变，相机而动！继续看一下streaming-llm的论文和源代码，这个项目的star涨的好快，现在已经有6.2k了，看样子很有可取之处。这个论文的核心思想还是sparse attention机制，只不过是在观察了attention logits的数值分布特征之后，提出的针对这个特征的sparse方案。另：win11上配置ssh server的教程，就是nm几mb的东西为什么安装的那么慢？而且还安装失败？————好吧，似乎是梯子的问题，把梯子关了之后正常安装了。和pip换清华源之后不能开梯子install有异曲同工之妙。 streaming-llm对positio embedding也做了调整，感觉如果改qwen1的话会很麻烦，因为它的代码不明晰。 分析一下streaming-llm种modify_llama.py文件的思路： 12345678def apply_rotary_pos_emb_single(x, cos, sin, position_ids): # The first two dimensions of cos and sin are always 1, so we can `squeeze` them. cos = cos.squeeze(1).squeeze(0) # [seq_len, dim] sin = sin.squeeze(1).squeeze(0) # [seq_len, dim] cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] x_embed = (x * cos) + (rotate_half(x) * sin) return x_embed 这个函数的作用，顾名思义是进行RoPE；而“single”的意义在于，指定位置进行RoPE，而不是一整个raw context。这个函数的作用正好和论文中的 “根据cache进行位置编码” 相对应。通过这个函数，可以自由选定、构建token组进行RoPE。我也查看了qwen1中的RoPE，应该就是这样，没有问题。 下午nm给拉出去干杂活，说明天早起去那儿打下手现在去踩点😅，挤在出租车上有点晕；然后nm到中关村会议中心那儿啥事儿没干，蹲了一会儿出去找饭吃，吃了31的大盘鸡拌面，味道一般，比不上南京一根；然后吃一半打电话来说不用再回去了直接回家😅😅😅😅😅，纯dirty work了属于是。实习生牛马忍气吞声ing。 明日任务：当杂活、群演、牛马😅😅 5.11今日，在中关村会议中心当群演和迎宾。纯牛马。中午包饭，食堂比公司的好，比较有意思的一点就是酸奶居然都是一样的，而且都是摞起来给你自取。可惜之前吃了个卷饼和豆浆，不太饿了。开会议，无事发生。大佬讲的“低空经济”这个词儿有点意思。 下周任务：看ft效果；改模型 5.12再说一遍：我恨调休！我恨调休！放一天真的啥事儿也不想干，也不想出去玩、吃饭，纯在出租屋躺一天。晚上和SH、LSH、LT一起大乱斗，今天赢得比较多。ZZK从上海的滴滴实习跑到了北京的快手实习，5.10入的职。这几天和他聊了聊工作怎么样啥的。他是早上10:30，晚上9点还是10点来着。在西二旗那边。预期下周和他吃个饭，他本科就是北京的，也在百京实习过，因而游刃有余一点。聊到WXF、ZW的实习，询问过后发现他们还在找。唉、后端是不好活，ZXQ也在学校躺尸没找到实习…… 5.13再说一遍：我恨调休！我恨调休！早上测试了A800还连不上😅。接下来继续分析一下modify_llama.py文件的函数作用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131def llama_pos_shift_attention_forward( self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, # 这个参数应该是新增的 past_key_value: Optional[Tuple[torch.Tensor]] = None, output_attentions: bool = False, use_cache: bool = False,) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]: bsz, q_len, _ = hidden_states.size() #获取batch_size和query_len if self.config.pretraining_tp &gt; 1: # 是否并行，tensor parallel key_value_slicing = ( self.num_key_value_heads * self.head_dim ) // self.config.pretraining_tp query_slices = self.q_proj.weight.split( (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0 ) key_slices = self.k_proj.weight.split(key_value_slicing, dim=0) value_slices = self.v_proj.weight.split(key_value_slicing, dim=0) # 对query、key、value进行切分，目的是并行计算； 计算tensor的slice大小，然后对各个切片进行proj，然后结果concat起来 query_states = [ F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp) ] query_states = torch.cat(query_states, dim=-1) key_states = [ F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp) ] key_states = torch.cat(key_states, dim=-1) value_states = [ F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp) ] value_states = torch.cat(value_states, dim=-1) else: #不并行 简单的proj一下即可 query_states = self.q_proj(hidden_states) key_states = self.k_proj(hidden_states) value_states = self.v_proj(hidden_states) query_states = query_states.view( bsz, q_len, self.num_heads, self.head_dim ).transpose(1, 2) key_states = key_states.view( bsz, q_len, self.num_key_value_heads, self.head_dim ).transpose(1, 2) value_states = value_states.view( bsz, q_len, self.num_key_value_heads, self.head_dim ).transpose(1, 2) kv_seq_len = key_states.shape[-2] # key_states的seq_len，上面transpose一下之后，shape应该是[bsz, self.num_key_value_heads, q_len, self.head_dim] if past_key_value is not None: kv_seq_len += past_key_value[0].shape[-2] # 从这可以看出，past kv的shape应该也是[bsz, self.num_key_value_heads, q_len, self.head_dim]，或者至少倒数第二个是q_len？ cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len) # RoPEcos和sin参数 ### Shift Pos: query pos is min(cache_size, idx) # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids) query_states = apply_rotary_pos_emb_single(query_states, cos, sin, position_ids) ### 此处的shape应该没变 if past_key_value is not None: # reuse k, v, self_attention key_states = torch.cat([past_key_value[0], key_states], dim=2) value_states = torch.cat([past_key_value[1], value_states], dim=2) # 对past进行concat，结果应该得到q_len的states，因为当前的时间步只会处理当前的token，要得到完整的q_len还得要concat前面的cache。 past_key_value = (key_states, value_states) if use_cache else None ### Shift Pos: key pos is the pos in cache key_position_ids = torch.arange(kv_seq_len, device=position_ids.device).unsqueeze(0) key_states = apply_rotary_pos_emb_single(key_states, cos, sin, key_position_ids) ### 上面改的RoPE在此进行了应用 # repeat k/v heads if n_kv_heads &lt; n_heads GQA的做法，tensor shape是(batch, num_key_value_heads * n_rep, slen, head_dim) key_states = repeat_kv(key_states, self.num_key_value_groups) value_states = repeat_kv(value_states, self.num_key_value_groups) attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt( self.head_dim ) if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len): # 后两维的shape raise ValueError( f&quot;Attention weights should be of size &#123;(bsz, self.num_heads, q_len, kv_seq_len)&#125;, but is&quot; f&quot; &#123;attn_weights.size()&#125;&quot; ) if attention_mask is not None: if attention_mask.size() != (bsz, 1, q_len, kv_seq_len): raise ValueError( f&quot;Attention mask should be of size &#123;(bsz, 1, q_len, kv_seq_len)&#125;, but is &#123;attention_mask.size()&#125;&quot; ) attn_weights = attn_weights + attention_mask # upcast attention to fp32 attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to( query_states.dtype ) attn_output = torch.matmul(attn_weights, value_states) # (batch, num_key_value_heads * n_rep, slen, head_dim) 应该也是这个shape if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim): raise ValueError( f&quot;`attn_output` should be of size &#123;(bsz, self.num_heads, q_len, self.head_dim)&#125;, but is&quot; f&quot; &#123;attn_output.size()&#125;&quot; ) attn_output = attn_output.transpose(1, 2).contiguous() attn_output = attn_output.reshape(bsz, q_len, self.hidden_size) if self.config.pretraining_tp &gt; 1: # 如果并行，则切分后计算 attn_output = attn_output.split( self.hidden_size // self.config.pretraining_tp, dim=2 ) o_proj_slices = self.o_proj.weight.split( self.hidden_size // self.config.pretraining_tp, dim=1 ) attn_output = sum( [ F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp) ] ) else: attn_output = self.o_proj(attn_output) # 否则直接proj if not output_attentions: attn_weights = None return attn_output, attn_weights, past_key_value # 输出没啥好说的，标准输出罢了 这个函数的作用是，将前面的修改后的、带position_id的位置信息的RoPE应用到attention模块的forward上。从命名中可以看出这个用法。这个关键在于，对forward新增了一个position_ids参数，然后RoPE的函数重构一下，其他流程是和原生一样的。目前，没有在这个函数里面看见基于cache的RoPE，也没有看见sink attn的设置。 草！A800连不上的原因是安徽那边的看机器的人用root账户把东西搞烂了，现在正在修，环境啥的都没了，都要重新配置，真NM无语了，有这种事啊？ 在分析上面代码的过程中，突然对自己之前修改的model产生了质疑：tensor的shape是否正确？我需要查看原生的output tensor shape和修改后的output tensor shape是否一致。此外，还需要输出一下past_key_value的shape，这我之前咋没想到看呢？也许是model_qwen里面没有?好吧，之前看过，但是时间太长我给忘了，嘻🤗🤗。 1torch.Size([1, 407, 16, 128]) # past_key_value[0][0].shape 这个也许是k的shape，如果是[1][0]则是v的shape？ 而原生model，print attn_output的结果是： 12attn_output.shape:torch.Size([1, 404, 2048])attn_output.shape:torch.Size([1, 1, 2048]) 前面q_len一直是404，后面一直是1。但是这个没分开num_heads，捏妈我改的代码不会就是因为这个所以效果不好的吧？赶紧查看： 1attn_output before c_proj.shape:torch.Size([1, 419, 16, 128]) 虚惊一场，代码里面attn一直是这个形状，中间的merge后的形状是context_layer这个中间参数；modeling代码里面的shape也没问题。 modify_llama.py三个函数中的最后一个函数是： 1234567891011def enable_llama_pos_shift_attention(model): for name, module in reversed(model._modules.items()): if len(list(module.children())) &gt; 0: enable_llama_pos_shift_attention( module, ) # 一个递归，似乎是为了处理一个网络中的所有层；transformer的网络结构大抵是nn.module及其嵌套。 if isinstance(module, LlamaAttention): model._modules[name].forward = types.MethodType( llama_pos_shift_attention_forward, model._modules[name] ) # 找到网络中的LlamaAttention，替换forward函数 这个函数的作用很简单。 看下来之后，感觉论文中提到的sink和基于cache的RoPE似乎没有实现？此外，softmax的修改也没体现。 服务器上面的代码失去了，我这里没有备份；幸好一些关键代码不多，我还记得： safetensor逐个转化为ckpt文件： 12345678910111213141516from safetensors.torch import load_fileimport torchimport sysimport osimport globDEFAULT_CKPT_PATH = &#x27;Qwen-7B-Chat&#x27;CUDA_VISIBLE_DEVICES = &#x27;2,3,4,5,6,7&#x27; # 0、1卡已经部署了研究院那边sft好的模型的apidevice = &#x27;cuda&#x27;file_paths = glob.glob(os.path.join(DEFAULT_CKPT_PATH, &#x27;/*.safetensors&#x27;))for path in file_paths: weights = load_file(path, device=device) weights[&quot;state_dict&quot;] = weights torch.save(weights, os.path.splitext(path)[0] + &#x27;.ckpt&#x27;) 构建训练数据make_train_data.py：此代码在四月份的记录中已存，具体是4.28的记录 修改cli_demo.py中的加载模型部分： 1234567891011121314from transformers import AutoConfigfrom safetensors.torch import load_modelimport globconfig = AutoConfig.from_pretrained(&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem&quot;)model = AutoModelForCausalLM.from_config(config)DEFAULT_CKPT_PATH = &#x27;Qwen-7B-Chat-Mem&#x27;file_paths = glob.glob(os.path.join(DEFAULT_CKPT_PATH, &#x27;/*.ckpt&#x27;))CUDA_VISIBLE_DEVICES = &#x27;2,3,4,5,6,7&#x27;device = &#x27;cuda&#x27;for path in file_paths: model.load_state_dict(torch.load(path), strict=False)model.to(device) 明日任务：阅读kv_cache.py，吃透总体上的代码逻辑 5.14昨天夜里领导突然告诉我，让我到昌平区的那个地方的公司去帮忙，说那边任务紧急。然后就过去了，早上坐了一个多小时的地铁，9点才将将赶上到那个园区里面，早饭也没吃。到那儿，在找中心B座的时候遇见做数据的实习兄弟S，原来他也被派来了。俺们在园区里面迷了一阵子路，最后找到了。到那儿，给我们安排到一楼的工位，似乎是因为公司部门在三楼的工位不够了。说是有下载huggingface数据的任务，但是一楼工位又给我们申请了工作账号，用来派发数据校验审查的任务，通俗来说就是打赛博螺丝😅😅。这边的人倒是挺好的，给我们讲解怎么做，然后时不时来问问有没有遇到问题，但是跑大老远的紧急任务就是打赛博螺丝真是让人无大语。中午，也不知道有啥地方吃饭，公司在这边带我们的F大哥的话，我和S兄跟着人流，到了园区外面的饭馆。哪儿是一排面馆的聚集地，但是也卖盖浇饭什么的，总之是接地气的快餐。人很多，我点了辣子鸡丁盖浇饭，等了好久才吃上，味道和分量都不错。价格也比较便宜，比宣武门公司和出租屋那边都便宜，而且是现炒的，有锅气。吃完饭，买了瓶饮料，回去工位，勉勉强强睡了一阵子。下午继续打螺丝。临到下班问F大哥，数据集下载的硬盘在哪儿，大哥说明天给😅😅，合着今天来真是就打赛博螺丝啊！哎！宣武门那边工位上的键盘、被子、药、手机充电器都没带来！周边的绿化倒是挺好的，楼顶很高、楼很大、给人空旷开阔的感觉，毕竟是郊区。看了看周围的房价，发现自如上的不便宜。然后NJ找的58同城上的巨便宜，还是单间！不贵！比现在出租屋那边条件好太多了！我草tmd自如！！！如果要呆在这边时间长，那肯定是要搬家的。但是一切未定。问到底在这干多久，领导也说不确定😅😅😅😅😅😅。实习生牛马呗！快下班的时候告诉我明天回宣武门那边聊一下到底怎么做、后续做啥、是在哪边的公司😅😅。真是折腾人！不能今天正常上班跟我聊吗！怎么说呢，今天的经历，给了我很强的跑路理由，之前顾虑到沈老师师姐这边，总想着忍一下吧，忍忍过去了！但是今天发现，正常的研究都不打算让我做了，国企这边管理混乱、没有主线。那我跑起来是心安理得，心情反倒感到轻松了不少🤗🤗。琢磨琢磨后面能投哪儿的公司捏~~反正不想待百京了 明日任务：寄！开摆喽！ 5.15关于sink attn和cache RoPE的部分，实现似乎在kv_cache.py中，这里记录并分析一下这个代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import torchdef slice2d(x, start, end): return x[:, :, start:end, ...]def slice3d(x, start, end): return x[:, :, :, start:end, ...]def slice1d(x, start, end): return x[:, start:end, ...]# 这三个是对tensor的某个维度进行cut，似乎就是论文中对attn实行sink+window中window的部分DIM_TO_SLICE = &#123; 1: slice1d, 2: slice2d, 3: slice3d,&#125;class StartRecentKVCache: def __init__( self, start_size=4, # sink大小 4 + 508构成总window，论文里是这样写的，但代码实现好像是4+512=516？ recent_size=512, # window大小 k_seq_dim=2, v_seq_dim=2, # tensor的shape应该是[batch, num_heads, seq_len, head_dim] ): print(f&quot;StartRecentKVCache: &#123;start_size&#125;, &#123;recent_size&#125;&quot;) self.start_size = start_size self.recent_size = recent_size self.cache_size = start_size + recent_size self.k_seq_dim = k_seq_dim self.v_seq_dim = v_seq_dim self.k_slice = DIM_TO_SLICE[k_seq_dim] # 结合上面定义的函数和dict，这里确定seq所在的dim，从而选好slice函数 self.v_slice = DIM_TO_SLICE[v_seq_dim] def __call__(self, past_key_values): if past_key_values is None: return None seq_len = past_key_values[0][0].size(self.k_seq_dim) # 这行代码的作用是计算past_key_values中第一个元素的第一个子tensor的大小，并将结果存储在seq_len变量中。具体地说，它使用size方法来获取特定维度上的尺寸。例如，假设past_key_values[0][0]是一个形状为(3, 4, 5)的3D张量，且k_seq_dim的值为2，则该代码将计算第2个维度（从0开始计数）上的尺寸，即5，并将结果存储在seq_len变量中。 if seq_len &lt;= self.cache_size: # 如果目前的seq长度小于等于我们预先定义的sink+window的总大小，则不需要对past_key_values进行任何处理，直接返回。 return past_key_values return [ # 否则，我们需要对past_key_values进行处理，每个seq维度对应的kv都剪切sink+window的长度，其余都删去，最后concat起来 [ torch.cat( [ self.k_slice(k, 0, self.start_size), self.k_slice(k, seq_len - self.recent_size, seq_len), ], dim=self.k_seq_dim, ), torch.cat( [ self.v_slice(v, 0, self.start_size), self.v_slice(v, seq_len - self.recent_size, seq_len), ], dim=self.v_seq_dim, ), ] for k, v in past_key_values ] def evict_for_space(self, past_key_values, num_coming): # 逐出_for_space if past_key_values is None: return None seq_len = past_key_values[0][0].size(self.k_seq_dim) if seq_len + num_coming &lt;= self.cache_size: # 如果seq + 预备加入的token数量 小于等于cache大小 return past_key_values return [ [ torch.cat( [ self.k_slice(k, 0, self.start_size), self.k_slice( k, seq0._len - self.recent_size + num_coming, seq_len # 和上面的__call__相比，不同在于 + num_coming ), ], dim=self.k_seq_dim, ), torch.cat( [ self.v_slice(v, 0, self.start_size), self.v_slice( v, seq_len - self.recent_size + num_coming, seq_len # 同上 ), ], dim=self.v_seq_dim, ), ] for k, v in past_key_values ] def evict_range(self, past_key_values, start, end): if past_key_values is None: return None seq_len = past_key_values[0][0].size(self.k_seq_dim) # 这一步三个函数都是一样的。 assert start &lt;= end and end &lt;= seq_len return [ [ torch.cat( [ self.k_slice(k, 0, start), self.k_slice(k, end, seq_len), # 这个方法应该是自定义从哪儿到哪儿的方法。 ], dim=self.k_seq_dim, ), torch.cat( [ self.v_slice(v, 0, start), self.v_slice(v, end, seq_len), ], dim=self.v_seq_dim, ), ] for k, v in past_key_values ] 阅读完毕之后，发现还是有点搞不懂他这个代码是怎么运行的，还得看一下它这个运行的run_streaming_llama.py的代码。需要明晰的是，这个总体上的思路应该是：写class、method满足基于cache进行RoPE的需求、动态维护一个sink + window的cache需求，然后在llama模型中运用这个方法，在生成的forward过程中，替换成sink + window 查看一下官方实现的以streaming形式运行llama的代码： 1234567891011121314151617181920212223242526import warningswarnings.filterwarnings(&quot;ignore&quot;)import torchimport argparseimport jsonimport osimport timeimport reimport sysfrom tqdm import tqdmfrom streaming_llm.utils import load, download_url, load_jsonlfrom streaming_llm.enable_streaming_llm import enable_streaming_llm@torch.no_grad()def greedy_generate(model, tokenizer, input_ids, past_key_values, max_gen_len): # qwen里面的QwenLMHeadModel里面有generate函数，llama里面没有，这个greedy_generate应该是类似qwen里面的功能？ outputs = model( # 这个关键应该是在input_ids这个参数上？通过这个参数来进行..？ input_ids=input_ids, past_key_values=past_key_values, use_cache=True, ) past_key_values = outputs.past_key_values pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1) 这里我去看了一下qwen里面的logits的相关数据： 12345678910111213141516171819202122232425262728293031323334print(lm_logits)print(lm_logits.shape)print(lm_logits[..., :-1, :])print(lm_logits[..., :-1, :].shape)tensor([[[ 7.6367, 4.5312, 4.6406, ..., -2.9141, -2.9141, -2.9141], [ 7.6797, 4.5391, 4.5664, ..., -2.9512, -2.9512, -2.9512], [ -5.9531, -11.9766, -8.3281, ..., -1.5859, -1.5869, -1.5869], ..., [ -4.7500, -3.5156, -4.6289, ..., -2.9043, -2.9043, -2.9062], [ 3.0527, 6.7930, -0.4011, ..., -3.6289, -3.6289, -3.6289], [ 3.1152, 7.9453, 0.1495, ..., -2.8750, -2.8750, -2.8750]]], device=&#x27;cuda:0&#x27;, dtype=torch.float16)torch.Size([1, 404, 151936])tensor([[[ 7.6367, 4.5312, 4.6406, ..., -2.9141, -2.9141, -2.9141], [ 7.6797, 4.5391, 4.5664, ..., -2.9512, -2.9512, -2.9512], [ -5.9531, -11.9766, -8.3281, ..., -1.5859, -1.5869, -1.5869], ..., [ -7.8203, -4.4961, -4.9688, ..., -0.6299, -0.6323, -0.6323], [ -4.7500, -3.5156, -4.6289, ..., -2.9043, -2.9043, -2.9062], [ 3.0527, 6.7930, -0.4011, ..., -3.6289, -3.6289, -3.6289]]], device=&#x27;cuda:0&#x27;, dtype=torch.float16)torch.Size([1, 403, 151936])tensor([[[ 1.8477, 1.7461, 1.9453, ..., -2.3418, -2.3418, -2.3418]]], device=&#x27;cuda:0&#x27;, dtype=torch.float16)torch.Size([1, 1, 151936])tensor([], device=&#x27;cuda:0&#x27;, size=(1, 0, 151936), dtype=torch.float16)torch.Size([1, 0, 151936]) 一开始seq_len维度的是query的长度，然后[…, :-1, :]这个操作正如之前所说的，减了一个维度，只把最后一个时间步作为预测结果。在第二个时间步之后，可以看到print的结果，seq这个维度是1，削减之后是0；之后就一直是[1][0]循环了。[…, :-1, :]就纯没有结果。那么，也就是说，内部这个lm_logits在第一步输入的时候会进行一个整的计算，然后到了生成token的时候，这个lm_logits就是一个单词表长度的概率分布了，直接从这个概率softmax再decode啥的出一个token。那么pred_token_idx就是根据logits而decode出来的token了。 12345678910111213141516171819202122232425262728293031generated_ids = [pred_token_idx.item()] # 这里取出预测出来的idpos = 0for _ in range(max_gen_len - 1): # 上面那个似乎是单独预测第一个token？这里预测后面的token直到到达模型设置的max_gen_len为止。为啥要分开呢？难道是为了初始化list？那为啥不用空list呢？ outputs = model( input_ids=pred_token_idx, past_key_values=past_key_values, use_cache=True, ) past_key_values = outputs.past_key_values pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1) generated_ids.append(pred_token_idx.item()) # 新decode出来的token对应的id加入了 generated_text = ( tokenizer.decode( # decode成token， id -&gt; token generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True, spaces_between_special_tokens=False, ) .strip() .split(&quot; &quot;) ) now = len(generated_text) - 1 # 好吧，单独生成第一个token是为了这一步的循环 if now &gt; pos: print(&quot; &quot;.join(generated_text[pos:now]), end=&quot; &quot;, flush=True) pos = now # 预测出终止符 if pred_token_idx == tokenizer.eos_token_id: breakprint(&quot; &quot;.join(generated_text[pos:]), flush=True)return past_key_values # 下面那个method需要这个返回值。迭代的kv更新也需要past_key_values这个参数。 总的来说，这个method是一个获取model输出概率并进行greedy decode的过程。没有看见之前改的kv cache在此有应用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@torch.no_grad()def streaming_inference(model, tokenizer, prompts, kv_cache=None, max_gen_len=1000): past_key_values = None for idx, prompt in enumerate(prompts): # 迭代式单轮对话？ prompt = &quot;USER: &quot; + prompt + &quot;\\n\\nASSISTANT: &quot; print(&quot;\\n&quot; + prompt, end=&quot;&quot;) input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids # 通过tokenizer把prompt转成pt（pytorch）形式的tensor，然后从中提取input_ids input_ids = input_ids.to(model.device) seq_len = input_ids.shape[1] # 第二个维度是seq_len if kv_cache is not None: # 如果存在kv cache，即不是第一个token space_needed = seq_len + max_gen_len past_key_values = kv_cache.evict_for_space(past_key_values, space_needed) # 在这里实现past_key_values的sink + window的cache的维护。 past_key_values = greedy_generate( # 调用上面那个method进行greedy decoding model, tokenizer, input_ids, past_key_values, max_gen_len=max_gen_len )def main(args): model_name_or_path = args.model_name_or_path model, tokenizer = load(model_name_or_path) test_filepath = os.path.join(args.data_root, &quot;mt_bench.jsonl&quot;) print(f&quot;Loading data from &#123;test_filepath&#125; ...&quot;) # 导入模型和数据 if not os.path.exists(test_filepath): download_url( &quot;https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl&quot;, args.data_root, ) os.rename(os.path.join(args.data_root, &quot;question.jsonl&quot;), test_filepath) list_data = load_jsonl(test_filepath) prompts = [] for sample in list_data: prompts += sample[&quot;turns&quot;] if args.enable_streaming: kv_cache = enable_streaming_llm( model, start_size=args.start_size, recent_size=args.recent_size )# 这个method的作用大概是：替换llama中的RoPE以及forward部分，从而可以实现sink + windows，以及cache RoPE的功能，然后会返回一个应用了上面那些修改的kv cache，即sink + window else: kv_cache = None # 如果不启用streaming，为啥kv_cache设置为none？原因在于，单轮对话不需要cache，如果是多轮，则有history参数传入，作为query加到我们当前轮的对话中，也就是说kv cache没必要存。 # 但是，在streaming的情况下，似乎可以把max_gen_len设置成很大，一直无限地对话下去。 streaming_inference( model, tokenizer, prompts, kv_cache, )if __name__ == &quot;__main__&quot;: parser = argparse.ArgumentParser() parser.add_argument( &quot;--model_name_or_path&quot;, type=str, default=&quot;lmsys/vicuna-13b-v1.3&quot; ) parser.add_argument(&quot;--data_root&quot;, type=str, default=&quot;data/&quot;) parser.add_argument(&quot;--enable_streaming&quot;, action=&quot;store_true&quot;) parser.add_argument(&quot;--start_size&quot;, type=int, default=4) parser.add_argument(&quot;--recent_size&quot;, type=int, default=2000) args = parser.parse_args() main(args) 实现streaming-llm，主要是对模型的kv_cache及其forward机制进行修改，对model的forward也会进行修改；此外，在generate上也需要进行相应的调整。而像是qwen中的cli_demo.py，其对话的print之类的东西也要调整，总的来说感觉这个调整是全面的，而非单一改算法。 那么，项目全部阅读完毕之后，确定一下modeling_qwen修改的思路：总的来说，各种文件大抵都可以结合modeling_qwen.py的源文件相关部分进行一些调整；greedy_generate这个修改，在qwen中应该是chat&#x2F;chat_stream，而非generate 中午，和Z哥一起去吃沙县小吃，颇有感慨。想起自己初中的时候，很喜欢吃小区门口那家沙县小吃的肉丝饭，11块，经常去吃，有时候家里做了饭，但馋那一口，也会去吃；记得有一次，我已经过世的奶奶做好了饭叫我吃，我说不吃，去买沙县小吃吃，买回来她看到，说“就这个东西啊”。哈哈，她也没有逼我吃家里的饭。倒是现在，想吃一口，终究也是吃不上了。上了高中，由于学业压力大，加之没啥钱，有钱会去上网吧，似乎就再也没吃过那家沙县。大学再去，发现已经关门了，吃其他沙县也找回不了记忆里的感觉。想来，沙县或许是我初中生活的一个缩影，有和朋友在沙县吃的记忆、有坐在电脑前打游戏吃沙县的莫名其妙的很舒适的感觉。然而一切都已过去，我也找不回那种懵懂、纯粹快乐的感觉。 晚上，线上会议，纯看客。开完和LT，NZY和她对象打大乱斗。捏妈，向N开炮似乎给人姑娘吓着了，后面麦都没啥声音😨😨。 明日任务：将streaming-llm实现到qwen中 or 接手今早才跟我说的多路召回检索优化项目 此处记录streaming-llm应用到qwen上的一些具体修改 enable_streaming_llm.py 123456789101112# 在elif新增： elif &quot;qwen&quot; in model.config.model_type: # qwen结构和llama类似，应该copy llama的设置就行。 k_seq_dim = v_seq_dim = 1 from .pos_shift.modify_falcon import ( enable_falcon_pos_shift_attention, ) enable_falcon_pos_shift_attention(model)# qwen结构和llama类似，但是llama中 attn_output.size的shape是(bsz, self.num_heads, q_len, self.head_dim)# 而正如之前所提到的，qwen里面应该是(bsz, q_len, self.num_heads, self.head_dim)，因而seq_dim应该是1# 此外，根据目录，from streaming_llm.kv_cache import StartRecentKVCache 这种或许要改成 from .kv_cache import StartRecentKVCache kv_cache.py + utils.py 似乎不需要改动；kv_cache.py是感觉没有需要改的；utils.py是下载、加载数据的class，暂时没打算用这个。而且应该可以照搬过来用 pos_shift&#x2F;modify_llama.py 这个文件夹下面，针对每个model文件都写了一个modify_xxx.py，那么qwen需要在这下面写个文件modify_qwen.py，应该是在llama的基础上进行修改即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202# 尚未做完，qwen1的model文件太混乱了，有很多多此一举的举动，有很多操作直接使用pytorch的原生方法就可以直接完成，它非要定义一个函数；传参什么的跟llama很不相同；而qwen1.5的model文件就和llama很近似。 # 暂时没时间改了，工作内容有变。效果尚未验证。import mathfrom typing import Optional, Tupleimport torchfrom torch import nnimport torch.utils.checkpointimport torch.nn.functional as Ffrom ..modeling_qwen import ( QWenAttention, _rotate_half, apply_rotary_pos_emb,)import types__all__ = [&quot;enable_qwen_pos_shift_attention&quot;]def apply_rotary_pos_emb_single(x, cos, sin, position_ids): # The first two dimensions of cos and sin are always 1, so we can `squeeze` them. cos = cos.squeeze(1).squeeze(0) # [seq_len, dim] sin = sin.squeeze(1).squeeze(0) # [seq_len, dim] cos = cos[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] sin = sin[position_ids].unsqueeze(1) # [bs, 1, seq_len, dim] x_embed = (x * cos) + (_rotate_half(x) * sin) return x_embeddef qwen_pos_shift_attention_forward( self, hidden_states: Optional[Tuple[torch.FloatTensor]], rotary_pos_emb_list: Optional[List[List[torch.Tensor]]] = None, layer_past: Optional[Tuple[torch.Tensor]] = None, attention_mask: Optional[torch.FloatTensor] = None, head_mask: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, output_attentions: Optional[bool] = False, use_cache: Optional[bool] = False,) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]: bsz, q_len, _ = hidden_states.size() mixed_x_layer = self.c_attn(hidden_states) query, key, value = mixed_x_layer.split(self.split_size, dim=2) query = self._split_heads(query, self.num_heads, self.head_dim) key = self._split_heads(key, self.num_heads, self.head_dim) value = self._split_heads(value, self.num_heads, self.head_dim) past_len = 0 if layer_past is not None: past_len = layer_past[0].shape[1] if rotary_pos_emb_list is not None: # cur_len = query.shape[1] if len(rotary_pos_emb_list) == 1: rotary_pos_emb = rotary_pos_emb_list[0] rotary_pos_emb = [i[:, -past_len:, :, :] for i in rotary_pos_emb] rotary_pos_emb = (rotary_pos_emb,) * 2 q_pos_emb, k_pos_emb = rotary_pos_emb # Slice the pos emb for current inference query = apply_rotary_pos_emb(query, q_pos_emb) key = apply_rotary_pos_emb(key, k_pos_emb) else: query_list = [] key_list = [] for i, rotary_pos_emb in enumerate(rotary_pos_emb_list): rotary_pos_emb = [i[:, -past_len:, :, :] for i in rotary_pos_emb] rotary_pos_emb = (rotary_pos_emb,) * 2 q_pos_emb, k_pos_emb = rotary_pos_emb # Slice the pos emb for current inference query_list += [apply_rotary_pos_emb(query[i:i+1, :, :], q_pos_emb)] key_list += [apply_rotary_pos_emb(key[i:i+1, :, :], k_pos_emb)] query = torch.cat(query_list, dim=0) key = torch.cat(key_list, dim=0) if self.use_cache_quantization: key = quantize_cache_v(key.permute(0, 2, 1, 3), bits=8, qmin=self.cache_qmin, qmax=self.cache_qmax) value = quantize_cache_v(value.permute(0, 2, 1, 3), bits=8, qmin=self.cache_qmin, qmax=self.cache_qmax) if layer_past is not None: past_key, past_value = layer_past[0], layer_past[1] if self.use_cache_quantization: # use_cache_quantization: # present=((q_key,key_scale,key_zero_point), # (q_value,value_scale,value_zero_point)) key = (torch.cat((past_key[0], key[0]), dim=2), torch.cat((past_key[1], key[1]), dim=2), torch.cat((past_key[2], key[2]), dim=2)) value = (torch.cat((past_value[0], value[0]), dim=2), torch.cat((past_value[1], value[1]), dim=2), torch.cat((past_value[2], value[2]), dim=2)) else: # not use_cache_quantization: # present=(key,value) key = torch.cat((past_key, key), dim=1) value = torch.cat((past_value, value), dim=1) if use_cache: present = (key, value) else: present = None key_size = key[0].size(2) if self.use_cache_quantization else key.size(1) if key_size &gt; self.seq_length and self.use_logn_attn and not self.training: if self.use_cache_quantization: seq_start = key[0].size(2) - query.size(1) seq_end = key[0].size(2) else: seq_start = key.size(1) - query.size(1) seq_end = key.size(1) logn_tensor = self.logn_tensor[:, seq_start:seq_end, :, :].type_as(query) query = query * logn_tensor.expand_as(query) if ( self.use_flash_attn and flash_attn_unpadded_func is not None and not self.is_fp32 and query.is_cuda ): q, k, v = query, key, value attn_output = self.core_attention_flash(q, k, v, attention_mask=attention_mask) else: key_size = key[0].size(2) if self.use_cache_quantization else key.size(1) if query.size(1) == key_size: causal_mask = torch.tril( torch.ones((key_size, key_size), dtype=torch.bool, device=query.device) ).view(1, 1, key_size, key_size) else: causal_mask = None query = query.permute(0, 2, 1, 3) if not self.use_cache_quantization: key = key.permute(0, 2, 1, 3) value = value.permute(0, 2, 1, 3) if ( causal_mask is None and self.use_flash_attn and flash_attn_unpadded_func is not None and not self.is_fp32 and not query.is_cuda ): raise Exception(_ERROR_INPUT_CPU_QUERY_WITH_FLASH_ATTN_ACTIVATED) if not self.use_cache_quantization and SUPPORT_TORCH2: if attention_mask is not None: attention_mask = attention_mask.expand(-1, -1, query.size(2), -1) if causal_mask is not None: attention_mask = attention_mask.masked_fill(~causal_mask, torch.finfo(query.dtype).min) else: attention_mask = causal_mask attn_output = F.scaled_dot_product_attention( query, key, value, attn_mask=attention_mask ).transpose(1, 2) attn_weight = None else: attn_output, attn_weight = self._attn( query, key, value, causal_mask, attention_mask, head_mask ) context_layer = self._merge_heads( attn_output, self.num_heads, self.head_dim ) # print(&#x27;attn_output before c_proj.shape:&#123;&#125;&#x27;.format(attn_output.shape)) attn_output = self.c_proj(context_layer) # print(&#x27;attn_output.shape:&#123;&#125;&#x27;.format(attn_output.shape)) outputs = (attn_output, present) if output_attentions: if ( self.use_flash_attn and flash_attn_unpadded_func is not None and not self.is_fp32 ): raise ValueError(&quot;Cannot output attentions while using flash-attn&quot;) elif not self.use_cache_quantization and SUPPORT_TORCH2: raise ValueError(&quot;Cannot output attentions while using scaled_dot_product_attention&quot;) else: outputs += (attn_weight,) return outputsdef enable_qwen_pos_shift_attention(model): for name, module in reversed(model._modules.items()): if len(list(module.children())) &gt; 0: enable_llama_pos_shift_attention( module, ) if isinstance(module, LlamaAttention): model._modules[name].forward = types.MethodType( llama_pos_shift_attention_forward, model._modules[name] ) 5.16今天继续尝试将streaming-llm适配qwen1，但是进展不理想，具体内容如上面的代码注释所示。问题出现在streaming-llm的核心代码，其中使用的基于cache的RoPE在qwen中应用难度较大。因为qwenattention的forward中没用position_id这个参数，于是去参考另一个modify_falcon.py文件，因为这个的model文件也是没有position id的。但是，结合其modify文件和model文件之后，没发现有很明显的应用基于cache的RoPE做法。仅仅是基于past len做了一些调整。或许，modeling_qwen的修改，在attention部分，应该也是像falcon中进行past_len这个参数的设置和应用。然后，对于kv_cache.py这个文件直接使用，然后再改写chat方法，基于改写后的chat方面写cli_demo.py。正如之前所说，这个流程应该是一个综合的过程。或许qwen1里面只要应用kv_cache.py对它这个模型的kv_cache进行修改后，自动RoPE就是水到渠成的基于cache了。 明日任务：应用今天的思路，修改qwen 5.17把RoPE中cur_len的部分，参照modify_falcon.py中，修改成了past_len。但是应该最好还是qwen1.5修改为佳。这个项目还有一个问题就是greedy decoding，没有温度、随机性啥的。 无奈了，终究不能搞长文本了。领导要看到效果，立刻转战多路召回检索优化。本以为逐渐稳定步入正轨，终究是一厢情愿。人在江湖，身不由己！ 看了项目已经写好的基于llamaindex + fastapi的代码，但是因为没有readme，代码也没用注释，所以看的一头雾水。 下周任务：人在江湖噜~~研究多路召回的实现，从读懂代码开始！没开发规范真是烦人！草台班子说是！ 5.18 - 5.19周六，本来和ZZK越好下午三点半在北京动物园碰面，结果我一点睡午觉睡过了，还是他打电话过来我才醒。急急忙忙地赶过去，已经四点半了。他去买了饮料，然后我们刷票进动物园。进熊猫馆的时候，发现ZZK买的是不带熊猫馆的票，结果他又买了一份，然后尝试退之前的票，失败；尝试退刚买的票，失败；因为两张票都已经被实际上使用过了🤣🤣，铸币吧。下午，这个时间动物们都没什么精神，熊猫趴在那儿睡觉，看不见正脸。有好长的队在排一个好像是网红熊猫的馆，我们嫌太长就没凑热闹。幸运的是，在熊猫馆里面逛了一阵子，正好遇见一个大熊猫打盹结束出来逛逛，拍下来了。之后，在动物园里面其他地方逛，边逛边和ZZK聊天。看了鹦鹉、狼、羚羊、北极熊啥的。都是懒趴趴的。走到大概六点多，逛累了，于是我们走去德凯mall吃一家绿茶餐厅，好像是杭州菜😅，真是明知美食荒漠，偏向荒漠行呀。点了荔枝虾球和抹茶饼两个甜品，可惜这个虾球很一般，不是我记忆力高三暑假和ZZC一起去杭州在新周记吃的那种。招牌菜一个糖醋口的排骨，难吃😅；烤鸡、烤肉片、竹笋这三道菜中规中矩，但这tm好像也不是杭州菜，无语子……铸币ZZK一开始还想点西湖醋鱼，幸好给我拦住了。之后，在商场-1层的超市逛了逛，然后准备回去了。在luckyin买了两杯咖啡，中途聊到WXF，和他打电话，约了下周俺们三一起去爬八达岭长城噜。晚上到出租屋，和LSH、LT、SH打游戏，十点多的时候把咖啡喝完了。结果，晚上，不知道是热的还是因为咖啡，睡不着，开空调舒服点，但是也睡不着。迷迷糊糊到三点多似乎才睡着，结果六点多好像就醒了。捏妈的，一上午都状态不太对，还好不上班！周日，混。改了简历，投了TX实习。晚上和LSH、SH、LT、NZY、WS打大乱斗。感觉得控制一下自己的脾气的，唉，严于律人宽以待己。感觉没有什么外卖点了，附近的外卖没有想吃的。不知道什么情况，提不起劲，愁啊！ 、 5.20早上看昨天投的结果，秒挂了尼玛的，简历秒挂！继续看项目代码，似乎有点明白结构和用法了。下午发现被tx捞了，捏妈明天面试。 似乎懂了一点这个项目的结构。llamaindex似乎可以直接导入预先构建的索引，然后设置为model chat的参数，就可以实现rag。那么，我就要在这个index的生成上做手脚？具体的明天再说吧。 晚上他们打游戏，我没去。看leetcode上的hot 100中的最大正方形面积，写了一种解法，结果和之前的一样，都只打败了5%的人。看来要看题解了，唉……给SH开了个腾讯会议，讲了一下LLM的知识和细节，给他产品经理AI方向的补一下盲。 明日任务：多路召回 5.21测试项目里面fastapi的功能，发现这个吊项目基本上跑不通，document的向量化以及基于知识库的chat都nm跑不通。问开发的，也支支吾吾的，草！这还让我加班这周把这多路召回跑通呢？这不纯坑人呢嘛！下午有面试，中午回出租屋，发现门吸到了，安上之后，似乎有点效果，但比较麻烦；美美睡了一觉。2.30面试，半小时结束，面试官摄像头都没开。让我写二叉树先序遍历，捏妈的非递归形式没写出来，还是得刷题！感觉是kpi面，草他妈滴！面完，吃了个沙县小吃鸭腿饭，然后回班上干活。 全是坑啊兄弟们！没有开发规范，没有产品文档，我真是操了！ 一个现在才明白的小知识: 12345678# a/b/c.py中# .代表b目录，可以./xxx可以访问b目录下所有文件；from . import是从b目录引入# ..代表a目录# power_system_aiqa-develop/server/configs/db_config.py中local_embedding_model_path = &quot;../embed_models/m3e&quot;# 如果在server下的文件里面，运行程序，间接调用了这个config，无论嵌套多少回，都会以pwd = server进行上级目录寻址和返回；也就是说，这个情况下则会访问power_system_aiqa-develop/embed_models/m3e 明日任务：多路召回 5.22 - 5.23没啥好说的，改那个项目的多路召回部分。加班改。发现没跑通的问题在于，似乎是官方实现的代码有缺陷，chromaDB转化为vector store的时候转化不完全&#x2F;没法正确转化，因而进行检索的时候是空集。只能写死了，指定文档库的文件夹，然后直接从文档转化为vector index然后进行后续操作。实际上，更精细的操作，还需要直接从document加载成node，然后像是summary index、graph index都可以从这个node创建。这样，就可以构建不能由vector index直接转化的index。后续，可以在chat里面加载各种index，构建多样化的retriever，然后整合起来。 明日任务：多路召回 5.24早上改retriever，之前其他类型的index to retriever的部分没有跑通，可能是不兼容或者数据格式不对？开了个会。发现在国企里面开发只需要做应声虫就可以了，提的意见和想法人家都当没听到。纯按照先入为主的理解在聊、派任务。反正我实习生不背锅，你咋说我咋做呗！反正是准备跑路了，这边本来说的卡现在也没了，科研也不让做了，做开发还nm没人带，没有培养机制，还nm催进度。我的评价是草台班子一拖四。 回顾这段时间，本来刚来的时候和沈老师那边会开会，也有一些指导，但是渐渐会也不开了。本来说沈老师那边的学生们能一起聊一聊，结果那边的学生也就开了一两次会，就再也没来。由于距离遥远，感觉沟通的时候也很费劲。总而言之，刚开始的时候还兢兢业业，慢慢发现这边是个草台班子、研发队伍和管理机制混乱、缺陷较大。虽然学到了点东西，但是感觉不多。需要一个新环境、正式的、有明确培养机制的环境。 预计六月初辞职，然后面面其他的吧唉。总归是一步慢、步步慢。理想情况下是一月份就开始实习，然后4月辞职或是请假什么的投暑期实习，然后中途休息一下、旅个游啥的，然后暑期实习回来投秋招。但是现在，无可奈何地晚了两个月，只能步步紧逼，却又总是赶不及……回顾从高中毕业到现在的时间，因为懒惰、愚蠢而导致的信息差，错过了许多机会、有许多可以更好的选择没有做出。悲叹没有意义了，只能吃更多的苦…… 向项目里面加入了document summary retriever，这个需要调用llm解析文档，因而慢，或许换成embedding会好一些。换成embedding的写法： 1234retriever = DocumentSummaryIndexEmbeddingRetriever( doc_summary_index, # similarity_top_k=1,) 但是没有测试，下周再搞！llm解析时运行时报错： 1ValueError: invalid literal for int() with base 10: &#x27;Doc&#x27; 下周任务：多路召回 5.25 - 5.26前两天，想起ZZK的推荐，在美团的小象超市上买了康师傅方便面、一桶酸奶、一包王中王火腿、一个2L冰红茶。周六，下雨，预计的长城之旅移到明天。在出租屋歇着，玩玩垃圾页游，中午点了鸡公煲，油油的，吃到一半，开了一包前两天在美团小象超市上买的康师傅方便面加了进去，吃撑了。下饭的片是《哆啦A梦：大雄的宇宙小战争》。下午和LT、SH、WS、NZY打了大乱斗。晚上，还吃那个鸡公煲。这次感觉不太行了，也许是我终于吃腻了，下饭的片是《哆啦A梦：大雄与机器人王国》，但是只看了大概三分之一？今天一天爽喝冰镇冰红茶，简直就是仙酿！周日，早起，去北京北坐高铁到八达岭。与ZZK、WXF汇合。书包里面带了昨晚在小象超市买的三个三明治和一个面包，以及三瓶盐汽水，一瓶大瓶怡宝。到站饿了先吃一个面包，在上长城的路上经过商区，看见有肯德基、蜜雪冰城……蜜雪冰城稍微贵一两块钱，神！但是饮料带够了，没买。昨晚还买了两板酸奶，但是忘带了，嘻嘻。过了九曲十八弯的安检，又走了一段，才验票进入城墙。向上爬。或许是由于昨天下了一天的雨、亦或许是因为处于山林之间，长城上的北风很大，在城墙口吹的很凉爽，足以抵消烈日。城墙上人很多，慢慢地走、慢慢地爬。可惜的是，ZZK似乎因为低血糖，爬了一阵子就撑不住，发晕呕吐；也有可能是爬长城前刚吃东西所以运动导致胃部不适应？总之，在爬到的好汉石那里，大概是北四段？（总共是12段），我们必须下去了。花了80r一人买了下山的缆车、抑或是过山车的票？哐哐哐一顿冲，没几分钟就到出口了。在出口，看看了一下黑熊，这个出口正好是“八达岭黑熊乐园”。说是乐园，只有三四只小黑熊懒懒的趴在那里晒太阳。坐上了回去的公交，把预定的两点多的高铁票退了。唉，这两天为了这长城的来回，推了三回票，-11r。公交一个多小时后到了啥门，忘了，我也不想去瞅一眼，总而言之是在恭王府附近。在附近找了饭店，排了队，一点多才吃上饭，饭店名字好像是长亭酒馆。点了韭菜炒虾米，炒一个啥菇，这俩菜是家里的口味。还有牛杂煲、生菜、小酥肉，我的评价是团购经典套餐。本来点了羊肉串，结果上来发现nm是小串，而且nm还是酸的，给退了。吃完，WXF去看看他的另一些同学。我和ZZK一起，沿着后海逛，逛到了地铁站，聊聊天，蛮惬意的，最后分道扬镳，大概四点多，我回到了出租屋。晚上要和ZYR、LF一起吃饭，所以六点开始睡觉补一补。本来预计的是九点吃海底捞，但是LF七点多就能回来，不是他之前说的大概九点，因而又变回涮肉。八点出头，我们就在西直门的聚宝源汇合了。吃涮肉的过程中，聊了聊各自的经历、心态什么的，也聊了聊八卦，还有一些不能言说的东西。吃完饭已经是十点多了，我们骑着自行车，去人民大学站的地铁站。欢乐的时光总是短暂的，唉！11点多，回到了出租屋。明天又要上班了唉！ 5.27加拿大的后端实习生入职了。在本地上测试了llamaindex中其他类型的index加入到多路召回里面，但是由于调用llm消耗过大、似乎与非llm的检索不能整合等原因，总之是整合失败了。看官方文档的时候，突然看到，用一个index去生成不同的retriever，打开了另一个思路。于是基于vector index，又加了两个retriever。至于更多的，不是不想加，而是都需要llm或者之类的，总之是定制化支持程度不高，无法实现或是实现的价值不大。但是无论如何，多路召回的名头总算不是虚名了，目前是四路召回。其中，三路召回是和embeddding模型深度绑定的，后面优化应该就是调整embedding。还有一路deep memory，似乎自己支持基于数据的训练。无论如何，再优化的话需要搞好训练数据集。目前，这个多路召回还是本地，和知识库的联动没做好；因而，后端这部分的业务，还有需要做的就是把这个api整体上跑通。预计明天还是做这个东西。看了leader和teacher分享的RAG方面的推文与项目，理解更深了。 明日任务：阅读RAG相关论文&#x2F;后端api实现与修改 5.28继续测试本地多路召回的效果，基本已经定型了。晚上开会说，差不多可以推进在线的了。也就是说要调通api刷了两道leetcode 明日任务：调整在线api 5.29记录一下查看端口进程的命令： 1234# 查看端口进程netstat -anp | grep 端口号# or netstat -tnlp | grep 端口号 尝试修复在线多路召回的API，正如之前定位的那样，问题代码应该出在这两行： 12chroma_collection = chromadb_client.get_collection(kb_id)vector_store = ChromaVectorStore(collection_name=kb_name, collection=chroma_collection, host = &#x27;localhost&#x27;, port = 8000) collection显示doc是存在的，但是dict最后两个key显示的是none： 1&#x27;data&#x27;: None, &#x27;uris&#x27;: None vectorstore读取后，显示collection为空。又去GitHub issue上看了一下，终于明白了解决思路：首先，这个问题其实不是vector store的问题，而是bm25计算的问题。在llama index中，bm25计算需要的是nodes，也只能通过nodes进行bm25的构建，并没有适配从vector store转化到bm25的api，这是源码本身的缺陷。那么，解决思路也就很明确了：要么自己去改源码，适配这个转化；要么就搞一个返回document的函数，然后在bm25创建前面读取这个返回的document，再构建nodes传入。出于方便和稳定性考虑，还是采取后者，当然这种做法会浪费一些时间，因为预先构建的切片不能使用，还要再构建一遍。修改之后，api可以跑通了。但是时间花费很大，不知道是不是因为gpu资源不足的原因。可以看到，生成query、SYNTHESIZE、LLM三步花费时间巨多。比较奇怪的是，在之前的步骤里面，直接从本地文档读取的话，反而不慢。明明这修改后的代码只是额外生成了一次bm25，但是时间却增加了好多。难道是因为后续一系列数值要重新计算、结构要重新生成？看来后续有很大的优化空间；此外，很有意思的一点是，这个用于bm25的、由document转化而来的nodes，其数据结构和chroma直接读取的collection似乎很相似，前者似乎是List[Dict{}]，而后者似乎是Dict{List[]}；没有细看，但或许可以写一个转化函数，直接转化成nodes的数据格式导入bm25计算。 晚上，和SH、LT打大乱斗捏。 明日任务：似乎要配合前端来展示项目总体的效果。 5.30昨天的测试对话结果巨慢，但是今天，同样的东西，结果反而快了很多，直接变成了昨天测试的十几分之一？后端对接了之前的前端，做了测试。发现这个前端基本都是写死的，对话、设置啥的没法调整，其实无法测试目前修改的效果。但是从fastapi和命令行结果来看，应该是没有问题的。api里面document部分啥的还需要修改一下。今天修正了doc_index_create这个api。下班前开会 下午，叶man阿姨突然打电话来，说周末可以吃饭。我一开始看到河北廊坊的号码还在好奇呢，以为是诈骗电话啥的都没想接，犹豫一下还是接了，没想到是叶阿姨！叶阿姨是我幼儿园时候，在俺爹开的小公司上过班的姐姐，后面虽然走了，但是和我妈妈关系很好，她们一直有联系。我也是大概大三？或是什么时候才加上她，但是刚加上的时候说了几句话，后面一直没联系，没啥说的感觉。这次趁我在北京这个机会，吃个饭，这很好！家里喝的茶叶似乎有不少都是叶阿姨给的。好久不见！也不知道如今能不能认出来，唉！我小时候对她的相貌记忆是一点也没有了！她的女儿在百京海淀区上学，马上高考了！我去！京✌！ 刷了3道题，基本都是回溯。基本不会。基本看答案。需要加强。 明日任务：我不到啊！看一步吧！ 5.31今天和后端实习生一起测试了各种api，逐个排查，修正了一些bug。五部分的api，有三个部分都可以跑通了。项目里面有一个小bad case就是，把id和name混淆了，函数名称啥的没改，在函数内部用查询把name转化成了id。测试前端对话能否调用后端，主要是对话api。其他的前端内容基本上都是假的。对话有四类： 1CHAT_TYPE = [&quot;RAG_CHAT&quot;,&quot;RAG_SEARCH&quot;,&quot;LLM_CHAT&quot;,&quot;RAG_RERANK&quot;] 前两个对话好像是一样的返回结果。本质上这俩对话没啥区别？第三、四个对话，结果和前两个不一样。快下班的时候总算是跑通了。之前是ollama 显存不够所以没跑通。 下周任务：优质解答——我不知道 五月结束了！到北京来已经两个多月了！回首过去，是一段不太美好，但应该还是比较有价值的经历……？","categories":[{"name":"记录","slug":"记录","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/"},{"name":"大模型","slug":"记录/大模型","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"实习","slug":"记录/大模型/实习","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%AE%9E%E4%B9%A0/"}],"tags":[{"name":"日记","slug":"日记","permalink":"https://yukino256.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"随想","slug":"随想","permalink":"https://yukino256.github.io/tags/%E9%9A%8F%E6%83%B3/"},{"name":"心情","slug":"心情","permalink":"https://yukino256.github.io/tags/%E5%BF%83%E6%83%85/"}]},{"title":"DailyRecord-April","slug":"DailyRecord-April","date":"2024-04-01T08:10:59.000Z","updated":"2024-05-17T03:18:58.614Z","comments":true,"path":"2024/04/01/DailyRecord-April/","link":"","permalink":"https://yukino256.github.io/2024/04/01/DailyRecord-April/","excerpt":"","text":"4.1上午，把qwenllm&#x2F;qwen的docker image放到服务器上了，但是下午发现模型没放，还要下载72b-chat的模型再放上去；而且这个东西好像要自己写应该服务端py文件？下午复习了一下long-context的论文，看了retrieval的一些如longllama，温故知新，没跳出已有框架的同时，感觉理解更深了晚上，和SH打了一把游戏，然后和沈老师开周会，汇报了一下自己目前的一些理解。得到下一步的研究内容是：把retrieval 的方式在大模型上都实现一下之后，继续和SH、NJ一起打了大乱斗 虽然是愚人节，但是无事发生 明日任务：阅读论文；学习修改大模型的方法 4.2 草！这一天干啥了我给忘了！原来日记漏了一天，4.8才发现！ 好像还是在看代码和教程？ 4.3上午，阅读qwen的model_qwen.py文件，尝试理解模型结构，寻找修改方法。最终目的是将kv retrieval加入到模型中去；目前没什么头绪，是直接改model文件，还是写个新的继承一下？继承的话如何与已有文件保持联系和交互？qwen的py代码没有啥注释，突然想到可以看一下transformer包中的代码，拿llama做参考，希望有注释可以好看一点吃了好大的饼将4090机器上的qwen-72b-chat放到A800上，尝试让它跑起来；昨天前辈哥用vllm加速一些模型，基本上没跑起来报错，不知道是什么情况；没查到解决方案 明日任务： 明日个P！清明节假期！好吧，这个饼还是挺不错的，也许会假期学习一下。不会的好多 4.4-4.6假期第一天：上午打游戏，中午吃了昨晚下班时买的罗森便当；晚上去NJ那里，吃了川菜馆子“椒榆”（好像是这个名字），一个炒鸡、一个黄焖茄子、一个蒜泥白肉、一个小酥肉（没怎么吃，打包带回去当第二天的早饭了）；之后在附近转了转，然后坐地铁去上地一个超市逛了逛，吃了又一家杏仁豆腐，不如德和斋；打包了红豆双皮奶和一个什么奶回去第二天吃；去地铁站打道回府，路上看见北体的一个小破门，另外还有漂亮MM。之后到家，和SH打了会儿游戏。假期第二天：打游戏的一天奥，无事发生！和SH、NJ打了好久的游戏；中午吃的猪脚饭，还不错；晚上吃的炸蛋螺丝混，也挺不错，熙螺湾这牌子在仙林NJU的对面也开了一家，开了不久；所以点的时候还是比较放心的。假期第三天：中午和NJ一起去安贞门；在一家小巷子里的老馆子里面吃地地地地地地地道儿的百京菜！八大碗中的牛杂，好吃！牛肠煮的最好吃；一个炒牛尾，还行；一个羊杂砂锅，不错，但是没牛杂好吃；一个麻豆腐，挺好吃，很新奇；吃完饭，去买了芬达，合起来和大洋是有点差别，感觉大洋更好喝一点；然后沿河的公园里面有什么花展，走马观花的逛了逛。忘记给NJ带德和斋杏仁豆腐了，明明前天才夸下海口…三天里，没怎么学习，网络小说倒是没少看；代码什么的搞了搞但好像是在做无用功….. 4.7不想上班啊啊啊啊啊啊！上午折腾随身wifi，顺便把qwen-72b-chat完全放到了A800的服务器上，跑了一下服务端，正常；后续工作可能就是，尝试修改模型之类的，再把后端接口什么的搞一搞？下午，搜索阅读了一些论文，感觉收获不大；觉得是自己的代码能力不足的问题，找了篇开源代码的去读，GitHub 1k star， 但是没读懂，感觉代码写的好像不太好；遂重新阅读transformer中llama的源码；因为之前发现qwen的结构和llama很相似，希望读懂llama后能很快的触类旁通。遇到了一些问题需要记录 RoPe编码的实现，看的半懂不懂，没有深究，后续视情况看是否需要深入。 llama attention模块中，有这两个参数，尚未搞懂作用是什么： 12cache_position: Optional[torch.LongTensor]position_ids: Optional[torch.LongTensor] = None 晚上在DNF神迹上抽黑钻售货机，因为没有自动抽取的功能，所以尝试用pyautogui写了个自动抽奖的脚本，但是实际运行的时候发现移动位置是对了，但无法点击到游戏程序内部的东西，又发现自己使用鼠标一直点击倒也可以，于是又开了鼠标连点器，发现鼠标连点器也点不到游戏程序内部的东西；这两种相似的现象带来的启发是：这种基于GUI模拟点击的程序，或许没有聚焦到应用程序内部，亦或者是DNF.exe的特殊性之类的。想来之前应该有一个获取游戏窗口的方法，但是不想搞了，就这样吧。 明日任务：继续阅读llama的源码 4.8今天，勉强把qwen的model文件看完，仍然是半懂不懂的状况，尤其是generate()和chat()这两个函数。logit的值映射到id再映射到文字，思路很好懂但代码有点复杂后续重新看了GitHub上qwen的项目，以及modelscope上qwen7b模型的具体内容和文件。本来以为modelscope上面的是运行文件之类的，原来它真的只是模型，这个模型安装一定方式组织，transformer还是huggingface依照generation_config.json和config.json来完整的读取模型、运行之类的。运算、生成之类的函数确实是写在这个模型里面的，github上面的是介绍、示例的demo、各个数据集上的测试文件。也就是说，如果要在模型中加入cache的retrieval，就要在modeling_qwen.py这个文件中直接修改？（但是我更希望的是，写个继承的文件然后互不影响？？）得看看论文和论文代码去晚上，和沈教授开了组会，表明目前的问题是不知道怎么改代码一头雾水，直接在模型文件上改感觉不稳妥；他建议我直接改试试 明日任务：阅读recurrent-memory-transformer，开始修改qwen代码文件 4.9工程师大哥入职噜！希望能带我飞！阅读recurrent-memory-transformer的源码，思考其结构和代码作用modeling_rmt文件，大致上应该是模型文件，其中定义了两个类，MemoryCell——似乎是负责向原有model中加入MemoryCell的类；RecurrentWrapper——似乎是可以将原有model的输出再处理之后的输出，这样看来，好像可以直接将其运用到qwen中去？下午和GG聊了一阵子，到出租屋后又和他聊了大概一个小时；和NJ打了几把大乱斗 明日任务：继续阅读recurrent-memory-transformer，开始修改qwen代码文件 4.10今日，新入职了一个本地学校的做数据的实习生上午，稍微思考了一下RMT的代码结构和用法；然后尝试在PC上跑一下千问的小模型查看效果采用Int4版本，结果报错： CUDA extension not installed. 尝试安装cudatoolkit，结果依然没有解决破案了，byd一直安装的cpu版本的torch之类的，可能是版本没对齐&#x2F;清华源的问题来自HXD的支援： 建议用pip别用condaconda不会检查环境里的冲突直接装pip会先看有没有装 他妈的破案了，python和torch版本太高了 明日任务：改千问暂停，协助跑通RAGFLOW 4.11RAGFlow昨天下午工程师跑了，它最方便的功能还是直接调用api，但工程师尝试搞明白阿里的api怎么申请怎么用却失败了；从示例图上开RAgflow可以支持本地模型，但是在我们跑通的结果来看那个图上面的选项消失了，今天工程师问ragflow的群主得知，已经用ollama直接取代了。似乎ollam调用更方便？准备部署ollama试试，但是由于4090显存不够了，因而要将其部署到A800上；镜像的上传又是折磨踩了一个坑： scp命令指定远程端口，-p其实要大写成-P才是正确命令 导入之后，显示名称什么的为none，需要使用docker tag命令自己命名继续琢磨怎么加mem， 想搜RMT的解析文章，结果搜出来发现说它就是Transformer-XL，然后去搜transoformer-XL，发现它是2019年的文章被拒稿了？后续改进的XLNet，这个已经被加入到了transformer官方包里面查看了一些代码，感觉加不进去；但似乎渐渐得到了一些理解：这些网络结构什么的已经定好了，包括qwen，这些放在src&#x2F;model下的modeling文件定义了模型，而huggingface或者什么地方可以找到模型的一些与训练好的参数。那么已知的是qwen确实没法改模型结构？最多只能在推理的时候采取一些不影响它模型本身流动的trick，如lora&#x2F;adpter之类的东西。qwen1.5采用了滑动窗口attn，可以提高推理效率；能否提高长文本能力尚不明晰。似乎可行的两个思路： 使用adapter，直接将之前的kv cache揉到attn里面？ 直接在模型推进的某一层，对kv cache和外部向量数据库 进行向量检索，然后揉进去？ 研究方向暂时转变：研究ReAct + CoT的应用；唉人在江湖身不由己，但好在是这回有人指导了，希望这段时间代码能力能有突破；越来越觉得LLM尤其需要强工程能力 用千问的api，跑RAGFlow成功了，后续工作是更改成本地模型；此外，还有一些功能待添加：聊天界面可以输入图片&#x2F;文件，支持连接sql并发挥BI功能 尝试用langchain，将React和CoT结合起来 明日任务：langchain的ReAct和CoT；了解一下CoT 4.12今天没什么印象深刻的任务，在读一篇新的论文BPO，阅读它的代码。由于工程能力不足，阅读代码总是吃力半懂不懂；或许，LLM学习困难的原因在于实验条件高，我平时真的很难上手去改一些东西并快速查看效果。 晚上，与ZYR和SL一起吃了聚宝源（西直门店）的涮肉；说实话感觉一般，那个B麻酱根本不香！感觉不如一般的火锅涮肉，亦或者之前在西安吃过的冰煮羊 吃完饭，一起去紫什么公园逛了大概一小时；今天与朋友们聊了一些求职、读博方面的事情，聊了一些高中同校、学弟学妹们的一些发展；再次感叹人外有人，每次聊天总是能感受到自己的不足，希望我能赶上去！实习实现工程能力的巨大提高！！！！ 明日任务： 假期噜！明天和LT，NJ一起聚一下，吃筋头巴脑！ 下周任务： 读懂BPO代码，复现 4.13-4.14周六中午，和LT，NJ一起在西二旗吃了湘菜（筋头巴脑无了），280的套餐，大众点评好评送手撕包菜；有臭鳜鱼、擂椒皮蛋、小炒肉、辣炒茶干、肘子肉（应该不是这个，但我想不起来具体名字了）和鸡蛋烧的一个菜；喝了可乐；吃撑了，LT饭量没衰减，而我已经吃不动了，唉。吃完饭，随便逛了逛；路上飞虫进眼，忙忙糟糟虚惊一场；简单粗暴地买了瓶矿泉水冲了一下眼睛。和LT一起去天坛公园逛逛。路上闻到一些味道，结果发现是沙比LT的衣服馊了；想到自己当年也穿过馊的衣服而不自知，还是亲娘发现的，不免有些好笑。天坛公园要购票，15一张。和LT随便逛了逛，由于他要赶火车因而走马观花，只看了很小的一部分；聊了聊过去、现在、未来。似乎朋友们没怎么变，似乎又有点变化。但朋友身边总归还是安心一点。实际上，和朋友们聊天、自己写博客记录，是我迷茫的一种体现，我或许希望在交流中明心见性、获得奋斗下去的动力。 下午回去，打发时间；到了10.40，SH上号叫我打LOL；周日纯躺家一天，下午1点多和SH打了两小时大乱斗。NJ撺掇我玩dota2，未果；我暂时不想花费脑子在游戏上了，新游戏的学习在上学的时候可能是快乐，但现在我没有动力。 4.15上午又入职了一位员工。开了个会，妈的不知道现在要干啥，云里雾里的。 上午看了一些东西CoT的一些东西，结果下午又告诉我回去搞长文本。这是否有点……下午，被告知搞一个长文本工程思路的PPT，于是梳理了一下想法，工程思路分为三类，每个类对应目前的一个具体例子。突然发现，之前的LongMem好像可以借鉴，于是重新去看它的代码。这一看不得了，之前的认识还是太浅薄了，现在感觉他的代码思路都很明晰，然后虽然用的fairseq这个我没接触过的package，但是只是为了memory的方便，因而使用它的incremental decoder类。从它的eval.py、memory以及fuse等文件中，隐隐感觉到可以用在qwen上，但我需要更详细的去理解代码。这个项目中一个memory bank模块，似乎可以很方便的迁移。我必须立刻发动同调！沈教授今晚有事，预定的周会推迟一天噜。晚上，和NJ和SH大乱斗；和SH输麻了。 明日任务：呱！我要狠狠地使用longmem呀！ 4.16上午继续阅读longmem的源码；它虽说进行了解耦，但从网络结构来看，是自定义了一个层数为底模一半的网络，然后输入、训练、生成memory bank的结果，然后一直存着不动，同时底模向前运行推进，最后再加一个融合层？也许是我理解不到位，代码看错了；总之是有点搞不懂的。感觉代码好像挺不错，但转化到qwen中有难度，因为他主要用的fairseq，而我还没学过，感觉这是一个深度学习的库，自定义模型什么的。下午，尝试跑一下qwen的memory增强；首先跑了基础的底模，啥都没改，让它从西游记的一段话中抽取一个信息，应该是一个大海捞针的任务，没回答出来，那我的思路就是调整模型文件来查看效果。其实，下午也看了一阵子评测数据集的东西，但是也是粗略的看看没大搞懂，可能主要因为看的评测数据集与我预想中的有所区别，C-EVAL什么的是通用，但是长文本&#x2F;超长文本可能还要那种名著数据集什么的，我记得之前看过有，但是一时间忘记去找了；倒也无所谓，因为这个qwen-1.8b-chat模型本身就没有长文本能力，ntk和logn缩放都无，只有7b和14b好像支持这个；因而我采用了上面的一个测试数据。修改模型还是没什么头绪，longmem究竟如何加进去？ 晚上，开了周例会；得到指点，预计看三篇论文、使用faiss检索并将检索结果加入prompt中作为self-RAG的baseline，需要跑通这个实验 明日任务：学习faiss，将输入query先切片、再faiss、再构建prompt，再输入到模型中查看效果 4.17今日，尝试将query 按句子切片；结果使用nltk、langchain等切片都未果，不知什么情况；于是使用简单的split暂作切分；结果发现是这个标点符号好像有点特殊，应该是另一种编码还是什么东西？但是NLTK应该确实是不能进行中文分词的。尝试了langchain的文本chunk方法，也未果，到底是怎么回事？终究，实现了将query进行faiss，然后加入到prompt中去，返回的结果不是很好，可能是由于模型本身的能力不足，也有可能是prompt模板需要优化。他妈的！公司的网还是没接上！我都没办法测试真正的模型的效果、没法保证自己调整的可用性。 晚上回家，打大乱斗；输麻了，这b游戏能不能死一死。目前寻找其他可以和朋友一起玩的游戏中……但这个更换游戏的限制其实在于朋友之间共识的达成。 明日任务：阅读周会上的论文，尝试优化prompt，构建模板 4.18上午，稍微调整了一下prompt，之前的prompt格式有点不正确；然后测试了西游记第一章其中一节的效果，似乎有提升，但是感觉不明显，可能是因为模型不太能处理这种半古文的原因。于是，切换了内部的测试数据，运行查看效果，确实提升了模型的性能。 思考：首先说一下目前存在的不足：目前，对于query的faiss处理，没有实现问题与前置信息的分离，仅仅是将整个query进行cut&#x2F;chunk，然后使用faiss检索、返回topk的结果，利用其构建一个增强的query；prompt注入方法的缺点已经很明显了，这里要说的主要是未来可能的faiss&#x2F;其他向量库检索结果错误&#x2F;不匹配的问题，正如之前提到的，没有实现问题和背景的分离；此外，就算实现了分离，问题和背景知识的向量也不一定能够匹配上去。目前的实践，仅仅是一个简单的test，还有许多细节需要调整：history与历史faiss向量库的保留筛选机制；faiss向量在内存中的废弃与删除；未来的应用场景应该是去理解长文本乃至超长文本，或许还要涉及生成，因而超长文本下token、input、cache等的限制也需要考虑，当然这是后面的课题。其次，说一下确实实现结果优化的可能原因：正如之前提到的，这种实践的方法论与理论基础比较不踏实，但为什么实现了结果优化？可能的原因在于，最后的问题，由于关键词还是语义之类的，在向量化之后确实与背景信息的某一部分高度相关，从而在query的cut与向量化之后增强了相关的向量（sentence_transformer的原理似乎在于，计算句子向量与全文向量的相关程度，从而返回topk），使得大多数情况下相关背景信息能够被 “大海捞针”，这部分topk信息，加入到prompt中去，再输入到LLM中，在LLM内部又会因为重点信息的二次&#x2F;多次出现实现attention的增强，从而增强了返回结果。 下午阅读TransoformerFAM的论文，但是尚未理解透彻，不太能构想出来这种结构应该怎么在代码里实现。论文中说，可以不影响之前的权重参数，也就是说应该定义一个class来作为cache？内网接入许可批下来了，可以使用服务器上的模型了。 明日任务：阅读TransformerFAM，测试服务器上的模型的prompt优化效果 4.19尝试将内外网访问走的通道分离，不然每次访问内网都要断wifi就很傻逼。下列命令需要cmd以管理员身份运行 1route -p add 172.17.39.189 mask 255.255.255.0 173.17.39.254 三个地址分别是内网ip、内网掩码、内网网关。（幸好网络接入需要mac验证，不然我这命令还不好放上来）。网上搜的教程还额外所有路由全走了一遍内网，纯沙比做法来的，只要这一个命令其实就可以了。 突然发现自己鞋子穿的不是同一双，好笑之余，又有点感慨。或许我仍然没有长大，仍然毛毛燥燥地去面对这社会的一切，或许我就像穿上大人西装的蜡笔小新，在假装成熟。上次发生类似的事情，似乎是袜子还是什么东西穿的不是同一双，已经记不太清了。 下午，整了一下午curl访问那个b服务器模型，都失败了；官方文档的命令试过了一点用也没有；但是，使用langchain的命令却成功了？？？？？基本上两小时浪费了，我真是无语子。但好消息是，这个prompt优化的效果，在7b上也很明显。下午还看了一会儿TransformerFAM，似乎有一点实现的思路；不知周末是否会开始写一下 （基本不可能）今天发了一部分工资，上个月20号到本月10号的。 明日任务： 哇袄！高一下至高二末尾的112寝室！三分之二的四人再聚首！吃一口地地地地道儿的百京八大碗！没毛病奥老铁们！ 下周任务：后续复现捏~ 4.20-4.21周六，和SL、NJ一起吃之前和NJ在安贞门吃过的那个小馆子，地道儿！点了麻豆腐、炸蘑菇、八大碗之牛杂、豆泡（这个b豆泡和麻辣烫那个味道一样，真是血亏！）、扒肉条、羊肉烧卖、蘑菇和莴苣炒的一个素菜。吃的挺好。啥比LT用他之前在群里吹的向老板请假的理由来应付我们😅，家人们真是一整个无语住了。啥比SL坐地铁坐反了😅，坐到他妈的朝阳门去了，多等了半小时才吃饭，百京✌坐车主打一个随心所欲说是。快下地铁的时候，NJ告诉我附近有cosplay的什么展子，因为他在地铁口看见了好几个穿的奇奇怪怪的人🤣；我出地铁口的时候，看见了一个JK的背影，是那种日式JK，因为那衣服的质感啥的一看就是cosplay的。吃完饭，NJ猜测cosplay的场地是在北投购物公园，于是我们走过去看看热闹。在那个购物广场里面逛了第一次，我没看见，宝可梦道馆也被查封了，但在找宝可梦道馆的路上看见了一群coser，看背影认识一个钉琦野蔷薇；出了商场到附近的地方又稍微转了转，没什么好看的；于是原路返回，想去河边的公园邹洲、看一看花，结果这时候在购物广场里面看见了一大批coser，居然有小鸟游六花，泪目，细想已经12年了；SL说看见了约尔太太；当然还看见了二字游戏的萤妹（wc！O！）；其他零零散散的coser记不清了。出了购物广场，又看见两个coser进广场，看来是真有展子；其中一个是miku（我去！初音未来！），可惜听NJ说是个坦克；我看的这些coser都没看见正脸，全是背影。在b站会员购上查了查，发现真是个展子，门票80的coser展，纯cosplay交流之类的，无同人志啥的，感觉没意思，也没有啥声优给我看（**樋口円香你带我走吧😭*），就不去看了；去河边公园逛了逛，歇了歇，帮SL看一下他碧蓝航线的装备和配队；差不多14点的时候，各自打到回府了。下午，NJ发了淘宝链接给我，是大窑打折，3450ml只要8.9r，于是速速下单；又问他还有无推荐，给我推荐了7月临期的盐汽水，买了；到出租屋之后，躺、看小说、刷视频、打劣质游戏。晚上10点多，查公主链接EX5的作业，查到一个好像能抄，遂决心战斗；耗费2k母猪石、8E玛娜，一个多小时、SL十几次，终于通关EX5； 周日，混；中午吃的烤肉饭外卖，晚上吃的螺 吸 混；和SH打大乱斗；他妈的！返回百京的票没抢到！全是候补！五一节回不了家什么的那种事情不要啊啊啊啊啊啊啊啊啊啊啊啊！如果没办法，只能请一两天的假延迟回百京了晚上8、9点的时候感觉眼睛有点不舒服，干涩，又有点困，于是早早睡了； 4.22周末的时候，华为的移动wifi到了，加上之前网管过来接网线但啥事儿都没解决反而把服务器的外网权限给断了😅（百京的国企✌就是爷！），因而服务器和PC都会通过这个wifi相连（终于不用我开热点了！）。上午，阅读Infini-Attention的论文，稍微看了一下源码。这个示例代码是基于qwen2MoE模型的，迁移到qwen-chat上应该比较容易。但是话又说回来，MoE模型与chat模型的区别在哪里，还需要阅读一下源码。可能的好消息是，Inifini-attention本体的代码还挺短的。在阅读论文中的一个疑问就是：论文中Memory cache的更新，似乎是增量循环，同时在这个过程中memory没有Norm，这样会不会数值溢出？ 错误的，没疑问了，activation和norm发生在后面retrieval attention的计算与检索上面。 下午，比较infini和qwen2-moe原生attention之间的不同；代码和思路应该是简单的，就是把额外的memory加到attn_output里面去；但是 为 啥 qwen2-moe 的 attention 不 做 softmax 和 norm ？ 铸币了，不是在这儿加add和norm的；qwen这个attention，在计算完attention之后，又对它额外做了个nn.linear。这个操作的出处是哪儿？我查了llama的源码，发现它也是这么做的————哦tmd，又铸币了，原来这是MHA，多头concat到一起之后再用linear层融合一下。理论到实践之间的距离还需要去努力弥补！看代码的时候突然想到，论文中做了cache消耗的对比，infini只用额外的1.5M，而memorizing transformer用了183M，为啥会差这么多？只在某一层加入cache有那么大的消耗吗？看完了项目里面的model文件，对照qwen原文件之后，确定了要改的地方，基本上就是有M_Z这个参数的地方；其他的三个文件还没看。晚上，和沈教授那边开了周例会，再次明确接下来的任务是无误的。和SH大乱斗，输赢参半吧，就是遇到沙比选C但是C不起来挺无语的。晚上，和ZZK聊了一会儿工作的事情，他现在在上海的滴滴实习，和我吐槽房子不好、天天加班、节假日也加班、实习工资低、孤独、精气神耗尽假期也不想动……我感同身受，还是那句话，大家对上班的感受都是一样的；再次感受到人的天性就是厌恶工作的。上完这个B班，回家往出租屋一躺，就感觉这辈子完辣😅。其实，写博客、在博客中放飞自我（指瞎几把说、粗俗用语），是我在迷茫、孤独时的一种排解手段吧；给自己找一个能看见进度的、做起来不太麻烦、有点儿动力做的事情做，可以从中感到一点活着的意义。记录日常，一方面是看看自己是不是真的有进步？另一方面，也是记录一些难以复刻的经历与心境，避免自己遗忘一些东西，像是异地的格格不入、大城市的自卑之类的。 明日任务：看另外三个文件，尝试修改本地的模型 4.23突然发现infini-attention的项目不是官方的，而是个人自己写的；github上还有几个实现的项目。好奇怪捏，为什么那篇TransformerFAM没有人写代码？对照着项目，尝试修改qwen的model文件，结果发现，示例的实现代码是基于qwen1.5的，这个代码更加简洁，相比之下qwen的model更加杂乱&#x2F;冗余？我也不知道怎么说，因为还没有看到qwen1.5的其他model源码。但是反过来说，修改qwen的model尽管可能更加困难，但相比之下应该更能提升我的工程能力、加深我对大模型流程和代码的理解，还是不能畏难放弃！ 下午，尝试修改qwen的model文件，对照infini attention、qwen源文件、qwen1.5源文件修改；qwen1在确定在哪儿融合memory上面有难度，按理说应该在实施了RoPE且加入到cache的q、k、v这一步操作之后，再利用这个cache进行memory融合，但这个代码有点杂乱，class里面为了实现attention又写了几个method，然而在qwen1.5里面就纯没有额外方法一步到位了，实现的很清爽，在1里面有些杂乱；总之，废了不少力气定位了memory加入的位置后，却又发现了另一个问题：infini里面使用的是GQA，这个所需要的参数，在1.5里面通过config可以定义、是存在的。但是在1里面，config.json文件里面并没有这个参数的影子；也就是说，qwen1是不支持GQA的。这个参数是： num_key_value_heads 在1.5里面，GQA中，需要计算group的数值，这个数值是这么计算出来的： 1self.num_key_value_groups = self.num_heads // self.num_key_value_heads 目前有两种思路，一是不要GQA，另一种是自己加上这个参数。在查看qwen1.5的configuration_qwen2.py之后，发现了这个新增参数它有一个默认初始化的值，初始化代码如下： 12if num_key_value_heads is None: num_key_value_heads = num_attention_heads 也就是说，如果num_key_value_heads没有定义，就默认等于num_attention_heads。我们可以直接qwen1的代码里面让它相等；这种方法实际上和不要GQA没什么区别，就是把infini的代码抄过来比较方便罢了。原以为难关已经被攻破了，但在repeat_kv的时候又发现了问题：首先，qwen1里面是没这个玩意儿的，也就是说1也不支持需要repeat_kv的一系列操作；这是小事，只要去1.5里面把这个函数抄过来就行了；但关键的是1里面好像也没有对v进行cache，仅仅cache了q和k，有query_list和key_list，至少我初步看来是这样的；我得再仔细看看，真要没cache_v的话，改的工程量就太大了；还是直接去改1.5的好。似乎，如果参数use_cache&#x3D;True的情况下，q、k、v是都会缓存的？而且直接就用query、key、value就行了？另：infini的代码更新了，之前一些M_Z的改动似乎是不必要的，他将其删除了（这沙比owener代码有个mistake，我issue告诉他，结果他回复说“没问题~~”，然后立刻进行一个issue的关，结果我再看代码发现他偷偷把那个mistake给改了😅，真沙比）。改好了代码，想要在本机上测试效果；结果cuda out of memory，难蚌；目前两个办法：一是换成0.5b-int4再试试，二是在服务器上部署跑一下；考虑到后续方便的问题，还是用服务器吧；但是conda create的时候报错了；解决ing……他妈的破案了！服务器的内存和硬盘都爆掉了！下班时间到了明日再战！ 明日任务：在服务器上测试修改后的模型效果 晚上，在出租屋混时间，没有打LOL；而是又拾起了✌最爱的怀旧页游！（如果WQ看到，又要搁那儿骂我山猪吃不了细糠啦！哈哈！）这游戏时不时就想捡起来玩一阵子，然后突然感觉无味、因而果断抛弃，往往下定决心再也不碰，却又偶尔想起、心痒难耐。我承认我是一个很怀旧的人，我或许一直无法脱离过去的光影。 4.24上午，按照计划、尝试在服务器上测试修改后的模型的效果；在部署环境的间歇之余，看了一下qwen1.5的model文件，开头跃入眼帘的就是rotray embedding这个class；其实，看过的model文件，大多都像这样将其放在文件的开头；而且，基本上都是一样的写法，似乎都是从llama或者什么原初模型中copy过来的；鉴于之前并没有太搞懂其原理、代码，只是了解了五六分，因而又打算再看看；花了一阵子时间，看的眼花缭乱，最终收获却不如想象之中那么大，还是没有明晰。终于，在环境部署好之后，尝试运行cli_demo.py，运行成功了！问答也能正确输出！ 虽然结果不是很好，但是考虑到模型本身、以及加入的attention还没有经过ft或者什么操作，因而已经很满意了！希望这个现象代表的是：修改的内容是成功的，后续是可以推进的。 接下来要考虑的，主要有两方面： input query过长时的segment和retrieval、或许还有chat history的相关操作； infini的深入理解，与微调可能； 在上述做完之后，qwen1.5的修改与尝试应该就是水到渠成的事情了 今天中午，又发了一部分工资，上个月20号到本月20号的。 下午，看了一下其他的论文。准备看streaming-llm的代码，但是这个代码结构我得梳理一下。因为苹果吃完了，所以在楼下lawson买了两个饭团当晚饭；饭团还在打折，好耶！到出租屋，依然混时间；和SH打了两三个消失的大乱斗。 明日任务：阅读streaming-llm源码，看LLM教程。 4.25今天上午，又入职了一位姑娘，似乎是北大的，和上周入职的老哥认识；捏妈，百京真是卧虎藏龙。感觉自己被Top2包围了。看streaming-llm的kv-cache部分的代码，结合论文看，还是有点难懂捏。昨天在服务器上创建了老长时间的环境给nm不知道是谁给删了😅😅😅😅。中午，突然想起来昨天改的qwen-infini代码似乎有问题；去查看之后果然如此；这么想来，昨天的cuda out of memory应该是一个失误，而不是修改模型成功运行的信号。于是，下午继续琢磨修改；遇到了许多问题。下面列出最严重的问题： 在infini-attention的初始化模块中register_buffer了M和z，同时对其进行kaiming初始化，但是后续forward的时候，却显示M和z是NoneType：如果不对M和z进行初始化，则回在计算logit的时候报错，因为logit值都为0；这边测试了一下初始化代码的写法，至少可以保证的是，init函数中的M和z是正确初始化的、正确登记的（？）————原因：如果模型权重文件里面没有这个buffer，就算我初始化了，加载权重之后也会是nonetype；所以需要在使用之前再强制初始化一下。 此外，还有似乎是因为精度问题导致的错误：probability tensor contains either inf, nan or element &lt; 0：可能量化的模型都容易出现这个问题，百川有一个issue似乎可以解决；但是一个下午看的头昏脑胀，不想再加班的，明天再搞吧。 奥牛逼，工程师大哥鼠标灵敏度贼低，结果nm是LOL电一前二十？？？ 明日任务：解决问题 4.26继续尝试修改错误，首先确定具体问题所在，根据报错结果，去修改代码，print报错之前的相关信息，其probs和softmax之后的结果打印如下： 12345#probstensor([[-inf, -inf, -inf, ..., -inf, -inf, -inf]], device=&#x27;cuda:0&#x27;, dtype=torch.float16)#prob_after_softmax： tensor([[0., 0., 0., ..., 0., 0., 0.]], device=&#x27;cuda:0&#x27;, dtype=torch.float16) 可以看到这个结果，可能是第一个token开始这个概率数值就出错了；正如昨日所说，应该是精度转化的过程中出现了问题。尝试Qwen下面一个issue的回答，结果运行了很长时间，返回结果如下：一开始运行的时候显卡风扇咔咔吹，我还以为陷入死循环了呢，没想到还是给我返回了结果。这个结果大致上符合预期吧，因为照着上面修改了代码之后，直接选择概率最大的作为decode的值，但是从一开始到最末尾，概率值都为0的情况下，自然所有decode出来的token都是相同的；qwen中这个都是0解出来就是“！”，应该是这样的。总的来说，这个结果是好的，说明修改代码没有问题，只是精度转化的问题。代码是可以正常跑通、一直decode到最后的；后面要做的就是进行微调之类的东西了；后续的问题，等到实战的时候再说吧！下午，借用同时的设备，接入了公司的网，可以使用A800了。目前收到的任务是，在A800上，对修改后的模型文件进行调整、评测效果对比。同时接入了wifi和以太网，花了好长去搜如何让ssh默认走以太网而非wifi，结果失败了，暂时只能物理切换了。下载qwen-7b-chat的模型文件，修改model文件，还需要从本地copy到远程服务器上，因为那儿不能连接外网。 没事儿了，下载一会儿突然想起来modelscope不是外网，可以直接下；对于服务器里面没有git-lfs，也无法使用root权限调用apt-get命令进行安装的问题，可以使用conda安装： 1conda install git-lfs 感谢之前的自己把conda什么的国内镜像源都换好了🥰🥰此外，没想到之前搞得环境有问题，安装不了faiss，原因在于python版本太高了。重写搞了个环境，幸好速度很快！搞好了环境之后，没量化的版本返回结果是这样的：乱说一通！但确实是量化的问题奥！没毛病奥老铁们。 明日任务： 我恨调休！放不起就别放！明天躺尸一天！ 下周后天任务：A800上的模型训练~ 4.27周末，在出租屋躺尸一天。本来打算中午睡个午觉、下午再出去骑行一两小时，结果下午百京竟然下雨了，计划泡汤。中午和SH打了几把大乱斗，好像一直在赢。昨晚和今天，看《魔都精兵的奴隶》，主要是看漫画，动漫也看了一点；不得不说很合我的胃口，纯爱卖肉番suki捏🥰🥰。看到了118话，禁漫上面的更新到此为止了，似乎还可以继续看十几二十话来着，但是我看小说、追剧之类的时候，总喜欢留一点，有种松鼠存松果的那种意思，好看的不能看完，得存着留点念想（除非忍不住）。怀旧页游游玩过程中遇到了一点小挫折捏。 4.28他妈的调休！我！要！杀！你！一！万！遍！也！不！够！！看qwen的finetune文件，看看这个参数怎么冻结、怎么调。我登记了一个param，名为beta，两个buffer，名为M和z；在一开始，我以为要训练beta，和M、z的初始分布，其他的参数进行冻结，于是我尝试查看这两个参数在哪儿。 123456# 查看paramfor name, param in model.named_parameters(): print(name, param.requires_grad)# 查看bufferfor name, buf in model.named_buffers(): print(name) 结果发现，param列表里面有beta，但是buffer列表里面没有M和z。我感到很奇怪，为什么M和z没有加上去？难道是我哪里写错了？于是，去搜集如何进行finetune的教程，终于发现，只有param是可以调整的参数，buffer只是常数罢了；因而，正确的调整方法应该是：调整每一层的beta参数和所有transformer结束后的全连接层。那么，问题是，为什么我会觉得M和z是可以调参的呢？这个误区的原因在于，在infini代码中，M和z是一个类似RNN的更新机制，这个更新实际上是根据每一层的Q、K、V进行的；之前，在加载模型的时候，因为预训练模型中没有M和z、因而被设置为None了，所以我在代码里面又加入了初始化M和z的分布；当时想的是，后面正常训练好的模型，M和z应该是有一个分布的；但是不是这样的，正如上面所言，M和z是Q、K、V基于类似RNN的方法得出的，是固定的生成流程；不固定的，只有门控参数beta罢了；但是，现在还有一个可能的问题就是，既然M和z在推理过程中被设置为None，那么我训练的时候会不会也是None、因而无法操作？在服务器端，将再次初始化的操作删除后，运行果然报错；但是，在将init里面的初始化中的kaiming分布去掉后，全用zero的分布反而不会报错正常运行，只是结果不太好罢了，但是也不会乱说一通。于是，在finetune.py文件中，在模型加载和后加入以下代码，理论上就可以运行qwen自带的bash脚本进行调参： 12345for name, param in model.named_parameters():if not(&#x27;beta&#x27; in name or &#x27;ln_f&#x27; in name or &#x27;lm_head&#x27; in name): param.requires_grad = Falseelse: print(name, param.requires_grad) 在训练数据集的选取上，则需要进行斟酌：qwen提到自己对多个数据集进行了打榜；那么，这些数据集是可以参考的，目前可能何时的，是C-EVAl以及中文长文本数据集；但是长文本方面，对输入进行segment的代码还没写，因而暂定使用C-EVAL进行训练和评测。仔细思考了一下，现在是模型的基础能力因为新加入参数和计算的原因有所下降，因而需要训练其基本的问答能力，至于评测集，我认为应该在基础能力满足之后进行进一步的调整。使用什么数据集呢？我直接开始摇人！沈教授推荐了指令微调数据集COIG-PC。一开始在检索的时候我被导向了同一个机构下的早期数据集COIG，这个数据集比较小；而COIG-PC足足有126G。COIG-PC这个数据集还需要申请，是gated的，幸好申请的流程很简单。由于模型太大，因而我希望直接在服务器上下载，但是服务器上又连不上外网，于是搜索huggingface镜像站。数据集地址在这里；此外，由于数据集是gated的，因而需要在本站上申请一个access token，在命令行的时候把这个传进去，才能git下来。相关教程在网站首页上有。总的来说，命令应该是这样的： 123pip install -U huggingface_hubexport HF_ENDPOINT=https://hf-mirror.comhuggingface-cli download --token hf_access --resume-download --repo-type dataset BAAI/COIG-PC --save-dir COIG-PC --local-dir-use-symlinks False 下吧！126G呢！该说不说服务器墙内下载速度还是快的。下完之后，梳理了这个数据集的情况：Hugging Face上面的datacard显示说，文件是jsonl、还给出了相关文件的命名；起初，我以为直接对jsonl文件处理即可，但事实不是。下载下来的文件全是parquet格式，是pandas的数据格式，而且命名与datacard中的完全不一样！；列名里面有filename，但我完全搞不懂这个玩意儿的意义何在： 1filename 00001-001-000-named_entity_recognition.jsonl 总之，借助GPT大人的力量，写了一个处理数据的脚本，用于遍历文件、提取train且是用于’推理’数据、并形成最终的训练数据格式。理论上来讲，这一步搞好之后，直接调用训练的sh脚本就可以了。唯一担心的问题就是RAM可能不够，但好像是我多虑了，白白花了一些时间纠结分段存储。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pyarrow.parquet as pqimport globimport pandas as pd import json# 获取所有符合条件的文件路径file_paths = glob.glob(&#x27;data/*train*.parquet&#x27;)samples_list = []idx = 0query = &#x27;&#x27;answer = &#x27;&#x27;def update_sample(idx, query, answer): sample = &#123; &quot;id&quot;: idx, &quot;conversations&quot;: [ &#123; &quot;from&quot;: &quot;user&quot;, &quot;value&quot;: query &#125;, &#123; &quot;from&quot;: &quot;assistant&quot;, &quot;value&quot;: answer &#125; ] &#125; return sample# 遍历每个文件并读取数据# f_idx = 0for file_path in file_paths: # 读取 Parquet 文件 # print(&#x27;正在读取文件：&#123;&#125;&#x27;.format(file_path[5:])) # file_path = file_paths[-5] data = pd.read_parquet(file_path) # 发现数据，构建数据 print(&#x27;正在遍历文件：&#123;&#125;&#x27;.format(file_path[5:])) for index, row in data.iterrows(): # print(row) # print(idx,row) is_correct_file = False for r_str in row[&#x27;task_type&#x27;][&#x27;major&#x27;]: if &#x27;推理&#x27; in r_str: # print(&#x27;find!&#x27;) query = row[0] + row[1] answer = row[2] sample = update_sample(idx, query, answer) samples_list.append(sample) idx += 1 is_correct_file = True elif &#x27;信息抽取&#x27; == r_str: query = row[0] + row[1] answer = row[2] sample = update_sample(idx, query, answer) samples_list.append(sample) idx += 1 is_correct_file = True if not is_correct_file: break if not is_correct_file: break print(&#x27;文件：&#123;&#125;处理完毕！&#x27;.format(file_path[5:])) # breakwith open(&#x27;train_data_2.json&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file: json.dump(samples_list, file, ensure_ascii=False,indent=2) 或许基于file_name这个东西能更轻松？我不知道，反正这样就行了，能实现目的，花费几十分钟也不太慢草！得四个多小时！。加班，等数据处理好到21点。我草！忘记import json了！尼玛白等了！为了快速，观察到每个文件里面数据的用途似乎都是相同的；简单地从第一条数据开始判断吧！没有就跳过！但是失败了，结果是空的！此外，还犯了严重错误：没有copy变量而是直接在变量上修改，导致最后list全是一个变量！ 明日任务：解决问题，准备好数据！开始训练！ 4.29首先，昨天确定的思路是： 查看昨晚的json数据是否正确 如果正确，则开始尝试训练模型；不正确，则改写正确的脚本，同时先用数个测试数据构成训练数据集，测试ft是否正常运行。 根据昨天晚上，运行训练的sh脚本失败的问题，排查原因。 遗憾的是，昨天的代码还有个小错误，导致数据未正确提取；这也是昨天“从第一条数据开始判断该文件是否是所需数据集的文件，返回结果为空的原因”——str in list只会查看list元素，而不会查看元素中是否含有相关字符串。总之，数据总算是正确提取了！尝试运行bash脚本，报错信息如下： 1234FAILED: multi_tensor_adam.cuda.o/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\&quot;_gcc\\&quot; -DPYBIND11_STDLIB=\\&quot;_libstdcpp\\&quot; -DPYBIND11_BUILD_ABI=\\&quot;_cxxabi1011\\&quot; -I/home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include/TH -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include/THC -isystem /home/zhangshuo/.conda/envs/QwenMem/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options &#x27;-fPIC&#x27; -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.onvcc fatal : Unknown option &#x27;-generate-dependencies-with-compile&#x27;ninja: build stopped: subcommand failed. 报错看不懂….头晕捏。手忙脚乱了一下午，在病急乱投医的情况下，运行了查看cuda的命令: 1echo $CUDA_HOME 结果发现输出为空，同时又注意到nvcc -V显示的cuda是根目录而非虚拟环境下的cuda，这是否意味着，虚拟环境的cuda没有安装&#x2F;安装失败？安装cuda 的命令，搜集了半天，踩了不少坑，最终确定是这个： 1conda install cuda -c nvidia/label/cuda-12.1.0 虽然echo的结果还是空，但是nvcc -V显示的cuda版本是12.1，对了！再次尝试运行，结果报错： 1KeyError: &#x27;conversations&#x27; 搜了一下，说是数据格式的问题。接下来要调整数据格式。结果发现自己铸币，dict中的conversations少打了一个s😅。一切调整好之后，运行终于成功了！让它在A800上跑着吧！不对，10天太长了！我应该先抽取一点数据，暂定4w条吧，进行训练试试；数据集这一块儿的选取其实还需要仔细斟酌。抽取了数据，训练大概需要7小时；明天看一下结果！ 好吧！来自沈教授的场外支援！我超参数用的默认的，这这不对。现在全数据也只要训17小时！明天中午就好了！ 明日任务：查看训练结果，思考后续优化方向（更好的数据集？） 4.30梳理一下目前的思路吧： 首先，对于目前长文本的解决思路，有两种，一是采用faiss之类的向量库，对输入的query进行重构、检索、增强；这种方法见效快，但是受到模型本身功能的限制；如果模型长文本能力、理解与提炼能力本身不足的话，这个方法遇到瓶颈之后就无法提升了。其次，则是对模型底层进行重构、修改，这种方法理论上可能性是无限的，但是，在实践过程中的问题在于，大模型对我们来说是一个黑箱。正如上面所提到的，在加入infini-attn的时候，模型的基本对话能力大大下降了；新增的参数，会对预训练好的权重造成影响，因而需要进行ft，这里的问题在于：训练数据如何选取？如何训练？评测集是什么？ 一个重点是，上面提到的两种方法，都需要使用到：对于input query的segment&#x2F;cut，以及之后的融合。原因在于，对于前者来说，query长度当然是固定的，我们所追求的目标是在有限的query长度中构建最好的prompt，对于长文本，则可能需要分段精炼再整合、循环往复；而对于后者，尽管不需要在prompt阶段耗费大量经历，但是模型本身的输入也是有长度限制的。目前不知道这个长度限制能否设置去改变，以Qwen-7B-Chat为例，它的输入长度就是512个token。加入infini之后，理论上可以接受无限长的数据，但是需要在generate&#x2F;chat或者哪个传入query的时候对query进行slice，一段段的喂入。效果上来说和全部输入没有区别，但是因为模型限制，需要一段段传入（细想一下不应该这样，或许可以设置输入无限长？） 今早的坏消息是：昨天的train的东西，似乎在我这边ssh断开之后就停止了？总而言之，预定的中午训练完成是没有的。我只好再次从里面提取更少的30w条数据，进行训练，需要三小时，预计今天中午完成。今早，出于同事们接入A800服务器的需求，突然想到可以把我目前在用的转接的这个eth给接到服务器上，这样服务器可以直连公司外网和A800；同时，我们的eth虽然不能访问公司外网，但是可以通过内网连接到服务器。也就是说，只要这样做，就可以都正常使用A800了！试了试果然成功了！昨天调用train.sh的时候，一个有趣的现象是：安装flash-attention后脚本会报错；这是因为我在model里面没改flash-attention的函数，因而加入的参数矩阵似乎因为shape不匹配无法相乘、使用flash-attn加速，卸载之后就好了。当时想要用flash的原因在于，10天的训练时间太长辣！但是后来多亏沈教授的支援，让我在实践中对LLM中ft的超参和作用有了更深的理解。顺便记录一下后续应该会常用的命令： 1234# nvidia-smi实时刷新watch -n 5 nvidia-smi# 查看文件夹下文件及其大小ls -lht 只会显示窗口大小内可以放下的gpu状态，如缩略状态是0-3卡，全屏化只会是0-7卡。查看qwen的各种文件，主要是config文件和chat方法，查看query的处理流程，似乎模型里面query进行split不太难？午休过后，模型跑完了，准备加载模型并检验效果，结果出现了如下报错： 1ValueError: Trying to set a tensor of shape torch.Size([64, 32, 128, 128]) in &quot;M&quot; (which has shape torch.Size([32, 128, 128])), this look incorrect. 对此，GPT大人的答复是： 1这个错误提示表明你正在尝试将一个形状为 [64, 32, 128, 128] 的张量设置到一个形状为 [32, 128, 128] 的张量中 琢磨了一个小时不得其解。与同事讨论之后，去上了个厕所，急躁的心略有平静。灵光一现，问题解决了！首先，最奇怪的是，这个M是register_buffer的cache，按理来说不是一个可学习的参数，在init中设置的初始形状就是三维的。而这里显示的是四维tensor载入到M。这带来的问题是：M不应该被保存到模型中，而是应该在每次query输入&#x2F;对话开始的时候被重新初始化。同时，这个四维tensor的第一维度大小是64，这个64是batch size，是我昨天调整的时候设置的。那么，结合上面的逻辑，这个64维出现在这里，应该是训练的时候，batch中有64个样本，对应64个M，因而最后，register_buffer保存的是这个形状的tensor。在加载模型的时候，M初始化应该是三维的，但是模型文件里面它想从safetensor里面加载M出错了，原因在于形状不符合。那么，问题的关键在于，register buffer的这两个参数不应该被记录到模型文件中去。解决方法很简单，persistent参数设置为False即可。解决这个问题后，用一个数据进行模型的训练、保存，并最后进行模型加载与对话，成功了！重新用之前准备好的数据去训练模型的时间已经不够了，于是我重新准备了COIG-PC中命名实体识别领域的10w条数据，只需要70分钟即可训练完成。同时，由于COIG-PC中这个数据集样本的文本都比较长，感觉也可以同时训练一点长文本和对话能力。一个参考样本数据如下： 当给一个没有标注的中文文本添加实体标记时，可以按照以下简明指示进行操作： 阅读并理解待标注的文本，识别出其中的实体，包括时间、地点、人名、组织名、公司名和产品名。 根据实体的类型，在实体文本两侧使用双大括号（&#123;&#123;&#125;&#125;）进行标记。 在双大括号内部，使用实体类型和冒号分隔实体文本，格式为：&#123;&#123;实体类型: 实体文本&#125;&#125;。 根据实际情况，使用以下标记类型来表示不同的实体： 时间：&#123;&#123;time: 2023年5月29日&#125;&#125; 地点：&#123;&#123;location: 北京&#125;&#125; 人名：&#123;&#123;person_name: 张三&#125;&#125; 组织名：&#123;&#123;org_name: 中国共产党&#125;&#125; 公司名：&#123;&#123;company_name: 腾讯&#125;&#125; 产品名：&#123;&#123;product_name: iPhone 13&#125;&#125; 在整个文本中，通过添加实体标记将所有识别出的实体进行标注。 确保实体标记的格式统一、准确，并与实体文本对应。 请按照如上指示标记下面文本：浙江在线杭州4月25日讯（记者 施宇翔 通讯员 方英）毒贩很“时髦”，用微信交易毒品。没料想警方也很“潮”，将计就计，一举将其擒获。记者从杭州江干区公安分局了解到，经过一个多月的侦查工作，江干区禁毒专案组抓获吸贩毒人员5名，缴获“冰毒”400余克，毒资30000余元，扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓，经常变换住址。他有一辆车，经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓，并且常要活动到凌晨6、7点钟，白天则在家里呼呼大睡。钱某不寻常的特征，引起了警方注意。禁毒大队通过侦查，发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。 训完之后，可以在对话中直接用这个样本测试一下效果。好吧，由于测试数据很单一，结果不好（测试数据是句法分析好像是）我将会用全数据去调一下，看看效果。理论上说，这个infini-attn的结构，用于单一场景应该是见效很快的。训练数据可能需要多样性、包含对话之类的。这个结构说实话不太有趣。我希望能尽量不对已有参数造成影响，而是增强。130G全数据A800也放不下，只能用Top200的数据，多跑几个epoch了。 明日任务：查看训练结果，思考后续优化方向（更好的数据集？）","categories":[{"name":"记录","slug":"记录","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/"},{"name":"大模型","slug":"记录/大模型","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"实习","slug":"记录/大模型/实习","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%AE%9E%E4%B9%A0/"}],"tags":[{"name":"日记","slug":"日记","permalink":"https://yukino256.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"随想","slug":"随想","permalink":"https://yukino256.github.io/tags/%E9%9A%8F%E6%83%B3/"},{"name":"心情","slug":"心情","permalink":"https://yukino256.github.io/tags/%E5%BF%83%E6%83%85/"}]},{"title":"DailyRecord-March","slug":"DailyRecord-March","date":"2024-03-20T12:16:21.000Z","updated":"2024-05-07T01:23:59.055Z","comments":true,"path":"2024/03/20/DailyRecord-March/","link":"","permalink":"https://yukino256.github.io/2024/03/20/DailyRecord-March/","excerpt":"","text":"3.20入职的第一天，接到的任务是：把闻达的demo在服务器上用docker给搭建出来。于是我在服务器上先创建了一个miniconda3的docker，然后使用conda安装了一些依赖、git项目等；其间遇到一个问题：docker+conda后，尽管bash上显示是root，但是好像是一个虚拟的root，没有sudo等的执行文件，需要重新安装一下。此外，公司的网有点差，清华源1Mb、不用清华源只有几十kb。因而进展较慢部署步骤进行到了模型下载这一步，但是因为网不好所以连不上hugging face，需要搭梯子。下班了，明天再搞！ 明日任务：搞定梯子，下载好生成模型和embedding模型，最终完成demo，并调整远程访问展示 3.21今天签合同了。搭梯子问题卡了好久😡。这国企是一天也不能待了，网差的要死。最后还是前辈哥帮忙解决的。但是terminal的命令，下载的那些怎么都那么几把慢啊！网速是压力之源！更悲剧的是：今日闻达demo部署至最后一步无法运行…………..可能是docker的问题，因而将其删除从头开始搭建。如果明天仍然不行，则尝试不用docker直接在服务器上部署 他妈的！狗日的北京通勤太痛苦了！ 明日任务：wenda的demo。哎我真佛了这个b服务器 + docker好难用，不能直接在服务器安装conda吗，conda创建虚拟环境不也挺方便的 3.22创建了一个git的container，但是在安装lfs的时候出了问题，需要用sudo等一些命令，但是apt下载很慢，似乎需要在docker file文件中就更换镜像源了解了docker中与宿主机相互cp文件、docker在创建时需要用GPU对容器可见的命令； docker run -it –net&#x3D;host –gpus all –name 容器名 -e NVIDIA_DRIVER_CAPABILITIES&#x3D;compute,utility -e NVIDIA_VISIBLE_DEVICES&#x3D;all 镜像名 闻达demo终于无报错安装起来了但是运行的时候，模型似乎卡住&#x2F;死循环？不返回计算结果。 他妈的！压力一天比一天大！EMO了要！ 今天美好的事情：与SL、ZYR一起在附近那啥招待所吃川菜。味道不错，就是环境有点热、菜有点辣。此外，和ZYR一起自行车骑行，他一路讲解，聊的很开心！到北京以来为数不多的快乐时间。SL骑着他那破电瓶车，本来跟我们一起的，半路给交警罚款了，就让他回去了，唉，我好难过 下周任务：调整闻达demo 3.23-3.24 周末！早上乘坐地铁到清河，和NJ汇合，一起去清河做了半小时火车到怀来辣！比北京通勤还快。下了火车有一种终于逃离压力之源的舒畅感。中文在饭店吃了什么洋葱和羊肉炒的东西，味道不错；然后还吃了莜面、炒扁豆角；喝了营养快线味的酸奶、杏仁露。杏仁露味道不错，加热和不加热是两个味道吃完饭和NJ一起顺路去菜市场买了点粑粑柑（最后一口没吃带回北京了）。然后不行一阵子去了宾馆，双人间一晚90，条件还挺不错….要哭了睡了午觉，然后出去找网吧打了几把大乱斗；下了新赛季的第一把云顶，玩不明白好痛苦，老六老七出局。然后出去转了转，看了一下县城的风土人情，买了西安特产甑糕，味道也不错。天气不好，阴沉天空使我不太开心。相对于老家，这里明显更荒凉一些，没有人、没有年轻人。晚上吃了砂锅。酸菜白肉和竹笋炖腊肉的砂锅；点了傀儡（土豆和面的混合物作为主食？），咸咸的，有葱花。点了冰糖芦荟，和椰果罐头一个味儿。吃完又转了转。回到宾馆玩了把金铲铲，然后下去买了10斤的半个西瓜和勺子。吃了一点，大头给NJ吃了（真能吃啊这B）睡了一晚，第二天早上吃了豆面还是什么东西，然后走路去买了个吊炉烧饼分着尝尝；NJ点了驴肉和驴肠火烧的外卖，带回北京吃了和NJ回北京了，BYD带我在海淀区骑了好久的自行车，最后在一家新疆馆子吃了抓饭和羊肉什么馍，喝了北京的汽水（和芬达一个味），吃的挺好。就是这b人带我转来转去最后吃这种快餐属实没绷住 3.25闻达demo部署完毕辣！接下来需要：尝试更换通义7B&#x2F;14B的模型；查看闻达前端技术栈，与前端做交接让其修改；部署另一个&#x2F;几个项目（似乎要加班，tmd！）已经将底模更新为7B，但是14B可能需要双卡，还不了解他这个框架怎么放到双卡上；前端框架不太清楚，但是有一个二次开发的前端框架正在尝试，但是服务器网不好，nodejs安装困难。下午看了B站上的一个从零开始大模型，感觉有点收获。还有一部分没看完 明日任务：或许需要看一下另一个agent代理的部署；看langchain和LLM的相关教程视频；RAG目前似乎有一套实践模板，基于faiss检索？ 3.26早上看langchain的视频教程下午部署了wenda的二次开发webui，其中有报错；能保证基本的对话，但是文档对话功能用不了下午抽空去面了腾讯的NLP应用研究实习生，他们是做腾讯视频剧本理解与智能助手的，感觉挺有意思。花了114定了钟点房，可惜面试的时候网不好、代码题（手写交叉熵、双指针&#x2F;滑动数组解决list中满足和为n的最小连续长度）也没敲出来，BERT的损失函数、交叉熵也没回答好。 和SH交流了一下面试的经历，他目前的实习产品经理似乎工资低但是很轻松。他说TX面试结果很快会出，就可以再投了，但是我到第二天也没看到结果部署了langchain-chatchat，可以基本运行，但存在一些问题，如知识库检索似乎没有工作、模型更换没有前端，需要在后端尝试修改 回家，地铁上和ZJX吐槽工作的事情，交流中得到片刻的安慰；到家后，和NJ电话聊了聊，大诉苦水，得到了一些开导和建议，好兄弟！；夜里躺在床上，和LF聊了聊他的离职和走全奖去美国读PHD躺平5年。唏嘘之余，有点羡慕 似乎，上班的人状况都不是很好：ZYR精神衰弱吃药、LF压力大怼领导辞职、SX天天emo不想动，我自己的精神状态也不是很好，唉。唉！ 明日任务：调整部署项目，学习 3.27早上给领导演示了一下两个demo，边上的前辈哥部署的DB-GPT也看了；这个效果挺好，但是也有要调整的地方。如服务器上部署模型的API调用等今天看阿里Qwen的vllm教程视频，大有收获铸币吧，LLM聊天一个字一个字显示的效果原来是清屏+ 打印啊！前端或许就是&lt;div&gt;内部的反复刷新？ 明日任务：调整部署项目，学习 晚上，和妈妈聊了聊天，倾诉了一下。得知妹妹的奶奶肿瘤晚期，突然有一种恐惧和悲伤笼罩心头。帮妈妈网上填写了检查需要的材料，相关材料也进行了保存。与SH、LMD一起打了大乱斗，片刻的开心，但是过程中没有完全放松。 3.28继续学习qwen+vllm的宝藏up主。对asyncio的内容有些不理解。这个up主好像是c++、java高手，学习大模型相关内容上手也很快，我感到很挫败。小米面试，问Qquant还是啥的量化不了解，adapter不了解，代码题一是n个全排列中k个逆序对数量，完全不会；一个是写梯度下降求算术平方根，初始化值、lr和反向传播这些都写的不太好下午，视频看的差不多了，准备看一下Dify是个什么东西晚上，打SH、NJ打游戏，和妈妈视频，开森和沈晓宇老师的师姐聊了聊城市、职业规划、未来发展，感觉认知更清晰了 明日任务：学习 悲苦萦绕心头，不能散去。可能真的是房子住的还是有点远，每天通勤1.5小时。或许我需要换租了。 3.28上午看了一会儿视频，然后捣鼓4090那台机子的docker和conda，没捣鼓出东西来。下午问了前辈，了解怎么搞了，稍微搞了一点下午，搞了会langchain-chatchat，发现推理慢的原因可能是gpu被占了；没找到如何更换模型为qwen 下周任务：qwenllm&#x2F;qwen的docker image本地下载好并上传到远程服务器上，运行查看效果 又 放 假 辣 ！ 3.30-3.31 周末！周六早上，和SH打了LOL，中午出去吃了吉野家；下午躺了一会儿，然后晚上和NJ去逛了古玩市场，感觉有点同质化。之后吃了白糖汁儿的杏仁豆腐，然后吃了永和大王的快餐，赶紧赶地铁回来参加TX的笔试题。但实际上我一面已经挂了，结果现在才出来，这题就没必要做了实际上，而且五题只做出来第一题，似乎打竞赛的才能做得出来多题，唉…又和SH打了一会儿大乱斗看小说，睡觉！","categories":[{"name":"记录","slug":"记录","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/"},{"name":"大模型","slug":"记录/大模型","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"实习","slug":"记录/大模型/实习","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%AE%9E%E4%B9%A0/"}],"tags":[{"name":"日记","slug":"日记","permalink":"https://yukino256.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"随想","slug":"随想","permalink":"https://yukino256.github.io/tags/%E9%9A%8F%E6%83%B3/"},{"name":"心情","slug":"心情","permalink":"https://yukino256.github.io/tags/%E5%BF%83%E6%83%85/"}]},{"title":"第一次实习前的一些准备工作与知识储备","slug":"实习准备","date":"2024-03-14T13:52:22.000Z","updated":"2024-03-17T13:53:12.026Z","comments":true,"path":"2024/03/14/实习准备/","link":"","permalink":"https://yukino256.github.io/2024/03/14/%E5%AE%9E%E4%B9%A0%E5%87%86%E5%A4%87/","excerpt":"","text":"千帆杯原生应用挑战赛 大赛主旨：大赛以“创意无限·生成未来”为主题，紧密围绕当前AI技术的前沿动态和应用趋势，借助百度智能云千帆AppBuilder和ModelBuilder两大智能开发助手，鼓励参赛者打造出更多具有创新性、实用性和社会价值的AI原生应用 第一期：游乐场排队规划助手：赛题聚焦春节假期游乐园排队效率问题，鼓励开发者利用 AI 能力施展“时间魔法”，打造一款具有实用性的“游乐场排队规划助手”，帮助游客更好地了解乐园的排队情况，设计个性化的游玩路线，在有限的时间内获得最“High”的体验，同时为管理者提供优化运营策略的决策支持。 此大赛没有规定数据集，需求成果是使用主办方框架的应用程序。参赛者需要自己获取相关数据，如大赛第一名使用的是香港迪士尼数据 第二期：生成一个可制作贺岁文案内容的精调模型（限定使用ERNIE Speed，通过对模型精调使其保持原有能力的同时，具备准确理解并执行文案创作中创作长度相关指令的能力）。 此大赛提供了少量数据集（56）条，同时要求对数据集进行扩展（最终至少需要100条数据），数据为json形式 与第一期不同，此期是方向特化的微调模型开发，需要使用主办方框架 大赛形式： 第一期：基于AppBuilder平台提供的强大开发套件和资源环境，使用平台预置的Agent框架，以零代码的方式创建Agent智能体，自动化调用各种工具打造游乐场排队规划助手AI原生应用。 游客只需要输入时间预算和游玩喜好，Agent智能体就能生成并执行Python代码，求解优化问题，智能规划出游玩项目路线。————————生成python，求解optimization问题的agent 第二期：通过在千帆大模型平台使用平台上的各种模型调优工具，结合相关数据，基于ERNIE-Speed调优生成符合赛题主题要求且效果优秀的模型。 作品信息需包含： 微调后的模型效果展示（输入输出示例截图） 部署后的模型API文档（包含url地址、超参配置和步骤描述） access_token； 阿里天池竞赛1. 基于LLM智能问答系统学习赛 赛题思想：未来金融科技领域将深刻体现Agent的价值，即一个智能代理能根据用户需求进行意图识别和决策。本次大赛的赛题虽为单一，但融合了数据查询与文本理解两大任务，充分体现了Agent核心思想：根据不确定输入，判断用户意图，并调用相应服务或功能生成答案。 模型使用：不限制选手的模型使用，选手可以选择商业化模型或者开源模型，也可以结合多个模型，共同创建一个问答系统。可以采用Prompt Engineering方法，也可以使用外部数据对模型进行微调。推荐使用“通义千问金融大模型”或“通义千问7B模型”作为基础大模型， 任务目标：数据查询题——根据用户的问题，生成相应的SQL查询语句，精准查询问题结果；文本理解题：对长文本进行细致检索与解读，高效提取关键信息。 数据集描述：赛事主办方提供三类数据。一个是10张数据表（sqlite），一个是招股说明书（pdf文件，标准的招股说明书形式，没有进行数据处理），以及将招股说明书pdf解析后的txt文件。 大模型拓展操作：扩展金融行业词表；增量训练行业金融200B规模，涵盖中英文财报、研报、新闻、书籍、论坛等多种类型数据；训练上下文扩展到16K，借助NTK和LogN等技术，推理长度可以扩展到64K 2. 通义千问AI挑战赛 - Code Qwen能力算法赛道 目的：如何通过高质量的数据微调提升基础语言模型的代码能力？——聚焦于通义千问大模型微调训练的竞赛，其主要目标是通过高质量的数据探索和拓展开源模型 Qwen 1.8B 及 Qwen 72B 的代码能力上限。高质量的数据是大模型提升效果的关键，初赛阶段主要聚焦在如何通过 SFT 提升基础模型的代码能力。需要选手基于最新开源的 Qwen 1.8 模型作为基础模型，在提供的训练框架上自行进行数据收集与微调，训练完成后将进行自动评估，返回最终结果进行排名； 数据集特征 Evol-instruction-66k： 基于开源open-evol-instruction-80k过滤低质量，重复和human eval相似的数据后得到的高质量代码类微调数据 e.x.{ “instruction”: “Using the text file from the link, https://norvig.com/big.txt, use the SpellChecker module from the pyspellchecker package in Python to find spelling mistakes in the text file. Additionally, measure the efficiency of the implemented spell checker. Report the number of false positives and false negatives, and provide a way to visualize these instances. Also, improve the spell checker by using custom dictionaries or other natural language processing techniques to reduce these errors.”, “output”: “Here is an outline of how to address this problem:\\n(Note we will need to install pyspellchecker and requests dependencies if not present already). You can install it using:\\n\\npip install pyspellchecker requests\\n\\n\\npython\\nimport requests\\nfrom spellchecker import SpellChecker\\n\\n# Get the text file\\nurl = \\&quot;https://norvig.com/big.txt\\&quot;\\nresponse = requests.get(url)\\ntext = response.text\\n\\n# Remove punctuation and special characters\\ntext = &#39;&#39;.join(e for e in text if e.isalnum() or e.isspace())\\n\\n# Tokenize text (naive method)\\nwords = text.split()\\n\\n# Initialize spell checker\\nspell = SpellChecker()\\n\\n# Find misspelled words\\nmisspelled = spell.unknown(words)\\n\\n# Print result\\nprint(f\\&quot;Number of misspelled words: &#123;len(misspelled)&#125;\\&quot;)\\n\\nThis snippet will output a number of misspelled words found in the text.\\n\\nIn order to measure the efficiency in terms of false negatives and false positives, you need to know the true positives and true negatives, which requires the ground truth that in this case would be some kind of ‘clean’ version of the text without typos. \\n\\nWe can perform a counter-example with a small list that has good and bad spelled word:\\npython\\n# Assuming we have two lists which contain correctly spelled and incorrectly spelled words.\\ncorrectly_spelled = [&#39;apple&#39;, &#39;banana&#39;, &#39;pear&#39;]\\nincorrectly_spelled = [&#39;aple&#39;, &#39;banan&#39;, &#39;per&#39;]\\n\\n# Everything the spellchecker says is fine should be in the correctly spelled list.\\nfalse_negatives = [word for word in correctly_spelled + incorrectly_spelled if word not in spell.known([word])]\\n\\n# Everything the spellchecker says is wrong should be in the incorrectly spelled list.\\nfalse_positives = [word for word in correctly_spelled + incorrectly_spelled if word not in spell.unknown([word])]\\n\\nprint(f\\&quot;False Negatives: &#123;false_negatives&#125;\\&quot;)\\nprint(f\\&quot;False Positives: &#123;false_positives&#125;\\&quot;)\\n\\n\\nVisualizing results can be done in many ways depending on your preferences and exact needs. In simple cases, you could list the incorrectly spelled words found by the spell checker, but you could also create a plot using matplotlib, or a word cloud using the WordCloud module.\\n\\nTo improve the spell checker, you can add words to its dictionary. The pyspellchecker has a corpus we can add our words, as shown below:\\n\\npython\\nupdated_dictionary = set(list(spell.word_frequency.dictionary.keys()) + [\\&quot;additional\\&quot;, \\&quot;dictionary\\&quot;, \\&quot;words\\&quot;])\\nspell.word_frequency.load_words(updated_dictionary)\\n\\n\\nIf you have a custom dictionary, you can just load this directly.\\n\\nPyspellchecker probabilities are based on the occurrence of a word in the text. As a result, the more domain-specific text you can provide it, the more accurate it becomes.\\n\\nAnother optimization could be to apply a stemming algorithm before comparing words to the dictionary since the dictionary typically includes only root words.\\n\\nIn addition, you can use Machine Learning or Natural Language Processing techniques to improve spell check. Techniques such as:\\n1. Using a context-based spelling correction by using BERT model. \\n2. Training a customized spell-check model using a dataset relevant to your problem.\\n3. Using N-gram approach, where the context of the word is used for the correction (this is used in Google\\u2019s spell checker as of 2010).\\n4. Use of phonetic algorithms (e.g., Soundex, Metaphone, etc.) which are used to correct phonetically misspelled words.”} CodeExercise-Python-27k： 基于chatgpt生成的高质量python练习题数据 e.x.{“data_name”: “CodeExercise-Py”, “id”: 0, “chat_rounds”: [{“role”: “human”, “content”: “Write a Python function named &quot;add_numbers&quot; that takes two parameters, &quot;num1&quot; and &quot;num2&quot;, and returns the sum of the two numbers.\\nInput: None”, “chat_round_id”: 0}, {“role”: “bot”, “content”: “python\\ndef add_numbers(num1, num2):\\n \\&quot;\\&quot;\\&quot;\\n This function takes two numbers as input and returns their sum.\\n\\n Parameters:\\n num1 (int or float): The first number.\\n num2 (int or float): The second number.\\n\\n Returns:\\n int or float: The sum of num1 and num2.\\n \\&quot;\\&quot;\\&quot;\\n return num1 + num2\\n\\n# Example usage\\nresult = add_numbers(5, 3)\\nprint(result) # Output: 8\\n\\n\\n“, “chat_round_id”: 1}]} 闻达：一个大规模语言模型调用平台1. 知识库检索增强 sentence_transformers + 检索，支持faiss&#x2F;fess检索，可以选择本地&#x2F;在线检索、索引预先&#x2F;实时构建 没有提到长文本能力，仅仅是取决于model本身的功能 DeepBI1. 定义&#x2F;底模 基于OPENAI GPT4 模型开发的BI系统，可以自动生成sql与图表 核心问题是：如何在通过API的情况下优化SQL生成与结果分析？图表生成是模型生成还是调用某些库方法？ 初步判断是用了python的第三方包，如pyecharts等进行图表生成；完全调用GPT4 API，似乎仅在prompt上做了一些简单的处理？ 还是midware的思想；在代码生成上，没有做到交叉生成，python和sql生成是分开的 CodeT5+ 模型结构：encoder-decoder结构 编码器学习从代码&#x2F;文本序列（完整、部分或跨度屏蔽序列）中对上下文表示进行编码。 根据预训练学习任务，解码器被训练以生成不同类型的输出。 预训练任务的混合使模型能够学习代码上下文的有意义的表示，并在不同级别恢复丢失的信息：代码跨度、部分程序和完整程序。 训练策略：多任务混合 目的是增强LLM的理解能力 结合了不同类型的学习任务，包括span denoising, causal language modeling (CLM), text-code contrastive learning, and matching tasks（跨度去噪、因果语言建模（CLM）、文本代码对比学习和匹配任务）。他们发现，如此广泛的预训练任务集可以帮助模型从代码和文本数据中学习丰富的表示，并弥合各种应用中的预训练微调差距。 如何实现通过对模型进行调整加速训练？—————shallow encoder and deep decoder策略 keep the small encoder and the cross-attention layers trainable while freezing the deep decoder LLM. ————Such architecture is designed with the intuition that the decoder is often employed to deal with a higher level of complexity in generation tasks and requires a larger number of neural parameters. 训练trick unimodal code data and bimodal code-text data.（纯代码 + 代码与注释？） 二阶段预训练策略（实践证明可以增强更丰富的文本表示） 第一阶段只采用纯代码训练：从GitHub等开源平台获得了数据（许可证）。总共采用了九种编程语言的多语言培训数据。 第二阶段采用代码+文本作为训练数据：每个数据样本都包含一个文本代码对，其中包括一个代码函数及其相应的描述函数语义的文档字符串。 Instruction Tuning tuning the models with synthetic instruction-following tasks 下列为可能参考到的论文及启发点————LLM agent for generating code with python and SQL1. CleanAgent: Automating Data Standardization with LLM-based Agents 提出一个具有声明性、统一 API 的 Python 库，用于标准化列类型，通过简洁的 API 调用简化 LLM 的代码生成。 类似与midware的思想，加了一个中间层方便tuning，实际价值似乎不大 2. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code 一套code generation效果评价方法，MIT伯克利大学出品，或许可以一试 3. KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction 中国科学院大学CAS Key Laboratory of Network Data Science and Technology出品 旨在开发一种LLM易于理解的统一模式表示，以及鼓励LLM遵循模式并准确提取结构化知识的有效学习框架（Universal Information Extraction (UIE) task） 已有UIE方法的一些不足：在类别及其归属的判定上、关联与实体的存在条件判断上不足；classification label&#x2F;自然语言 对LLM来说需要更多资源去理解；缺乏通用性 此文章实际上是用python代码来表示知识和信息，是information to python再to knowledge的过程","categories":[],"tags":[]},{"title":"云服务器使用及QQ机器人搭建","slug":"PcrQQbot","date":"2023-06-15T10:36:51.000Z","updated":"2023-06-15T11:12:36.159Z","comments":true,"path":"2023/06/15/PcrQQbot/","link":"","permalink":"https://yukino256.github.io/2023/06/15/PcrQQbot/","excerpt":"","text":"1. 服务器选择三个主流的：阿里云、腾讯云、华为云。萌新的我需要三选一华为云没用过。阿里云学生认证可以白嫖，腾讯云学生认证是打折，看着蛮便宜，但是这个时间买是1.9折，去年11月是1.1折。 服务器使用阿里云，因为可以白嫖7个月，新手上路还是找免费的试一下比较好，随便造，造坏了也不心疼。 cpu、内存这些东西就是看钱的啦，根据需要购买合适的就行。系统这个东西，不太清楚是怎么选择的。本人选择Ubuntu的依据是：尽管windows很熟悉，但是几个资深程序员的哥们儿用的都是Linux内核系统；此外，Liunx相对于windows的内存占用要小太多了，服务器还是要抠抠搜搜一点。Linux提供服务好像也蛮方便的 2. 服务器操作跟着阿里的教程走下来，基本上就会怎么操纵服务器了。远程连接服务器有两种方式：Workbench和VNC，前者似乎只能用命令行输入，后者则可以安装GUI。 实际上，不同云服务商对于自己云服务器的名称和操纵方式是有所不同的。 3. 图形界面的安装我的一个朋友对我说，只有当你能够用命令行完成你想要的所有操作后，你才算是真正懂计算机，我觉得很有道理。目前还没有这个能力，先用带GUI的VNC试一下，后续需要理解Workbench的命令行是什么东西，如何操作。其实目前在没有GUI的情况下无法理解系统内部结构。 GUI安装流程的方法，阿里云本身有提供，最终选择的系统是Ubuntun20.04。一开始选择的18.04的造了半天python安装还是出问题。因为机器人需求3.8的python，18.04默认是3.6，升级过程中会出现很多问题。 4. PCR会战机器人的选择之前主流是yobot，但是会战改版后用不了了，开发者不更新了。HoshinoBot是我一开始配置的，结果用不了会战功能，但是其他花里胡哨的似乎挺全。配置方法按照这个Liunx的安装流程走就是了。 实际上好像在哪里都无所谓？但是部署在这个位置的时候，运行.bat的时候，命令行显示的是： 顶级难蚌：18.04在numpy上安装疯狂失败，20.04一路绿灯😅 我这个服务器的带宽低，不用清华镜像的时候很慢。应该加上镜像设置的命令😇 1`python3.8 -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt` yobot_remix是yobot针对新版公会战的魔改，实测公会战能用。由于是基于yobot改的，命令和部署方法大体也和yobot相同 5. 机器人没响应典中典，被风控了，可以自行搜索解决方法。这个里面的高赞回答很有用 后记 阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋","categories":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/categories/python/"},{"name":"Liunx","slug":"python/Liunx","permalink":"https://yukino256.github.io/categories/python/Liunx/"},{"name":"Ubuntu20.04","slug":"python/Liunx/Ubuntu20-04","permalink":"https://yukino256.github.io/categories/python/Liunx/Ubuntu20-04/"}],"tags":[{"name":"QQbot","slug":"QQbot","permalink":"https://yukino256.github.io/tags/QQbot/"},{"name":"阿里云","slug":"阿里云","permalink":"https://yukino256.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"cloud_server","slug":"cloud-server","permalink":"https://yukino256.github.io/tags/cloud-server/"}]},{"title":"AI绘画初步尝试：Stable Difussion本地部署及WebUI使用","slug":"AIdrawing","date":"2023-04-25T06:10:31.000Z","updated":"2023-04-25T07:30:37.718Z","comments":true,"path":"2023/04/25/AIdrawing/","link":"","permalink":"https://yukino256.github.io/2023/04/25/AIdrawing/","excerpt":"","text":"1. 安装conda、git、cudaCuda版本需要首先查看自己nvida显卡参数，不能高于参数上的版本 2. Conda创建环境，python版本根据GitHub上的说明来选择 1`conda create --name sdweb python=3.10.6` 3. 进入创建的虚拟环境1`conda activate sdweb` 4. 在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘….1`git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git` 实际上好像在哪里都无所谓？但是部署在这个位置的时候，运行.bat的时候，命令行显示的是： 123`Creating venv in directory D:\\Anaconda3\\envs\\sdweb\\stable-diffusion-webui\\venv using python &quot;D:\\Anaconda3\\python.exe&quot;``venv &quot;D:\\Anaconda3\\envs\\sdweb\\stable-diffusion-webui\\venv\\Scripts\\Python.exe&quot;``Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar 1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]` 没有用虚拟环境下的python版本哎….但是存储位置好歹是对了 😇 不对！或许应该在虚拟环境下用命令行运行这个.bat而不是点击！ 环境对了，但是运行时系统突然卡死一段时间……不敢动不敢动 😰错误显示是Torch安装失败，这个文章有提到把.bat的代码改一下，尝试之。 第六行改为： 1`set COMMANDLINE_ARGS=--lowvram --precision full --no-half --skip-torch-cuda-test` 这个.bat文件是另一个.bat的参数传入文件。 又寄啦，而且途中很卡 😅 5. 还是看看电子佛祖布施的法器吧嘉人们 😋 【AI绘画】Stable Diffusion整合包v4 BV1iM4y1y7oA","categories":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/categories/python/"},{"name":"algorithm","slug":"python/algorithm","permalink":"https://yukino256.github.io/categories/python/algorithm/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://yukino256.github.io/tags/AI/"},{"name":"Stable Difussion","slug":"Stable-Difussion","permalink":"https://yukino256.github.io/tags/Stable-Difussion/"},{"name":"本地部署","slug":"本地部署","permalink":"https://yukino256.github.io/tags/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/"}]},{"title":"记录自己的初次搭建博客的经历——流程、路径与感悟","slug":"BlogBuilding","date":"2023-04-24T07:30:58.000Z","updated":"2024-09-17T12:13:09.774Z","comments":true,"path":"2023/04/24/BlogBuilding/","link":"","permalink":"https://yukino256.github.io/2023/04/24/BlogBuilding/","excerpt":"","text":"安装流程可以参见 这篇文章部署时尤其需要注意的设置：将hexo设置里面的分支名称改成与GitHub一致，因为前两年GitHub把main branch名字改了.. 1.网站部署的原理和逻辑是什么？在最开始，面对各种搭建网站的框架，本人其实是迷茫的，因为我一时间无法理解为什么经过这些步骤就能够产出一个可以被访问的页面？直到我在站点快搭建完成时，我才突然明白其原理，或许接下来的要说理解有点不准确，但是仍然在这里记录一下： 首先，需要理解的就是，网络上所有的资源都是以二进制的方式传输的，网站也是这样，传输的是010101的数据流，而不是直接将页面呈现在电脑上。数据在经过网络传输至本地的时候，需要本地利用某种“解码软件”将其转换成更高级的、便于用户阅读、输入的模式。浏览器其实就是将01数据解析成html等代码，然后再通过渲染、代码执行，将数据以GUI的形式呈现出来的解析器。 那么，为什么可以将页面文件夹的一整个数据包托管至某个服务器，从而能够对外可见呢？在GitHub中，如果你建立一个仓库，并将网站数据上传至仓库，利用GitHub Pages功能就可以将个人博客发布在互联网上。实际上，通过给仓库开通Pages功能，在输入 “用户名.github.io“ 时，你实际访问的是GitHub页面下你仓库的地址（这么说不知道对不对？），然后GitHub在你访问的时候，会检查仓库中是否有index.html这一个文件，如果有，则告诉浏览器，让浏览器解析仓库内的数据资源，从而使得页面能够呈现出来。 为什么是index.html？因为其是所有网站建设者约定俗成的一个最初始的入口文件，就好像python包中的__init__.py一样。 在理清上述逻辑之后，那么我们就可以理解到一个事实：站点部署在哪里都是无所谓的，只要站点托管的地方能够提供这样一种”判断” “解析”的服务即可，自己的电脑也可以作为server。只是PC经常保持不在线的状态，会影响用户访问。所以购买一个专门负责发送数据的服务器成为网站搭建的必要步骤，这种服务器专门处理访问网站的请求，效率上相对于PC高得多。 2.站点部署在哪？Github Pages，部署在这里的原因有两个：一是免费，二是顺带学习一下Github的用法 部署在这也有缺点，那就是国内访问不稳定 3.部署步骤？ git工具的安装、npm安装、Node.js安装（？）Node.js和npm分别是什么？————问问chatGPT吧！ 使用git将本地电脑与GitHub账户绑定 创建一个Github仓库，命名格式严格要求为：用户名.github.io 使用git命令行，将本地电脑的某个地址与github仓库关联，这个地址即是博客的文件所在，我命名为Blog 在Blog下，用git bash安装Hexo 使用Hexo命令，创建、测试、部署 4.主题个性化修改 选择一个主题并根据github上面提供的主题使用方法进行配置，此步骤git命令往往是在Blog这个根目录下进行 主题目录参见Hexo主题官方集合 经过一系列尝试，最终使用butterfly主题 个性化设置参见butterfly官方文档集，这是创作者的博客，博文全是详细的设置如何调整 5.博文上传与修改 需要注意的是，Hexo框架才是个人博客的架构，博客发表、修改等应该使用Hexo命令来执行 butterfly只是一个美化的框架，负责显示效果的渲染和调整 使用hexo命令创建新博文后，在source文件夹下可以找到。博文需要以md格式写作，本人使用VScode编写 md语法写作规范：Markdown官方教程 6.过程中踩过的坑仓库中README.md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来这个问题是Hexo的设置问题，不知道官方文档有没有提到，但是解决方法是：在Hexo目录下的source根目录下添加一个README.md。修改Hexo目录下的_config.yml。将skip_render参数的值设置上。skip_render: README.md保存退出即可。使用hexo d 命令就不会在渲染 README.md 这个文件了。 无法在仓库中手动添加README.md,因为部署时候会将仓库内容完全顶替为本地内容，只有在本地文件中也设置README才行。 网站在本地测试渲染失败可能是缺少相应的包，重新安装一遍即可，如果不确定，就从头都试一下通常而言，应该安装的包有：Hexo所需的包+所用主题渲染所需的包 引出一个问题：包的作用是什么？ 主题包：hexo-theme-fluid 是加入主题 渲染包：hexo-renderer-scss 是加入渲染引擎，因为Hexo里面可能并不支持主题内的一些样式 又引出一个问题：为什么不支持？Hexo究竟是什么？本人尚未了解，但是可以参见Hexo官方文档 网站本地测试成功，部署到GitHub后远程渲染失败？按F12，查看console，发现报错是Failed to load resource: the server responded with a status of 404 ()原因在于国内与GitHub的连接犯抽……..一些文件没下载回来因而无法加载 因为这个错误换了好多主题….本来其他主题也蛮漂亮的唉… 但是新页面还有问题，可能在我不知道的地方起了作用，尽管显示没有问题：Error with Permissions-Policy header: Origin trial controlled feature not enabled: ‘interest-cohort’. 其实在途中还遇到一个错误，稀里糊涂就没有了。 7.后续考虑加入的功能博客定时自动更新很简单，没搞是因为没必要，学习是需要时间的，不需要频繁更新……. 动态页面真正理解Hexo的底层原理8.一些感想 在配置过程中遇到过很多报错、和不了解的地方，直接搜索报错结果固然可以找到解决方法，但是很多可以查阅官方文档来解决。查阅固然快，但往往会使得”知其然而不知其所以然”。直接搜答案的过程和询问chatGPT很类似，工程性的问题只答复解决步骤，而不分析背后原理 Hexo也许是一套程序，自动提供一套框架，功能是将我们提交的文本，按照模板或者自己设定的样子渲染出来。同样功能的还要jekell，但是好像是Linux上比较好用。Hexo是静态网页框架，对于博客够用，但是动态网页框架才是未来的选择。 在编写md的时候还误用了中文符号导致md语法出错…….wssb 部署后发现页面没更新？刷新试试！再不然就开梯子刷新试试！ 原来写正文的时候不用自动换行….否则会突然换行，很难看.. Markdown语法：加粗这类修饰语法，前后如果有标点符号，则要视情况加空格…… 每次更新，clean、g、d三条命令必须走一趟啊….我还以为用更新用g、d就行了呢 确实只用g和d就行了帖子们。我也不知道当时为什么会这么感觉，呜呜呜😭😭😭。 bolg内插入图片失败…解决方法是这篇博客","categories":[{"name":"markdown","slug":"markdown","permalink":"https://yukino256.github.io/categories/markdown/"},{"name":"js","slug":"markdown/js","permalink":"https://yukino256.github.io/categories/markdown/js/"},{"name":"css","slug":"markdown/js/css","permalink":"https://yukino256.github.io/categories/markdown/js/css/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://yukino256.github.io/tags/Hexo/"},{"name":"Blog","slug":"Blog","permalink":"https://yukino256.github.io/tags/Blog/"},{"name":"Git","slug":"Git","permalink":"https://yukino256.github.io/tags/Git/"},{"name":"Github","slug":"Github","permalink":"https://yukino256.github.io/tags/Github/"}]},{"title":"YukinoのBlog","slug":"bolgname","date":"2023-04-20T03:33:24.000Z","updated":"2023-04-24T07:24:32.896Z","comments":true,"path":"2023/04/20/bolgname/","link":"","permalink":"https://yukino256.github.io/2023/04/20/bolgname/","excerpt":"","text":"这是搭建自己第一个博客后的第一篇文章","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2023-04-20T03:30:32.320Z","updated":"2023-04-20T03:30:32.320Z","comments":true,"path":"2023/04/20/hello-world/","link":"","permalink":"https://yukino256.github.io/2023/04/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"大模型， 强化学习","slug":"大模型，-强化学习","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"大模型， 基础知识","slug":"大模型，-基础知识","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"大模型， 并行策略","slug":"大模型，-并行策略","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C-%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"name":"大模型","slug":"大模型","permalink":"https://yukino256.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"日常","slug":"日常","permalink":"https://yukino256.github.io/categories/%E6%97%A5%E5%B8%B8/"},{"name":"记录","slug":"记录","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/"},{"name":"大模型","slug":"记录/大模型","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"实习","slug":"记录/大模型/实习","permalink":"https://yukino256.github.io/categories/%E8%AE%B0%E5%BD%95/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%AE%9E%E4%B9%A0/"},{"name":"python","slug":"python","permalink":"https://yukino256.github.io/categories/python/"},{"name":"Liunx","slug":"python/Liunx","permalink":"https://yukino256.github.io/categories/python/Liunx/"},{"name":"Ubuntu20.04","slug":"python/Liunx/Ubuntu20-04","permalink":"https://yukino256.github.io/categories/python/Liunx/Ubuntu20-04/"},{"name":"algorithm","slug":"python/algorithm","permalink":"https://yukino256.github.io/categories/python/algorithm/"},{"name":"markdown","slug":"markdown","permalink":"https://yukino256.github.io/categories/markdown/"},{"name":"js","slug":"markdown/js","permalink":"https://yukino256.github.io/categories/markdown/js/"},{"name":"css","slug":"markdown/js/css","permalink":"https://yukino256.github.io/categories/markdown/js/css/"}],"tags":[{"name":"python","slug":"python","permalink":"https://yukino256.github.io/tags/python/"},{"name":"LLM， RL","slug":"LLM，-RL","permalink":"https://yukino256.github.io/tags/LLM%EF%BC%8C-RL/"},{"name":"LLM， 基础","slug":"LLM，-基础","permalink":"https://yukino256.github.io/tags/LLM%EF%BC%8C-%E5%9F%BA%E7%A1%80/"},{"name":"LLM， parallelism","slug":"LLM，-parallelism","permalink":"https://yukino256.github.io/tags/LLM%EF%BC%8C-parallelism/"},{"name":"bash","slug":"bash","permalink":"https://yukino256.github.io/tags/bash/"},{"name":"LLM","slug":"LLM","permalink":"https://yukino256.github.io/tags/LLM/"},{"name":"adam","slug":"adam","permalink":"https://yukino256.github.io/tags/adam/"},{"name":"adamW","slug":"adamW","permalink":"https://yukino256.github.io/tags/adamW/"},{"name":"优化器","slug":"优化器","permalink":"https://yukino256.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"长文本","slug":"长文本","permalink":"https://yukino256.github.io/tags/%E9%95%BF%E6%96%87%E6%9C%AC/"},{"name":"YaRN","slug":"YaRN","permalink":"https://yukino256.github.io/tags/YaRN/"},{"name":"PoSE","slug":"PoSE","permalink":"https://yukino256.github.io/tags/PoSE/"},{"name":"概率","slug":"概率","permalink":"https://yukino256.github.io/tags/%E6%A6%82%E7%8E%87/"},{"name":"机器学习","slug":"机器学习","permalink":"https://yukino256.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"输入法算法","slug":"输入法算法","permalink":"https://yukino256.github.io/tags/%E8%BE%93%E5%85%A5%E6%B3%95%E7%AE%97%E6%B3%95/"},{"name":"日记","slug":"日记","permalink":"https://yukino256.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"随想","slug":"随想","permalink":"https://yukino256.github.io/tags/%E9%9A%8F%E6%83%B3/"},{"name":"心情","slug":"心情","permalink":"https://yukino256.github.io/tags/%E5%BF%83%E6%83%85/"},{"name":"QQbot","slug":"QQbot","permalink":"https://yukino256.github.io/tags/QQbot/"},{"name":"阿里云","slug":"阿里云","permalink":"https://yukino256.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"cloud_server","slug":"cloud-server","permalink":"https://yukino256.github.io/tags/cloud-server/"},{"name":"AI","slug":"AI","permalink":"https://yukino256.github.io/tags/AI/"},{"name":"Stable Difussion","slug":"Stable-Difussion","permalink":"https://yukino256.github.io/tags/Stable-Difussion/"},{"name":"本地部署","slug":"本地部署","permalink":"https://yukino256.github.io/tags/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/"},{"name":"Hexo","slug":"Hexo","permalink":"https://yukino256.github.io/tags/Hexo/"},{"name":"Blog","slug":"Blog","permalink":"https://yukino256.github.io/tags/Blog/"},{"name":"Git","slug":"Git","permalink":"https://yukino256.github.io/tags/Git/"},{"name":"Github","slug":"Github","permalink":"https://yukino256.github.io/tags/Github/"}]}