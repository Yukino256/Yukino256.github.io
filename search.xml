<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI绘画初步尝试：Stable Difussion本地部署及WebUI使用</title>
    <url>/2023/04/25/AIdrawing/</url>
    <content><![CDATA[<h2 id="1-安装conda、git、cuda"><a href="#1-安装conda、git、cuda" class="headerlink" title="1.	安装conda、git、cuda"></a>1.	安装conda、git、cuda</h2><p>Cuda版本需要首先查看自己nvida显卡参数，不能高于参数上的版本</p>
<h2 id="2-Conda创建环境，"><a href="#2-Conda创建环境，" class="headerlink" title="2.	Conda创建环境，"></a>2.	Conda创建环境，</h2><p>python版本根据<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">GitHub上的说明</a>来选择</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`conda create --name sdweb python=3.10.6`</span><br></pre></td></tr></table></figure>

<h2 id="3-进入创建的虚拟环境"><a href="#3-进入创建的虚拟环境" class="headerlink" title="3.	进入创建的虚拟环境"></a>3.	进入创建的虚拟环境</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`conda activate sdweb`</span><br></pre></td></tr></table></figure>

<h2 id="4-在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘…"><a href="#4-在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘…" class="headerlink" title="4.	在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘…."></a>4.	在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘….</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`</span><br></pre></td></tr></table></figure>

<p>实际上好像在哪里都无所谓？但是部署在这个位置的时候，运行.bat的时候，命令行显示的是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`Creating venv in directory D:\Anaconda3\envs\sdweb\stable-diffusion-webui\venv using python &quot;D:\Anaconda3\python.exe&quot;`</span><br><span class="line">`venv &quot;D:\Anaconda3\envs\sdweb\stable-diffusion-webui\venv\Scripts\Python.exe&quot;`</span><br><span class="line">`Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]`</span><br></pre></td></tr></table></figure>

<p>没有用虚拟环境下的python版本哎….但是存储位置好歹是对了 😇</p>
<p>不对！或许应该在虚拟环境下用命令行运行这个.bat而不是点击！</p>
<p>环境对了，但是运行时系统突然卡死一段时间……不敢动不敢动 😰<br>错误显示是Torch安装失败<img src="error1.png" alt="error" title="错误报告">，这个文章有提到把.bat的代码改一下，尝试之。</p>
<p>第六行改为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`set COMMANDLINE_ARGS=--lowvram --precision full --no-half --skip-torch-cuda-test`</span><br></pre></td></tr></table></figure>

<p>这个.bat文件是另一个.bat的参数传入文件。</p>
<p>又寄啦，而且途中很卡 😅</p>
<hr>
<h2 id="5-还是看看电子佛祖布施的法器吧嘉人们-😋"><a href="#5-还是看看电子佛祖布施的法器吧嘉人们-😋" class="headerlink" title="5.	还是看看电子佛祖布施的法器吧嘉人们 😋"></a>5.	还是看看电子佛祖布施的法器吧嘉人们 😋</h2><p> <strong>【AI绘画】Stable Diffusion整合包v4  <a href="https://www.bilibili.com/video/BV1iM4y1y7oA">BV1iM4y1y7oA</a></strong></p>
]]></content>
      <categories>
        <category>python</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>Stable Difussion</tag>
        <tag>本地部署</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/04/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>记录自己的初次搭建博客的经历——流程、路径与感悟</title>
    <url>/2023/04/24/BlogBuilding/</url>
    <content><![CDATA[<p>安装流程可以参见 [这篇文章] (<a href="https://zhuanlan.zhihu.com/p/158975269">https://zhuanlan.zhihu.com/p/158975269</a>)<br>部署时尤其需要注意的设置：将hexo设置里面的分支名称改成与GitHub<a href="https://zhuanlan.zhihu.com/p/345841098">一致</a>，因为前两年GitHub把main branch名字改了..</p>
<h2 id="1-网站部署的原理和逻辑是什么？"><a href="#1-网站部署的原理和逻辑是什么？" class="headerlink" title="1.网站部署的原理和逻辑是什么？"></a>1.网站部署的原理和逻辑是什么？</h2><p>在最开始，面对各种搭建网站的框架，本人其实是迷茫的，因为我一时间无法理解为什么经过这些步骤就能够产出一个可以被访问的页面？<br>直到我在站点快搭建完成时，我才突然明白其原理，或许接下来的要说理解有点不准确，但是仍然在这里记录一下：  </p>
<ol>
<li>首先，需要理解的就是，网络上所有的资源都是以二进制的方式传输的，网站也是这样，传输的是010101的数据流，而不是直接将页面呈现在电脑上。数据在经过网络传输至本地的时候，需要本地利用某种“解码软件”将其转换成更高级的、便于用户阅读、输入的模式。浏览器其实就是将01数据解析成html等代码，然后再通过渲染、代码执行，将数据以GUI的形式呈现出来的解析器。</li>
<li>那么，为什么可以将页面文件夹的一整个数据包托管至某个服务器，从而能够对外可见呢？在GitHub中，如果你建立一个仓库，并将网站数据上传至仓库，利用GitHub Pages功能就可以将个人博客发布在互联网上。实际上，通过给仓库开通Pages功能，在输入 <strong>“用户名.github.io</strong>“ 时，你实际访问的是GitHub页面下你仓库的地址（这么说不知道对不对？），然后GitHub在你访问的时候，会检查仓库中是否有index.html这一个文件，如果有，则告诉浏览器，让浏览器解析仓库内的数据资源，从而使得页面能够呈现出来。</li>
<li>为什么是index.html？因为其是所有网站建设者约定俗成的一个最初始的入口文件，就好像python包中的__init__.py一样。</li>
<li>在理清上述逻辑之后，那么我们就可以理解到一个事实：站点部署在哪里都是无所谓的，只要站点托管的地方能够提供这样一种”判断”  “解析”的服务即可，自己的电脑也可以作为server。只是PC经常保持不在线的状态，会影响用户访问。所以购买一个专门负责发送数据的服务器成为网站搭建的必要步骤，这种服务器专门处理访问网站的请求，效率上相对于PC高得多。</li>
</ol>
<h2 id="2-站点部署在哪？"><a href="#2-站点部署在哪？" class="headerlink" title="2.站点部署在哪？"></a>2.站点部署在哪？</h2><p>Github Pages，部署在这里的原因有两个：一是免费，二是顺带学习一下Github的用法</p>
<p>部署在这也有缺点，那就是国内访问不稳定</p>
<h2 id="3-部署步骤？"><a href="#3-部署步骤？" class="headerlink" title="3.部署步骤？"></a>3.部署步骤？</h2><ul>
<li>git工具的安装、npm安装、Node.js安装（？）<br>Node.js和npm分别是什么？————问问chatGPT吧！</li>
<li>使用git将本地电脑与GitHub账户绑定</li>
<li>创建一个Github仓库，命名格式严格要求为：用户名.github.io</li>
<li>使用git命令行，将本地电脑的某个地址与github仓库关联，这个地址即是博客的文件所在，我命名为Blog</li>
<li>在Blog下，用git bash安装Hexo</li>
<li>使用Hexo命令，创建、测试、部署</li>
</ul>
<h2 id="4-主题个性化修改"><a href="#4-主题个性化修改" class="headerlink" title="4.主题个性化修改"></a>4.主题个性化修改</h2><ul>
<li>选择一个主题并根据github上面提供的主题使用方法进行配置，此步骤git命令往往是在Blog这个根目录下进行</li>
<li>主题目录参见<a href="https://hexo.io/themes/">Hexo主题官方集合</a></li>
<li>经过一系列尝试，最终使用butterfly主题</li>
<li>个性化设置参见<a href="https://butterfly.js.org/">butterfly官方文档集</a>，这是创作者的博客，博文全是详细的设置如何调整</li>
</ul>
<h2 id="5-博文上传与修改"><a href="#5-博文上传与修改" class="headerlink" title="5.博文上传与修改"></a>5.博文上传与修改</h2><ul>
<li>需要注意的是，Hexo框架才是个人博客的架构，博客发表、修改等应该使用Hexo命令来执行</li>
<li>butterfly只是一个美化的框架，负责显示效果的渲染和调整</li>
<li>使用hexo命令创建新博文后，在source文件夹下可以找到。博文需要以md格式写作，本人使用VScode编写</li>
<li>md语法写作规范：<a href="https://markdown.com.cn/basic-syntax/">Markdown官方教程</a></li>
</ul>
<h2 id="6-过程中踩过的坑"><a href="#6-过程中踩过的坑" class="headerlink" title="6.过程中踩过的坑"></a>6.过程中踩过的坑</h2><h3 id="仓库中README-md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来"><a href="#仓库中README-md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来" class="headerlink" title="仓库中README.md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来"></a>仓库中README.md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来</h3><p>这个问题是Hexo的设置问题，不知道官方文档有没有提到，但是解决方法是：在Hexo目录下的source根目录下添加一个README.md。修改Hexo目录下的_config.yml。将skip_render参数的值设置上。skip_render: README.md保存退出即可。使用hexo d 命令就不会在渲染 README.md 这个文件了。  </p>
<p>无法在仓库中手动添加README.md,因为部署时候会将仓库内容完全顶替为本地内容，只有在本地文件中也设置README才行。</p>
<h3 id="网站在本地测试渲染失败"><a href="#网站在本地测试渲染失败" class="headerlink" title="网站在本地测试渲染失败"></a>网站在本地测试渲染失败</h3><p>可能是缺少相应的包，重新安装一遍即可，如果不确定，就从头都试一下<br>通常而言，应该安装的包有：Hexo所需的包+所用主题渲染所需的包  </p>
<p>引出一个问题：包的作用是什么？</p>
<ol>
<li>主题包：hexo-theme-fluid 是加入主题</li>
<li>渲染包：hexo-renderer-scss 是加入渲染引擎，因为Hexo里面可能并不支持主题内的一些样式</li>
</ol>
<p>又引出一个问题：为什么不支持？Hexo究竟是什么？<br>本人尚未了解，但是可以参见<a href="https://hexo.io/zh-cn/">Hexo官方文档</a></p>
<h3 id="网站本地测试成功，部署到GitHub后远程渲染失败？"><a href="#网站本地测试成功，部署到GitHub后远程渲染失败？" class="headerlink" title="网站本地测试成功，部署到GitHub后远程渲染失败？"></a>网站本地测试成功，部署到GitHub后远程渲染失败？</h3><p>按F12，查看console，发现报错是Failed to load resource: the server responded with a status of 404 ()<br>原因在于国内与GitHub的连接犯抽……..一些文件没下载回来因而无法加载</p>
<p>因为这个错误换了好多主题….本来其他主题也蛮漂亮的唉… </p>
<p>但是新页面还有问题，可能在我不知道的地方起了作用，尽管显示没有问题：<br>Error with Permissions-Policy header: Origin trial controlled feature not enabled: ‘interest-cohort’.</p>
<p>其实在途中还遇到一个错误，稀里糊涂就没有了。</p>
<h2 id="7-后续考虑加入的功能"><a href="#7-后续考虑加入的功能" class="headerlink" title="7.后续考虑加入的功能"></a>7.后续考虑加入的功能</h2><h3 id="博客定时自动更新-https-hexo-io-zh-cn-docs-one-command-deployment-html"><a href="#博客定时自动更新-https-hexo-io-zh-cn-docs-one-command-deployment-html" class="headerlink" title="[博客定时自动更新] (https://hexo.io/zh-cn/docs/one-command-deployment.html)"></a>[博客定时自动更新] (<a href="https://hexo.io/zh-cn/docs/one-command-deployment.html">https://hexo.io/zh-cn/docs/one-command-deployment.html</a>)</h3><p>很简单，没搞是因为没必要，学习是需要时间的，不需要频繁更新…….<br><img src="Heroku.png" alt="Heroku" title="配置示例"></p>
<h3 id="动态页面"><a href="#动态页面" class="headerlink" title="动态页面"></a>动态页面</h3><h3 id="真正理解Hexo的底层原理"><a href="#真正理解Hexo的底层原理" class="headerlink" title="真正理解Hexo的底层原理"></a>真正理解Hexo的底层原理</h3><h2 id="8-一些感想"><a href="#8-一些感想" class="headerlink" title="8.一些感想"></a>8.一些感想</h2><ul>
<li>在配置过程中遇到过很多报错、和不了解的地方，直接搜索报错结果固然可以找到解决方法，但是很多可以查阅官方文档来解决。查阅固然快，但往往会使得”知其然而不知其所以然”。直接搜答案的过程和询问chatGPT很类似，工程性的问题只答复解决步骤，而不分析背后原理</li>
<li>Hexo也许是一套程序，自动提供一套框架，功能是将我们提交的文本，按照模板或者自己设定的样子渲染出来。同样功能的还要jekell，但是好像是Linux上比较好用。Hexo是静态网页框架，对于博客够用，但是动态网页框架才是未来的选择。</li>
<li>在编写md的时候还误用了中文符号导致md语法出错…….wssb</li>
<li>部署后发现页面没更新？刷新试试！再不然就开梯子刷新试试！</li>
<li>原来写正文的时候不用自动换行….否则会突然换行，很难看..</li>
<li>Markdown语法：加粗这类修饰语法，前后如果有标点符号，则要视情况加空格……</li>
<li>每次更新，clean、g、d三条命令必须走一趟啊….我还以为用更新用g、d就行了呢</li>
<li>bolg内插入图片失败…解决方法是<a href="https://zhuanlan.zhihu.com/p/542101567">这篇博客</a></li>
</ul>
]]></content>
      <categories>
        <category>markdown</category>
        <category>js</category>
        <category>css</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>YukinoのBlog</title>
    <url>/2023/04/20/bolgname/</url>
    <content><![CDATA[<h1 id="这是搭建自己第一个博客后的第一篇文章"><a href="#这是搭建自己第一个博客后的第一篇文章" class="headerlink" title="这是搭建自己第一个博客后的第一篇文章"></a>这是搭建自己第一个博客后的第一篇文章</h1>]]></content>
  </entry>
  <entry>
    <title>云服务器使用及QQ机器人搭建</title>
    <url>/2023/06/15/PcrQQbot/</url>
    <content><![CDATA[<h2 id="1-服务器选择"><a href="#1-服务器选择" class="headerlink" title="1.	服务器选择"></a>1.	服务器选择</h2><p>三个主流的：阿里云、腾讯云、华为云。萌新的我需要三选一<br>华为云没用过。阿里云学生认证可以白嫖，腾讯云学生认证是打折，看着蛮便宜，但是这个时间买是1.9折，去年11月是1.1折。</p>
<p>服务器使用阿里云，因为可以白嫖7个月，新手上路还是找免费的试一下比较好，随便造，造坏了也不心疼。</p>
<p>cpu、内存这些东西就是看钱的啦，根据需要购买合适的就行。<br>系统这个东西，不太清楚是怎么选择的。本人选择Ubuntu的依据是：尽管windows很熟悉，但是几个资深程序员的哥们儿用的都是Linux内核系统；此外，Liunx相对于windows的内存占用要小太多了，服务器还是要抠抠搜搜一点。<br>Linux提供服务好像也蛮方便的</p>
<h2 id="2-服务器操作"><a href="#2-服务器操作" class="headerlink" title="2.	服务器操作"></a>2.	服务器操作</h2><p>跟着阿里的教程走下来，基本上就会怎么操纵服务器了。远程连接服务器有两种方式：Workbench和VNC，前者似乎只能用命令行输入，后者则可以安装GUI。</p>
<p>实际上，不同云服务商对于自己云服务器的名称和操纵方式是有所不同的。</p>
<h2 id="3-图形界面的安装"><a href="#3-图形界面的安装" class="headerlink" title="3.	图形界面的安装"></a>3.	图形界面的安装</h2><p>我的一个朋友对我说，只有当你能够用命令行完成你想要的所有操作后，你才算是真正懂计算机，我觉得很有道理。目前还没有这个能力，先用带GUI的VNC试一下，后续需要理解Workbench的命令行是什么东西，如何操作。其实目前在没有GUI的情况下无法理解系统内部结构。</p>
<p>GUI安装流程的方法，阿里云本身有<a href="https://help.aliyun.com/document_detail/59330.html">提供</a>，最终选择的系统是Ubuntun20.04。<br>一开始选择的18.04的造了半天python安装还是出问题。因为机器人需求3.8的python，18.04默认是3.6，升级过程中会出现很多问题。</p>
<h2 id="4-PCR会战机器人的选择"><a href="#4-PCR会战机器人的选择" class="headerlink" title="4.	PCR会战机器人的选择"></a>4.	PCR会战机器人的选择</h2><p>之前主流是yobot，但是会战改版后用不了了，开发者不更新了。<br><a href="https://github.com/Ice9Coffee/HoshinoBot">HoshinoBot</a>是我一开始配置的，结果用不了会战功能，但是其他花里胡哨的似乎挺全。配置方法按照这个Liunx的安装流程走就是了。</p>
<p>实际上好像在哪里都无所谓？但是部署在这个位置的时候，运行.bat的时候，命令行显示的是：</p>
<p>顶级难蚌：18.04在numpy上安装疯狂失败，20.04一路绿灯😅<br><img src="success.png" alt="success" title="成功"></p>
<p>我这个服务器的带宽低，不用清华镜像的时候很慢。应该加上镜像设置的命令😇</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`python3.8 -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`</span><br></pre></td></tr></table></figure>


<p><a href="https://github.com/eggggi/yobot_remix">yobot_remix</a>是yobot针对新版公会战的魔改，实测公会战能用。由于是基于yobot改的，<a href="https://yobot.pcrbot.com/features/">命令和部署方法</a>大体也和yobot相同</p>
<h2 id="5-机器人没响应"><a href="#5-机器人没响应" class="headerlink" title="5.	机器人没响应"></a>5.	机器人没响应</h2><p>典中典，被风控了，可以自行搜索解决方法。<br><a href="https://github.com/Mrs4s/go-cqhttp/issues/958">这个</a>里面的高赞回答很有用</p>
<hr>
<h2 id="后记阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋"><a href="#后记阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋" class="headerlink" title="后记	阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋"></a>后记	阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋</h2>]]></content>
      <categories>
        <category>python</category>
        <category>Liunx</category>
        <category>Ubuntu20.04</category>
      </categories>
      <tags>
        <tag>QQbot</tag>
        <tag>阿里云</tag>
        <tag>cloud_server</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次实习前的一些准备工作与知识储备</title>
    <url>/2024/03/14/%E5%AE%9E%E4%B9%A0%E5%87%86%E5%A4%87/</url>
    <content><![CDATA[<h1 id="千帆杯原生应用挑战赛"><a href="#千帆杯原生应用挑战赛" class="headerlink" title="千帆杯原生应用挑战赛"></a>千帆杯原生应用挑战赛</h1><ol>
<li><strong>大赛主旨</strong>：大赛以“创意无限·生成未来”为主题，紧密围绕当前AI技术的前沿动态和应用趋势，借助百度智能云千帆AppBuilder和ModelBuilder两大智能开发助手，鼓励参赛者打造出更多具有创新性、实用性和社会价值的AI原生应用  <ul>
<li><strong>第一期</strong>：游乐场排队规划助手：赛题聚焦春节假期游乐园排队效率问题，鼓励开发者利用 AI 能力施展“时间魔法”，打造一款具有实用性的“游乐场排队规划助手”，帮助游客更好地了解乐园的排队情况，设计个性化的游玩路线，在有限的时间内获得最“High”的体验，同时为管理者提供优化运营策略的决策支持。  <blockquote>
<ul>
<li>此大赛没有规定数据集，需求成果是使用主办方框架的应用程序。参赛者需要自己获取相关数据，如大赛第一名使用的是香港迪士尼数据</li>
</ul>
</blockquote>
</li>
<li><strong>第二期</strong>：生成一个可制作贺岁文案内容的精调模型（限定使用ERNIE Speed，通过对模型精调使其保持原有能力的同时，具备准确理解并执行文案创作中创作长度相关指令的能力）。<blockquote>
<ul>
<li>此大赛提供了少量数据集（56）条，同时要求对数据集进行扩展（最终至少需要100条数据），数据为json形式</li>
<li>与第一期不同，此期是方向特化的微调模型开发，需要使用主办方框架</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><strong>大赛形式</strong>：<ul>
<li><strong>第一期</strong>：基于AppBuilder平台提供的强大开发套件和资源环境，使用平台预置的Agent框架，以零代码的方式创建Agent智能体，自动化调用各种工具打造游乐场排队规划助手AI原生应用。<blockquote>
<ul>
<li>游客只需要输入时间预算和游玩喜好，Agent智能体就能生成并执行Python代码，求解优化问题，智能规划出游玩项目路线。<br>————————生成python，求解optimization问题的agent</li>
</ul>
</blockquote>
</li>
<li><strong>第二期</strong>：通过在千帆大模型平台使用平台上的各种模型调优工具，结合相关数据，基于ERNIE-Speed调优生成符合赛题主题要求且效果优秀的模型。  <blockquote>
<ul>
<li><strong>作品信息需包含</strong>：  <blockquote>
<ul>
<li>微调后的模型效果展示（输入输出示例截图）  </li>
<li>部署后的模型API文档（包含url地址、超参配置和步骤描述）  </li>
<li>access_token；<br></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
<h1 id="阿里天池竞赛"><a href="#阿里天池竞赛" class="headerlink" title="阿里天池竞赛"></a>阿里天池竞赛</h1><h2 id="1-基于LLM智能问答系统学习赛"><a href="#1-基于LLM智能问答系统学习赛" class="headerlink" title="1. 基于LLM智能问答系统学习赛"></a><strong>1. 基于LLM智能问答系统学习赛</strong></h2><blockquote>
<ul>
<li><strong>赛题思想：未来金融科技领域将深刻体现Agent的价值，即一个智能代理能根据用户需求进行意图识别和决策。本次大赛的赛题虽为单一，但融合了数据查询与文本理解两大任务，充分体现了Agent核心思想：根据不确定输入，判断用户意图，并调用相应服务或功能生成答案。</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>模型使用：不限制选手的模型使用，选手可以选择商业化模型或者开源模型，也可以结合多个模型，共同创建一个问答系统。可以采用Prompt Engineering方法，也可以使用外部数据对模型进行微调。推荐使用“通义千问金融大模型”或“通义千问7B模型”作为基础大模型，</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>任务目标：数据查询题——根据用户的问题，生成相应的SQL查询语句，精准查询问题结果；文本理解题：对长文本进行细致检索与解读，高效提取关键信息。</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong><a href="https://www.modelscope.cn/datasets/BJQW14B/bs_challenge_financial_14b_dataset/summary">数据集描述</a>：赛事主办方提供三类数据。一个是10张数据表（sqlite），一个是招股说明书（pdf文件，标准的招股说明书形式，没有进行数据处理），以及将招股说明书pdf解析后的txt文件。</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>大模型拓展操作：扩展金融行业词表；增量训练行业金融200B规模，涵盖中英文财报、研报、新闻、书籍、论坛等多种类型数据；</strong><br><strong><font color="#FF0000">训练上下文扩展到16K，借助NTK和LogN等技术，推理长度可以扩展到64K</font></strong></li>
</ul>
</blockquote>
<h2 id="2-通义千问AI挑战赛-Code-Qwen能力算法赛道"><a href="#2-通义千问AI挑战赛-Code-Qwen能力算法赛道" class="headerlink" title="2. 通义千问AI挑战赛 - Code Qwen能力算法赛道"></a><strong>2. 通义千问AI挑战赛 - Code Qwen能力算法赛道</strong></h2><blockquote>
<ul>
<li><strong>目的：如何通过高质量的数据微调提升基础语言模型的代码能力？——聚焦于通义千问大模型微调训练的竞赛，其主要目标是通过高质量的数据探索和拓展开源模型 Qwen 1.8B 及 Qwen 72B 的代码能力上限。高质量的数据是大模型提升效果的关键，初赛阶段主要聚焦在如何通过 SFT 提升基础模型的代码能力。需要选手基于最新开源的 Qwen 1.8 模型作为基础模型，在提供的训练框架上自行进行数据收集与微调，训练完成后将进行自动评估，返回最终结果进行排名；</strong></li>
<li><strong>数据集特征</strong><blockquote>
<ul>
<li><strong><a href="https://huggingface.co/datasets/codefuse-ai/Evol-instruction-66k">Evol-instruction-66k</a>：	基于开源open-evol-instruction-80k过滤低质量，重复和human eval相似的数据后得到的高质量代码类微调数据</strong><blockquote>
<ul>
<li><strong>e.x.</strong>{<br>  <strong>“instruction”</strong>: “Using the text file from the link, <a href="https://norvig.com/big.txt">https://norvig.com/big.txt</a>, use the SpellChecker module from the pyspellchecker package in Python to find spelling mistakes in the text file. Additionally, measure the efficiency of the implemented spell checker. Report the number of false positives and false negatives, and provide a way to visualize these instances. Also, improve the spell checker by using custom dictionaries or other natural language processing techniques to reduce these errors.”,<br>  <strong>“output”</strong>: “Here is an outline of how to address this problem:\n(Note we will need to install <code>pyspellchecker</code> and <code>requests</code> dependencies if not present already). You can install it using:\n<code>\npip install pyspellchecker requests\n</code>\n\n<code>python\nimport requests\nfrom spellchecker import SpellChecker\n\n# Get the text file\nurl = \&quot;https://norvig.com/big.txt\&quot;\nresponse = requests.get(url)\ntext = response.text\n\n# Remove punctuation and special characters\ntext = &#39;&#39;.join(e for e in text if e.isalnum() or e.isspace())\n\n# Tokenize text (naive method)\nwords = text.split()\n\n# Initialize spell checker\nspell = SpellChecker()\n\n# Find misspelled words\nmisspelled = spell.unknown(words)\n\n# Print result\nprint(f\&quot;Number of misspelled words: &#123;len(misspelled)&#125;\&quot;)\n</code>\nThis snippet will output a number of misspelled words found in the text.\n\nIn order to measure the efficiency in terms of false negatives and false positives, you need to know the true positives and true negatives, which requires the ground truth that in this case would be some kind of ‘clean’ version of the text without typos. \n\nWe can perform a counter-example with a small list that has good and bad spelled word:\n<code>python\n# Assuming we have two lists which contain correctly spelled and incorrectly spelled words.\ncorrectly_spelled = [&#39;apple&#39;, &#39;banana&#39;, &#39;pear&#39;]\nincorrectly_spelled = [&#39;aple&#39;, &#39;banan&#39;, &#39;per&#39;]\n\n# Everything the spellchecker says is fine should be in the correctly spelled list.\nfalse_negatives = [word for word in correctly_spelled + incorrectly_spelled if word not in spell.known([word])]\n\n# Everything the spellchecker says is wrong should be in the incorrectly spelled list.\nfalse_positives = [word for word in correctly_spelled + incorrectly_spelled if word not in spell.unknown([word])]\n\nprint(f\&quot;False Negatives: &#123;false_negatives&#125;\&quot;)\nprint(f\&quot;False Positives: &#123;false_positives&#125;\&quot;)\n</code>\n\nVisualizing results can be done in many ways depending on your preferences and exact needs. In simple cases, you could list the incorrectly spelled words found by the spell checker, but you could also create a plot using matplotlib, or a word cloud using the WordCloud module.\n\nTo improve the spell checker, you can add words to its dictionary. The pyspellchecker has a corpus we can add our words, as shown below:\n\n<code>python\nupdated_dictionary = set(list(spell.word_frequency.dictionary.keys()) + [\&quot;additional\&quot;, \&quot;dictionary\&quot;, \&quot;words\&quot;])\nspell.word_frequency.load_words(updated_dictionary)\n</code>\n\nIf you have a custom dictionary, you can just load this directly.\n\nPyspellchecker probabilities are based on the occurrence of a word in the text. As a result, the more domain-specific text you can provide it, the more accurate it becomes.\n\nAnother optimization could be to apply a stemming algorithm before comparing words to the dictionary since the dictionary typically includes only root words.\n\nIn addition, you can use Machine Learning or Natural Language Processing techniques to improve spell check. Techniques such as:\n1. Using a context-based spelling correction by using BERT model. \n2. Training a customized spell-check model using a dataset relevant to your problem.\n3. Using N-gram approach, where the context of the word is used for the correction (this is used in Google\u2019s spell checker as of 2010).\n4. Use of phonetic algorithms (e.g., Soundex, Metaphone, etc.) which are used to correct phonetically misspelled words.”<br>}</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li><strong><a href="https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k">CodeExercise-Python-27k</a>：	基于chatgpt生成的高质量python练习题数据</strong><blockquote>
<ul>
<li><strong>e.x.</strong>{<strong>“data_name”</strong>: “CodeExercise-Py”, <strong>“id”</strong>: 0, <strong>“chat_rounds”</strong>: [{<strong>“role”</strong>: “human”, <strong>“content”</strong>: “Write a Python function named &quot;add_numbers&quot; that takes two parameters, &quot;num1&quot; and &quot;num2&quot;, and returns the sum of the two numbers.\nInput: None”, “chat_round_id”: 0}, {<strong>“role”</strong>: “bot”, <strong>“content”</strong>: “<code>python\ndef add_numbers(num1, num2):\n    \&quot;\&quot;\&quot;\n    This function takes two numbers as input and returns their sum.\n\n    Parameters:\n    num1 (int or float): The first number.\n    num2 (int or float): The second number.\n\n    Returns:\n    int or float: The sum of num1 and num2.\n    \&quot;\&quot;\&quot;\n    return num1 + num2\n\n# Example usage\nresult = add_numbers(5, 3)\nprint(result)  # Output: 8\n\n\n</code>“, “chat_round_id”: 1}]}</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</blockquote>
<br>

<h1 id="闻达：一个大规模语言模型调用平台"><a href="#闻达：一个大规模语言模型调用平台" class="headerlink" title="闻达：一个大规模语言模型调用平台"></a><a href="https://github.com/wenda-LLM/wenda">闻达：一个大规模语言模型调用平台</a></h1><h2 id="1-知识库检索增强"><a href="#1-知识库检索增强" class="headerlink" title="1. 知识库检索增强"></a><strong>1.</strong> <strong>知识库检索增强</strong></h2><blockquote>
<ul>
<li><strong>sentence_transformers + 检索，支持faiss&#x2F;fess检索，可以选择本地&#x2F;在线检索、索引预先&#x2F;实时构建</strong></li>
<li><strong>没有提到长文本能力，仅仅是取决于model本身的功能</strong></li>
</ul>
</blockquote>
<h1 id="DeepBI"><a href="#DeepBI" class="headerlink" title="DeepBI"></a><a href="https://github.com/DeepInsight-AI/DeepBI/tree/main">DeepBI</a></h1><h2 id="1-定义-x2F-底模"><a href="#1-定义-x2F-底模" class="headerlink" title="1. 定义&#x2F;底模"></a><strong>1.</strong> <strong>定义&#x2F;底模</strong></h2><blockquote>
<ul>
<li><strong>基于OPENAI GPT4 模型开发的BI系统，可以自动生成sql与图表</strong></li>
<li><strong>核心问题是：如何在通过API的情况下优化SQL生成与结果分析？图表生成是模型生成还是调用某些库方法？</strong><blockquote>
<ul>
<li><strong>初步判断是用了python的第三方包，如pyecharts等进行图表生成；完全调用GPT4 API，似乎仅在prompt上做了一些简单的处理？</strong></li>
<li><strong>还是midware的思想；在代码生成上，没有做到交叉生成，python和sql生成是分开的</strong></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h1 id="CodeT5"><a href="#CodeT5" class="headerlink" title="CodeT5+"></a>CodeT5+</h1><ol>
<li><h2 id="模型结构：encoder-decoder结构"><a href="#模型结构：encoder-decoder结构" class="headerlink" title="模型结构：encoder-decoder结构"></a><strong>模型结构：encoder-decoder结构</strong></h2><ul>
<li><strong>编码器学习从代码&#x2F;文本序列（完整、部分或跨度屏蔽序列）中对上下文表示进行编码。</strong></li>
<li><strong>根据预训练学习任务，解码器被训练以生成不同类型的输出。</strong></li>
<li><strong>预训练任务的混合使模型能够学习代码上下文的有意义的表示，并在不同级别恢复丢失的信息：代码跨度、部分程序和完整程序。</strong></li>
</ul>
</li>
<li><h2 id="训练策略：多任务混合"><a href="#训练策略：多任务混合" class="headerlink" title="训练策略：多任务混合"></a><strong>训练策略：多任务混合</strong></h2><ul>
<li><strong>目的是增强LLM的理解能力</strong></li>
<li><strong>结合了不同类型的学习任务，包括span denoising, causal language modeling (CLM), text-code contrastive learning, and matching tasks（跨度去噪、因果语言建模（CLM）、文本代码对比学习和匹配任务）。他们发现，如此广泛的预训练任务集可以帮助模型从代码和文本数据中学习丰富的表示，并弥合各种应用中的预训练微调差距。</strong></li>
<li><strong>如何实现通过对模型进行调整加速训练？—————shallow encoder and deep decoder策略</strong>  <blockquote>
<ul>
<li><strong>keep the small encoder and the cross-attention layers trainable while freezing the deep decoder LLM. ————Such architecture is designed with the intuition that the decoder is often employed to deal with a higher level of complexity in generation tasks and requires a larger number of neural parameters.</strong></li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><h2 id="训练trick"><a href="#训练trick" class="headerlink" title="训练trick"></a><strong>训练trick</strong></h2><ul>
<li><strong>unimodal code data and bimodal code-text data.（纯代码 + 代码与注释？）</strong></li>
<li><strong>二阶段预训练策略（实践证明可以增强更丰富的文本表示）</strong><blockquote>
<ul>
<li><strong>第一阶段只采用纯代码训练：从GitHub等开源平台获得了数据（许可证）。总共采用了九种编程语言的多语言培训数据。</strong></li>
<li><strong>第二阶段采用代码+文本作为训练数据：每个数据样本都包含一个文本代码对，其中包括一个代码函数及其相应的描述函数语义的文档字符串。</strong></li>
</ul>
</blockquote>
</li>
<li><strong>Instruction Tuning</strong><blockquote>
<ul>
<li><strong>tuning the models with synthetic instruction-following tasks</strong></li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
 <br>
  <br>

<h1 id="下列为可能参考到的论文及启发点"><a href="#下列为可能参考到的论文及启发点" class="headerlink" title="下列为可能参考到的论文及启发点"></a>下列为可能参考到的论文及启发点</h1><h1 id="————LLM-agent-for-generating-code-with-python-and-SQL"><a href="#————LLM-agent-for-generating-code-with-python-and-SQL" class="headerlink" title="————LLM agent for generating code with python and SQL"></a>————LLM agent for generating code with python and SQL</h1><h2 id="1-CleanAgent-Automating-Data-Standardization-with-LLM-based-Agents"><a href="#1-CleanAgent-Automating-Data-Standardization-with-LLM-based-Agents" class="headerlink" title="1. CleanAgent: Automating Data Standardization with LLM-based Agents"></a>1. CleanAgent: Automating Data Standardization with LLM-based Agents</h2><blockquote>
<ul>
<li><strong>提出一个具有声明性、统一 API 的 Python 库，用于标准化列类型，通过简洁的 API 调用简化 LLM 的代码生成。</strong></li>
<li><strong>类似与midware的思想，加了一个中间层方便tuning，实际价值似乎不大</strong></li>
</ul>
</blockquote>
<h2 id="2-LiveCodeBench-Holistic-and-Contamination-Free-Evaluation-of-Large-Language-Models-for-Code"><a href="#2-LiveCodeBench-Holistic-and-Contamination-Free-Evaluation-of-Large-Language-Models-for-Code" class="headerlink" title="2. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"></a>2. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</h2><blockquote>
<ul>
<li><strong>一套code generation效果评价方法，MIT伯克利大学出品，或许可以一试</strong></li>
</ul>
</blockquote>
<h2 id="3-KnowCoder-Coding-Structured-Knowledge-into-LLMs-for-Universal-Information-Extraction"><a href="#3-KnowCoder-Coding-Structured-Knowledge-into-LLMs-for-Universal-Information-Extraction" class="headerlink" title="3. KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction"></a>3. KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction</h2><blockquote>
<ul>
<li><strong>中国科学院大学CAS Key Laboratory of Network Data Science and Technology出品</strong></li>
<li><strong>旨在开发一种LLM易于理解的统一模式表示，以及鼓励LLM遵循模式并准确提取结构化知识的有效学习框架（Universal Information Extraction (UIE) task）</strong></li>
<li><strong>已有UIE方法的一些不足：在类别及其归属的判定上、关联与实体的存在条件判断上不足；classification label&#x2F;自然语言 对LLM来说需要更多资源去理解；缺乏通用性</strong></li>
<li><strong>此文章实际上是用python代码来表示知识和信息，是information to python再to knowledge的过程</strong></li>
</ul>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>DailyRecord-March</title>
    <url>/2024/03/20/DailyRecord-March/</url>
    <content><![CDATA[<h1 id="3-20"><a href="#3-20" class="headerlink" title="3.20"></a>3.20</h1><p><strong>入职的第一天，接到的任务是：把闻达的demo在服务器上用docker给搭建出来。于是我在服务器上先创建了一个miniconda3的docker，然后使用conda安装了一些依赖、git项目等；其间遇到一个问题：docker+conda后，尽管bash上显示是root，但是好像是一个虚拟的root，没有sudo等的执行文件，需要重新安装一下。此外，公司的网有点差，清华源1Mb、不用清华源只有几十kb。因而进展较慢</strong><br><strong>部署步骤进行到了模型下载这一步，但是因为网不好所以连不上hugging face，需要搭梯子。下班了，明天再搞！</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：搞定梯子，下载好生成模型和embedding模型，最终完成demo，并调整远程访问展示</strong></li>
</ul>
</blockquote>
<h1 id="3-21"><a href="#3-21" class="headerlink" title="3.21"></a>3.21</h1><p><strong>今天签合同了。</strong><br><strong>搭梯子问题卡了好久😡。这国企是一天也不能待了，网差的要死。最后还是前辈哥帮忙解决的。</strong><br><strong>但是terminal的命令，下载的那些怎么都那么几把慢啊！网速是压力之源！</strong><br><strong>更悲剧的是：今日闻达demo部署至最后一步无法运行…………..</strong><br><strong>可能是docker的问题，因而将其删除从头开始搭建。如果明天仍然不行，则尝试不用docker直接在服务器上部署</strong>  </p>
<h2 id="他妈的！狗日的北京通勤太痛苦了！"><a href="#他妈的！狗日的北京通勤太痛苦了！" class="headerlink" title=" 他妈的！狗日的北京通勤太痛苦了！ "></a><font color="red"> <strong>他妈的！狗日的北京通勤太痛苦了！</strong> </font></h2><blockquote>
<ul>
<li><strong>明日任务：wenda的demo。哎我真佛了这个b服务器 + docker好难用，不能直接在服务器安装conda吗，conda创建虚拟环境不也挺方便的</strong></li>
</ul>
</blockquote>
<h1 id="3-22"><a href="#3-22" class="headerlink" title="3.22"></a>3.22</h1><p><strong>创建了一个git的container，但是在安装lfs的时候出了问题，需要用sudo等一些命令，但是apt下载很慢，似乎需要在docker file文件中就更换镜像源</strong><br><strong>了解了docker中与宿主机相互cp文件、docker在创建时需要用GPU对容器可见的命令；</strong></p>
<blockquote>
<ul>
<li>docker run -it –net&#x3D;host –gpus all –name 容器名 -e NVIDIA_DRIVER_CAPABILITIES&#x3D;compute,utility -e NVIDIA_VISIBLE_DEVICES&#x3D;all 镜像名</li>
</ul>
</blockquote>
<p><strong>闻达demo终于无报错安装起来了</strong><br><strong>但是运行的时候，模型似乎卡住&#x2F;死循环？不返回计算结果。</strong>  </p>
<h2 id="他妈的！压力一天比一天大！EMO了要！"><a href="#他妈的！压力一天比一天大！EMO了要！" class="headerlink" title=" 他妈的！压力一天比一天大！EMO了要！ "></a><font color="red"> <strong>他妈的！压力一天比一天大！EMO了要！</strong> </font></h2><h2 id="今天美好的事情："><a href="#今天美好的事情：" class="headerlink" title="今天美好的事情："></a><strong>今天美好的事情：</strong></h2><p><strong>与SL、ZYR一起在附近那啥招待所吃川菜。味道不错，就是环境有点热、菜有点辣。此外，和ZYR一起自行车骑行，他一路讲解，聊的很开心！到北京以来为数不多的快乐时间。</strong><br><strong>SL骑着他那破电瓶车，本来跟我们一起的，半路给交警罚款了，就让他回去了，唉，我好难过</strong></p>
<blockquote>
<ul>
<li><strong>下周任务：调整闻达demo</strong></li>
</ul>
</blockquote>
<h1 id="3-23-3-24-周末！"><a href="#3-23-3-24-周末！" class="headerlink" title="3.23-3.24  周末！"></a>3.23-3.24  周末！</h1><p><strong>早上乘坐地铁到清河，和NJ汇合，一起去清河</strong><br><strong>做了半小时火车到怀来辣！比北京通勤还快。下了火车有一种终于逃离压力之源的舒畅感。</strong><br><strong>中文在饭店吃了什么洋葱和羊肉炒的东西，味道不错；然后还吃了莜面、炒扁豆角；喝了营养快线味的酸奶、杏仁露。杏仁露味道不错，加热和不加热是两个味道</strong><br><strong>吃完饭和NJ一起顺路去菜市场买了点粑粑柑（最后一口没吃带回北京了）。然后不行一阵子去了宾馆，双人间一晚90，条件还挺不错….要哭了</strong><br><strong>睡了午觉，然后出去找网吧打了几把大乱斗；下了新赛季的第一把云顶，玩不明白好痛苦，老六老七出局。然后出去转了转，看了一下县城的风土人情，买了西安特产甑糕，味道也不错。天气不好，阴沉天空使我不太开心。</strong><br><strong>相对于老家，这里明显更荒凉一些，没有人、没有年轻人。</strong><br><strong>晚上吃了砂锅。酸菜白肉和竹笋炖腊肉的砂锅；点了傀儡（土豆和面的混合物作为主食？），咸咸的，有葱花。点了冰糖芦荟，和椰果罐头一个味儿。吃完又转了转。回到宾馆玩了把金铲铲，然后下去买了10斤的半个西瓜和勺子。吃了一点，大头给NJ吃了（真能吃啊这B）</strong><br><strong>睡了一晚，第二天早上吃了豆面还是什么东西，然后走路去买了个吊炉烧饼分着尝尝；NJ点了驴肉和驴肠火烧的外卖，带回北京吃了</strong><br><strong>和NJ回北京了，BYD带我在海淀区骑了好久的自行车，最后在一家新疆馆子吃了抓饭和羊肉什么馍，喝了北京的汽水（和芬达一个味），吃的挺好。就是这b人带我转来转去最后吃这种快餐属实没绷住</strong></p>
<h1 id="3-25"><a href="#3-25" class="headerlink" title="3.25"></a>3.25</h1><p><strong>闻达demo部署完毕辣！</strong><br><strong>接下来需要：尝试更换通义7B&#x2F;14B的模型；查看闻达前端技术栈，与前端做交接让其修改；部署另一个&#x2F;几个项目（似乎要加班，tmd！）</strong><br><strong>已经将底模更新为7B，但是14B可能需要双卡，还不了解他这个框架怎么放到双卡上；</strong><br><strong>前端框架不太清楚，但是有一个二次开发的前端框架正在尝试，但是服务器网不好，nodejs安装困难。</strong><br><strong>下午看了B站上的一个从零开始大模型，感觉有点收获。还有一部分没看完</strong> </p>
<blockquote>
<ul>
<li><strong>明日任务：或许需要看一下另一个agent代理的部署；看langchain和LLM的相关教程视频；RAG目前似乎有一套实践模板，基于faiss检索？</strong></li>
</ul>
</blockquote>
<h1 id="3-26"><a href="#3-26" class="headerlink" title="3.26"></a>3.26</h1><p><strong>早上看langchain的视频教程</strong><br><strong>下午部署了wenda的二次开发webui，其中有报错；能保证基本的对话，但是文档对话功能用不了</strong><br><strong>下午抽空去面了腾讯的NLP应用研究实习生，他们是做腾讯视频剧本理解与智能助手的，感觉挺有意思。花了114定了钟点房，可惜面试的时候网不好、代码题（手写交叉熵、双指针&#x2F;滑动数组解决list中满足和为n的最小连续长度）也没敲出来，BERT的损失函数、交叉熵也没回答好。</strong>  </p>
<ul>
<li><p><strong>和SH交流了一下面试的经历，他目前的实习产品经理似乎工资低但是很轻松。他说TX面试结果很快会出，就可以再投了，但是我到第二天也没看到结果</strong><br><strong>部署了langchain-chatchat，可以基本运行，但存在一些问题，如知识库检索似乎没有工作、模型更换没有前端，需要在后端尝试修改</strong>  </p>
</li>
<li><p><strong>回家，地铁上和ZJX吐槽工作的事情，交流中得到片刻的安慰；到家后，和NJ电话聊了聊，大诉苦水，得到了一些开导和建议，好兄弟！；夜里躺在床上，和LF聊了聊他的离职和走全奖去美国读PHD躺平5年。唏嘘之余，有点羡慕</strong>  </p>
</li>
<li><p><strong>似乎，上班的人状况都不是很好：ZYR精神衰弱吃药、LF压力大怼领导辞职、SX天天emo不想动，我自己的精神状态也不是很好，唉。唉！</strong></p>
<blockquote>
<ul>
<li><strong>明日任务：调整部署项目，学习</strong></li>
</ul>
</blockquote>
</li>
</ul>
<h1 id="3-27"><a href="#3-27" class="headerlink" title="3.27"></a>3.27</h1><p><strong>早上给领导演示了一下两个demo，边上的前辈哥部署的DB-GPT也看了；这个效果挺好，但是也有要调整的地方。如服务器上部署模型的API调用等</strong><br><strong>今天看阿里Qwen的vllm教程视频，大有收获</strong><br><strong>铸币吧，LLM聊天一个字一个字显示的效果原来是清屏+ 打印啊！前端或许就是&lt;div&gt;内部的反复刷新？</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：调整部署项目，学习</strong></li>
</ul>
</blockquote>
<p>晚上，和妈妈聊了聊天，倾诉了一下。得知妹妹的奶奶肿瘤晚期，突然有一种恐惧和悲伤笼罩心头。帮妈妈网上填写了检查需要的材料，相关材料也进行了保存。与SH、LMD一起打了大乱斗，片刻的开心，但是过程中没有完全放松。</p>
<h1 id="3-28"><a href="#3-28" class="headerlink" title="3.28"></a>3.28</h1><p><strong>继续学习qwen+vllm的宝藏up主。对asyncio的内容有些不理解。这个up主好像是c++、java高手，学习大模型相关内容上手也很快，我感到很挫败。</strong><br><strong>小米面试，问Qquant还是啥的量化不了解，adapter不了解，代码题一是n个全排列中k个逆序对数量，完全不会；一个是写梯度下降求算术平方根，初始化值、lr和反向传播这些都写的不太好</strong><br><strong>下午，视频看的差不多了，准备看一下Dify是个什么东西</strong><br><strong>晚上，打SH、NJ打游戏，和妈妈视频，开森</strong><br><strong>和沈晓宇老师的师姐聊了聊城市、职业规划、未来发展，感觉认知更清晰了</strong></p>
<blockquote>
<ul>
<li><strong>明日任务：学习</strong></li>
</ul>
</blockquote>
<p>悲苦萦绕心头，不能散去。可能真的是房子住的还是有点远，每天通勤1.5小时。或许我需要换租了。</p>
<h1 id="3-28-1"><a href="#3-28-1" class="headerlink" title="3.28"></a>3.28</h1><p><strong>上午看了一会儿视频，然后捣鼓4090那台机子的docker和conda，没捣鼓出东西来。</strong><br><strong>下午问了前辈，了解怎么搞了，稍微搞了一点</strong><br><strong>下午，搞了会langchain-chatchat，发现推理慢的原因可能是gpu被占了；没找到如何更换模型为qwen</strong>  </p>
<blockquote>
<ul>
<li><strong>下周任务：qwenllm&#x2F;qwen的docker image本地下载好并上传到远程服务器上，运行查看效果</strong></li>
</ul>
</blockquote>
<h2 id="又-放-假-辣-！"><a href="#又-放-假-辣-！" class="headerlink" title=" 又 放 假 辣 ！ "></a><font color="red"> <strong>又 放 假 辣 ！</strong> </font></h2><h1 id="3-30-3-31-周末！"><a href="#3-30-3-31-周末！" class="headerlink" title="3.30-3.31  周末！"></a>3.30-3.31  周末！</h1><p><strong>周六早上，和SH打了LOL，中午出去吃了吉野家；下午躺了一会儿，然后晚上和NJ去逛了古玩市场，感觉有点同质化。</strong><br><strong>之后吃了白糖汁儿的杏仁豆腐，然后吃了永和大王的快餐，赶紧赶地铁回来参加TX的笔试题。但实际上我一面已经挂了，结果现在才出来，这题就没必要做了实际上，而且五题只做出来第一题，似乎打竞赛的才能做得出来多题，唉…</strong><br><strong>又和SH打了一会儿大乱斗</strong><br><strong>看小说，睡觉！</strong>  </p>
<hr>
]]></content>
  </entry>
  <entry>
    <title>DailyRecord-April</title>
    <url>/2024/04/01/DailyRecord-April/</url>
    <content><![CDATA[<h1 id="4-1"><a href="#4-1" class="headerlink" title="4.1"></a>4.1</h1><p><strong>上午，把qwenllm&#x2F;qwen的docker image放到服务器上了，但是下午发现模型没放，还要下载72b-chat的模型再放上去；而且这个东西好像要自己写应该服务端py文件？</strong><br><strong>下午复习了以下long-context的论文，看了retrieval的一些如longllama，温故知新，没跳出已有框架的同时，感觉理解更深了</strong><br><strong>晚上，和SH打了一把游戏，然后和沈老师开周会，汇报了一下自己目前的一些理解。得到下一步的研究内容是：把retrieval 的方式在大模型上都实现一下</strong><br><strong>之后，继续和SH、NJ一起打了大乱斗</strong>  </p>
<h2 id="虽然是愚人节，但是无事发生"><a href="#虽然是愚人节，但是无事发生" class="headerlink" title=" 虽然是愚人节，但是无事发生 "></a><font color="red"> <strong>虽然是愚人节，但是无事发生</strong> </font></h2><blockquote>
<ul>
<li><strong>明日任务：阅读论文；学习修改大模型的方法</strong></li>
</ul>
</blockquote>
<h1 id="4-2"><a href="#4-2" class="headerlink" title="4.2"></a>4.2</h1><h2 id="草！这一天干啥了我给忘了！原来日记漏了一天，4-8才发现！"><a href="#草！这一天干啥了我给忘了！原来日记漏了一天，4-8才发现！" class="headerlink" title=" 草！这一天干啥了我给忘了！原来日记漏了一天，4.8才发现！ "></a><font color="red"> <strong>草！这一天干啥了我给忘了！原来日记漏了一天，4.8才发现！</strong> </font></h2><p><strong>好像还是在看代码和教程？</strong></p>
<h1 id="4-3"><a href="#4-3" class="headerlink" title="4.3"></a>4.3</h1><p><strong>上午，阅读qwen的model_qwen.py文件，尝试理解模型结构，寻找修改方法。最终目的是将kv retrieval加入到模型中去；目前没什么头绪，是直接改model文件，还是写个新的继承一下？继承的话如何与已有文件保持联系和交互？</strong><br><strong>qwen的py代码没有啥注释，突然想到可以看一下transformer包中的代码，拿llama做参考，希望有注释可以好看一点</strong><br><strong>吃了好大的饼</strong><br><strong>将4090机器上的qwen-72b-chat放到A800上，尝试让它跑起来；昨天前辈哥用vllm加速一些模型，基本上没跑起来报错，不知道是什么情况；没查到解决方案</strong></p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del> 明日个P！清明节假期！</strong><br><strong><del>好吧，这个饼还是挺不错的，也许会假期学习一下。不会的好多</del></strong></li>
</ul>
</blockquote>
<h1 id="4-4-4-6"><a href="#4-4-4-6" class="headerlink" title="4.4-4.6"></a>4.4-4.6</h1><p><strong>假期第一天：上午打游戏，中午吃了昨晚下班时买的罗森便当；晚上去NJ那里，吃了川菜馆子“椒榆”（好像是这个名字），一个炒鸡、一个黄焖茄子、一个蒜泥白肉、一个小酥肉（没怎么吃，打包带回去当第二天的早饭了）；之后在附近转了转，然后坐地铁去上地一个超市逛了逛，吃了又一家杏仁豆腐，不如德和斋；打包了红豆双皮奶和一个什么奶回去第二天吃；去地铁站打道回府，路上看见北体的一个小破门，<del>另外还有漂亮MM</del>。之后到家，和SH打了会儿游戏。</strong><br><strong>假期第二天：打游戏的一天奥，无事发生！和SH、NJ打了好久的游戏；中午吃的猪脚饭，还不错；晚上吃的炸蛋螺丝混，也挺不错，熙螺湾这牌子在仙林NJU的对面也开了一家，开了不久；所以点的时候还是比较放心的。</strong><br><strong>假期第三天：中午和NJ一起去安贞门；在一家小巷子里的老馆子里面吃地地地地地地地道儿的百京菜！八大碗中的牛杂，好吃！牛肠煮的最好吃；一个炒牛尾，还行；一个羊杂砂锅，不错，但是没牛杂好吃；一个麻豆腐，挺好吃，很新奇；吃完饭，去买了芬达，合起来和大洋是有点差别，感觉大洋更好喝一点；然后沿河的公园里面有什么花展，走马观花的逛了逛。</strong><br><strong><del>忘记给NJ带德和斋杏仁豆腐了，明明前天才夸下海口…</del></strong><br><strong>三天里，没怎么学习，网络小说倒是没少看；代码什么的搞了搞但好像是在做无用功…..</strong>  </p>
<h1 id="4-7"><a href="#4-7" class="headerlink" title="4.7"></a>4.7</h1><p><strong>不想上班啊啊啊啊啊啊！</strong><br><strong>上午折腾随身wifi，顺便把qwen-72b-chat完全放到了A800的服务器上，跑了一下服务端，正常；后续工作可能就是，尝试修改模型之类的，再把后端接口什么的搞一搞？</strong><br><strong>下午，搜索阅读了一些论文，感觉收获不大；觉得是自己的代码能力不足的问题，找了篇开源代码的去读，GitHub 1k star， 但是没读懂，感觉代码写的好像不太好；遂重新阅读transformer中llama的源码；因为之前发现qwen的结构和llama很相似，希望读懂llama后能很快的触类旁通。遇到了一些问题需要记录</strong>  </p>
<blockquote>
<ul>
<li>RoPe编码的实现，看的半懂不懂，没有深究，后续视情况看是否需要深入。</li>
<li>llama attention模块中，有这两个参数，尚未搞懂作用是什么：</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cache_position: <span class="type">Optional</span>[torch.LongTensor]</span><br><span class="line">position_ids: <span class="type">Optional</span>[torch.LongTensor] = <span class="literal">None</span>  </span><br></pre></td></tr></table></figure>

<p><strong>晚上在DNF神迹上抽黑钻售货机，因为没有自动抽取的功能，所以尝试用pyautogui写了个自动抽奖的脚本，但是实际运行的时候发现移动位置是对了，但无法点击到游戏程序内部的东西，又发现自己使用鼠标一直点击倒也可以，于是又开了鼠标连点器，发现鼠标连点器也点不到游戏程序内部的东西；这两种相似的现象带来的启发是：这种基于GUI模拟点击的程序，或许没有聚焦到应用程序内部，亦或者是DNF.exe的特殊性之类的。想来之前应该有一个获取游戏窗口的方法，但是不想搞了，就这样吧。</strong></p>
<blockquote>
<ul>
<li><strong>明日任务：继续阅读llama的源码</strong></li>
</ul>
</blockquote>
<h1 id="4-8"><a href="#4-8" class="headerlink" title="4.8"></a>4.8</h1><p><strong>今天，勉强把qwen的model文件看完，仍然是半懂不懂的状况，尤其是generate()和chat()这两个函数。logit的值映射到id再映射到文字，思路很好懂但代码有点复杂</strong><br><strong>后续重新看了GitHub上qwen的项目，以及modelscope上qwen7b模型的具体内容和文件。本来以为modelscope上面的是运行文件之类的，原来它真的只是模型，这个模型安装一定方式组织，transformer还是huggingface依照generation_config.json和config.json来完整的读取模型、运行之类的。运算、生成之类的函数确实是写在这个模型里面的，github上面的是介绍、示例的demo、各个数据集上的测试文件。</strong><br><strong>也就是说，如果要在模型中加入cache的retrieval，就要在modeling_qwen.py这个文件中直接修改？（但是我更希望的是，写个继承的文件然后互不影响？？）</strong><br><strong>得看看论文和论文代码去</strong><br><strong>晚上，和沈教授开了组会，表明目前的问题是不知道怎么改代码一头雾水，直接在模型文件上改感觉不稳妥；他建议我直接改试试</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：阅读recurrent-memory-transformer，开始修改qwen代码文件</strong></li>
</ul>
</blockquote>
<h1 id="4-9"><a href="#4-9" class="headerlink" title="4.9"></a>4.9</h1><p><strong>工程师大哥入职噜！希望能带我飞！</strong><br><strong>阅读recurrent-memory-transformer的源码，思考其结构和代码作用</strong><br><strong>modeling_rmt文件，大致上应该是模型文件，其中定义了两个类，MemoryCell——似乎是负责向原有model中加入MemoryCell的类；RecurrentWrapper——似乎是可以将原有model的输出再处理之后的输出，这样看来，好像可以直接将其运用到qwen中去？</strong><br><strong>下午和GG聊了一阵子，到出租屋后又和他聊了大概一个小时；和NJ打了几把大乱斗</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：继续阅读recurrent-memory-transformer，开始修改qwen代码文件</strong></li>
</ul>
</blockquote>
<h1 id="4-10"><a href="#4-10" class="headerlink" title="4.10"></a>4.10</h1><p>今日，新入职了一个本地学校的做数据的实习生<br><strong>上午，稍微思考了一下RMT的代码结构和用法；然后尝试在PC上跑一下千问的小模型查看效果</strong><br><strong>采用Int4版本，结果报错：</strong>  </p>
<blockquote>
<ul>
<li>CUDA extension not installed.</li>
</ul>
</blockquote>
<p>尝试安装cudatoolkit，结果依然没有解决<br>破案了，byd一直安装的cpu版本的torch之类的，可能是版本没对齐&#x2F;清华源的问题<br>来自HXD的支援：  </p>
<blockquote>
<ul>
<li>建议用pip<br>别用conda<br>conda不会检查环境里的冲突直接装<br>pip会先看有没有装</li>
</ul>
</blockquote>
<p><strong>他妈的破案了，python和torch版本太高了</strong>    </p>
<blockquote>
<ul>
<li><strong>明日任务：改千问暂停，协助跑通RAGFLOW</strong></li>
</ul>
</blockquote>
<h1 id="4-11"><a href="#4-11" class="headerlink" title="4.11"></a>4.11</h1><p>RAGFlow昨天下午工程师跑了，它最方便的功能还是直接调用api，但工程师尝试搞明白阿里的api怎么申请怎么用却失败了；从示例图上开RAgflow可以支持本地模型，但是在我们跑通的结果来看那个图上面的选项消失了，今天工程师问ragflow的群主得知，已经用ollama直接取代了。<br>似乎ollam调用更方便？准备部署ollama试试，但是由于4090显存不够了，因而要将其部署到A800上；镜像的上传又是折磨<br>踩了一个坑：  </p>
<blockquote>
<ul>
<li>scp命令指定远程端口，-p其实要大写成-P才是正确命令</li>
</ul>
</blockquote>
<p>导入之后，显示名称什么的为none，需要使用docker tag命令自己命名<br>继续琢磨怎么加mem， 想搜RMT的解析文章，结果搜出来发现说它就是Transformer-XL，然后去搜transoformer-XL，发现它是2019年的文章被拒稿了？后续改进的XLNet，这个已经被加入到了transformer官方包里面<br>查看了一些代码，感觉加不进去；但似乎渐渐得到了一些理解：这些网络结构什么的已经定好了，包括qwen，这些放在src&#x2F;model下的modeling文件定义了模型，而huggingface或者什么地方可以找到模型的一些与训练好的参数。那么已知的是qwen确实没法改模型结构？最多只能在推理的时候采取一些不影响它模型本身流动的trick，如lora&#x2F;adpter之类的东西。<br>qwen1.5采用了滑动窗口attn，可以提高推理效率；能否提高长文本能力尚不明晰。<br>似乎可行的两个思路：  </p>
<blockquote>
<ul>
<li>使用adapter，直接将之前的kv cache揉到attn里面？  </li>
<li>直接在模型推进的某一层，对kv cache和外部向量数据库 进行向量检索，然后揉进去？</li>
</ul>
</blockquote>
<p><strong>研究方向暂时转变</strong>：研究ReAct + CoT的应用；唉人在江湖身不由己，但好在是这回有人指导了，希望这段时间代码能力能有突破；越来越觉得LLM尤其需要强工程能力  </p>
<p>用千问的api，跑RAGFlow成功了，后续工作是更改成本地模型；此外，还有一些功能待添加：聊天界面可以输入图片&#x2F;文件，支持连接sql并发挥BI功能  </p>
<p>尝试用langchain，将React和CoT结合起来  </p>
<blockquote>
<ul>
<li><strong>明日任务：langchain的ReAct和CoT；了解一下CoT</strong></li>
</ul>
</blockquote>
<h1 id="4-12"><a href="#4-12" class="headerlink" title="4.12"></a>4.12</h1><p>今天没什么印象深刻的任务，在读一篇新的论文BPO，阅读它的代码。由于工程能力不足，阅读代码总是吃力半懂不懂；或许，LLM学习困难的原因在于实验条件高，我平时真的很难上手去改一些东西并快速查看效果。  </p>
<p>晚上，与ZYR和SL一起吃了聚宝源（西直门店）的涮肉；说实话感觉一般，那个B麻酱根本不香！感觉不如一般的火锅涮肉，亦或者之前在西安吃过的冰煮羊  </p>
<p>吃完饭，一起去紫什么公园逛了大概一小时；<br>今天与朋友们聊了一些求职、读博方面的事情，聊了一些高中同校、学弟学妹们的一些发展；再次感叹人外有人，每次聊天总是能感受到自己的不足，希望我能赶上去！实习实现工程能力的巨大提高！！！！  </p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del> 假期噜！明天和LT，NJ一起聚一下，吃筋头巴脑！</strong>  </li>
<li><strong>下周任务： 读懂BPO代码，复现</strong></li>
</ul>
</blockquote>
<h1 id="4-13-4-14"><a href="#4-13-4-14" class="headerlink" title="4.13-4.14"></a>4.13-4.14</h1><p>周六中午，和LT，NJ一起在西二旗吃了湘菜（<del>筋头巴脑无了</del>），280的套餐，大众点评好评送手撕包菜；有臭鳜鱼、擂椒皮蛋、小炒肉、辣炒茶干、肘子肉（应该不是这个，但我想不起来具体名字了）和鸡蛋烧的一个菜；喝了可乐；<br>吃撑了，LT饭量没衰减，而我已经吃不动了，唉。<br>吃完饭，随便逛了逛；路上飞虫进眼，忙忙糟糟虚惊一场；简单粗暴地买了瓶矿泉水冲了一下眼睛。<br>和LT一起去天坛公园逛逛。路上闻到一些味道，结果发现是沙比LT的衣服馊了；想到自己当年也穿过馊的衣服而不自知，还是亲娘发现的，不免有些好笑。<br>天坛公园要购票，15一张。和LT随便逛了逛，由于他要赶火车因而走马观花，只看了很小的一部分；聊了聊过去、现在、未来。似乎朋友们没怎么变，似乎又有点变化。但朋友身边总归还是安心一点。<br>实际上，和朋友们聊天、自己写博客记录，是我迷茫的一种体现，我或许希望在交流中明心见性、获得奋斗下去的动力。  </p>
<p>下午回去，打发时间；到了10.40，SH上号叫我打LOL；周日纯躺家一天，下午1点多和SH打了两小时大乱斗。NJ撺掇我玩dota2，未果；我暂时不想花费脑子在游戏上了，新游戏的学习在上学的时候可能是快乐，但现在我没有动力。  </p>
<h1 id="4-15"><a href="#4-15" class="headerlink" title="4.15"></a>4.15</h1><p>上午又入职了一位员工。开了个会，妈的不知道现在要干啥，云里雾里的。  </p>
<p>上午看了一些东西CoT的一些东西，结果下午又告诉我回去搞长文本。这是否有点……<br>下午，被告知搞一个长文本工程思路的PPT，于是梳理了一下想法，工程思路分为三类，每个类对应目前的一个具体例子。突然发现，之前的LongMem好像可以借鉴，于是重新去看它的代码。<br>这一看不得了，之前的认识还是太浅薄了，现在感觉他的代码思路都很明晰，然后虽然用的fairseq这个我没接触过的package，但是只是为了memory的方便，因而使用它的incremental decoder类。<br>从它的eval.py、memory以及fuse等文件中，隐隐感觉到可以用在qwen上，但我需要更详细的去理解代码。这个项目中一个memory bank模块，似乎可以很方便的迁移。<br>我必须立刻发动同调！<br>沈教授今晚有事，预定的周会推迟一天噜。<br>晚上，和NJ和SH大乱斗；和SH输麻了。</p>
<blockquote>
<ul>
<li><strong>明日任务：呱！我要狠狠地使用longmem呀！</strong></li>
</ul>
</blockquote>
<h1 id="4-16"><a href="#4-16" class="headerlink" title="4.16"></a>4.16</h1><p>上午继续阅读longmem的源码；它虽说进行了解耦，但从网络结构来看，是自定义了一个层数为底模一半的网络，然后输入、训练、生成memory bank的结果，然后一直存着不动，同时底模向前运行推进，最后再加一个融合层？也许是我理解不到位，代码看错了；总之是有点搞不懂的。感觉代码好像挺不错，但转化到qwen中有难度，因为他主要用的fairseq，而我还没学过，感觉这是一个深度学习的库，自定义模型什么的。<br>下午，尝试跑一下qwen的memory增强；首先跑了基础的底模，啥都没改，让它从西游记的一段话中抽取一个信息，应该是一个大海捞针的任务，没回答出来，那我的思路就是调整模型文件来查看效果。<br>其实，下午也看了一阵子评测数据集的东西，但是也是粗略的看看没大搞懂，可能主要因为看的评测数据集与我预想中的有所区别，C-EVAL什么的是通用，但是长文本&#x2F;超长文本可能还要那种名著数据集什么的，我记得之前看过有，但是一时间忘记去找了；倒也无所谓，因为这个qwen-1.8b-chat模型本身就没有长文本能力，ntk和logn缩放都无，只有7b和14b好像支持这个；因而我采用了上面的一个测试数据。<br>修改模型还是没什么头绪，longmem究竟如何加进去？  </p>
<p>晚上，开了周例会；得到指点，预计看三篇论文、使用faiss检索并将检索结果加入prompt中作为self-RAG的baseline，需要跑通这个实验  </p>
<blockquote>
<ul>
<li><strong>明日任务：学习faiss，将输入query先切片、再faiss、再构建prompt，再输入到模型中查看效果</strong></li>
</ul>
</blockquote>
<h1 id="4-17"><a href="#4-17" class="headerlink" title="4.17"></a>4.17</h1><p>今日，尝试将query 按句子切片；结果使用nltk、langchain等切片都未果，不知什么情况；于是使用简单的split暂作切分；结果发现是这个标点符号好像有点特殊，应该是另一种编码还是什么东西？但是NLTK应该确实是不能进行中文分词的。<br>尝试了langchain的文本chunk方法，也未果，到底是怎么回事？<br>终究，实现了将query进行faiss，然后加入到prompt中去，返回的结果不是很好，可能是由于模型本身的能力不足，也有可能是prompt模板需要优化。<br>他妈的！公司的网还是没接上！我都没办法测试真正的模型的效果、没法保证自己调整的可用性。</p>
<p>晚上回家，打大乱斗；输麻了，这b游戏能不能死一死。目前寻找其他可以和朋友一起玩的游戏中……但这个更换游戏的限制其实在于朋友之间共识的达成。  </p>
<blockquote>
<ul>
<li><strong>明日任务：阅读周会上的论文，尝试优化prompt，构建模板</strong></li>
</ul>
</blockquote>
<h1 id="4-18"><a href="#4-18" class="headerlink" title="4.18"></a>4.18</h1><p>上午，稍微调整了一下prompt，之前的prompt格式有点不正确；然后测试了西游记第一章其中一节的效果，似乎有提升，但是感觉不明显，可能是因为模型不太能处理这种半古文的原因。<br>于是，切换了内部的测试数据，运行查看效果，确实提升了模型的性能。  </p>
<blockquote>
<ul>
<li><strong>思考：</strong><br><strong>首先说一下目前存在的不足</strong>：目前，对于query的faiss处理，没有实现问题与前置信息的分离，仅仅是将整个query进行cut&#x2F;chunk，然后使用faiss检索、返回topk的结果，利用其构建一个增强的query；prompt注入方法的缺点已经很明显了，这里要说的主要是未来可能的faiss&#x2F;其他向量库检索结果错误&#x2F;不匹配的问题，正如之前提到的，没有实现问题和背景的分离；此外，就算实现了分离，问题和背景知识的向量也不一定能够匹配上去。<br>目前的实践，仅仅是一个简单的test，还有许多细节需要调整：history与历史faiss向量库的保留筛选机制；faiss向量在内存中的废弃与删除；未来的应用场景应该是去理解长文本乃至超长文本，或许还要涉及生成，因而超长文本下token、input、cache等的限制也需要考虑，当然这是后面的课题。<br><strong>其次，说一下确实实现结果优化的可能原因</strong>：正如之前提到的，这种实践的方法论与理论基础比较不踏实，但为什么实现了结果优化？可能的原因在于，最后的问题，由于关键词还是语义之类的，在向量化之后确实与背景信息的某一部分高度相关，从而在query的cut与向量化之后增强了相关的向量（sentence_transformer的原理似乎在于，计算句子向量与全文向量的相关程度，从而返回topk），使得大多数情况下相关背景信息能够被 <strong>“大海捞针”</strong>，这部分topk信息，加入到prompt中去，再输入到LLM中，在LLM内部又会因为重点信息的二次&#x2F;多次出现实现attention的增强，从而增强了返回结果。</li>
</ul>
</blockquote>
<p>下午阅读TransoformerFAM的论文，但是尚未理解透彻，不太能构想出来这种结构应该怎么在代码里实现。论文中说，可以不影响之前的权重参数，也就是说应该定义一个class来作为cache？<br>内网接入许可批下来了，可以使用服务器上的模型了。</p>
<blockquote>
<ul>
<li><strong>明日任务：阅读TransformerFAM，测试服务器上的模型的prompt优化效果</strong></li>
</ul>
</blockquote>
<h1 id="4-19"><a href="#4-19" class="headerlink" title="4.19"></a>4.19</h1><p>尝试将内外网访问走的通道分离，不然每次访问内网都要断wifi就很傻逼。<br>下列命令需要cmd以管理员身份运行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">route -p add 172.17.39.189 mask 255.255.255.0 173.17.39.254   </span><br></pre></td></tr></table></figure>
<p>三个地址分别是内网ip、内网掩码、内网网关。<del>（幸好网络接入需要mac验证，不然我这命令还不好放上来）</del>。<br>网上搜的教程还额外所有路由全走了一遍内网，纯沙比做法来的，只要这一个命令其实就可以了。  </p>
<p>突然发现自己鞋子穿的不是同一双，好笑之余，又有点感慨。或许我仍然没有长大，仍然毛毛燥燥地去面对这社会的一切，或许我就像穿上大人西装的蜡笔小新，在假装成熟。上次发生类似的事情，似乎是袜子还是什么东西穿的不是同一双，已经记不太清了。  </p>
<p>下午，整了一下午curl访问那个b服务器模型，都失败了；官方文档的命令试过了一点用也没有；但是，使用langchain的命令却成功了？？？？？基本上两小时浪费了，我真是无语子。但好消息是，这个prompt优化的效果，在7b上也很明显。<br>下午还看了一会儿TransformerFAM，似乎有一点实现的思路；不知周末是否会开始写一下 <del>（基本不可能）</del><br>今天发了一部分工资，上个月20号到本月10号的。</p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del>  哇袄！高一下至高二末尾的112寝室！三分之二的四人再聚首！吃一口地地地地道儿的百京八大碗！没毛病奥老铁们！</strong>  </li>
<li><strong>下周任务：后续复现捏~</strong></li>
</ul>
</blockquote>
<h1 id="4-20-4-21"><a href="#4-20-4-21" class="headerlink" title="4.20-4.21"></a>4.20-4.21</h1><p>周六，和SL、NJ一起吃之前和NJ在安贞门吃过的那个小馆子，地道儿！点了麻豆腐、炸蘑菇、八大碗之牛杂、豆泡（这个b豆泡和麻辣烫那个味道一样，真是血亏！）、扒肉条、羊肉烧卖、蘑菇和莴苣炒的一个素菜。吃的挺好。<br>啥比LT用他之前在群里吹的向老板请假的理由来应付我们😅，家人们真是一整个无语住了。<br>啥比SL坐地铁坐反了😅，坐到他妈的朝阳门去了，多等了半小时才吃饭，百京✌坐车主打一个随心所欲说是。<br>快下地铁的时候，NJ告诉我附近有cosplay的什么展子，因为他在地铁口看见了好几个穿的奇奇怪怪的人🤣；我出地铁口的时候，看见了一个JK的背影，是那种日式JK，因为那衣服的质感啥的一看就是cosplay的。吃完饭，NJ猜测cosplay的场地是在北投购物公园，于是我们走过去看看热闹。<br>在那个购物广场里面逛了第一次，我没看见，宝可梦道馆也被查封了，但在找宝可梦道馆的路上看见了一群coser，看背影认识一个钉琦野蔷薇；出了商场到附近的地方又稍微转了转，没什么好看的；于是原路返回，想去河边的公园邹洲、看一看花，结果这时候在购物广场里面看见了一大批coser，居然有<strong>小鸟游六花</strong>，泪目，细想已经12年了；SL说看见了约尔太太；当然还看见了二字游戏的萤妹（wc！O！）；其他零零散散的coser记不清了。<br>出了购物广场，又看见两个coser进广场，看来是真有展子；其中一个是miku（我去！初音未来！），可惜听NJ说是个坦克；我看的这些coser都没看见正脸，全是背影。在b站会员购上查了查，发现真是个展子，门票80的coser展，纯cosplay交流之类的，无同人志啥的，感觉没意思，也没有啥声优给我看（**<del>樋口円香你带我走吧😭</del>*<em>），就不去看了；去河边公园逛了逛，歇了歇，帮SL看一下他碧蓝航线的装备和配队；差不多14点的时候，各自打到回府了。<br>下午，NJ发了淘宝链接给我，是大窑打折，3</em>450ml只要8.9r，于是速速下单；又问他还有无推荐，给我推荐了7月临期的盐汽水，买了；<br>到出租屋之后，躺、看小说、刷视频、打劣质游戏。<br>晚上10点多，查公主链接EX5的作业，查到一个好像能抄，遂决心战斗；耗费2k母猪石、8E玛娜，一个多小时、SL十几次，终于通关EX5；  </p>
<p>周日，混；中午吃的烤肉饭外卖，晚上吃的螺 吸 混；和SH打大乱斗；<br><strong>他妈的！返回百京的票没抢到！全是候补！五一节回不了家什么的那种事情不要啊啊啊啊啊啊啊啊啊啊啊啊！如果没办法，只能请一两天的假延迟回百京了</strong><br>晚上8、9点的时候感觉眼睛有点不舒服，干涩，又有点困，于是早早睡了；</p>
<h1 id="4-22"><a href="#4-22" class="headerlink" title="4.22"></a>4.22</h1><p>周末的时候，华为的移动wifi到了，加上之前网管过来接网线但啥事儿都没解决反而把服务器的外网权限给断了😅（<del>百京的国企✌就是爷！</del>），因而服务器和PC都会通过这个wifi相连（终于不用我开热点了！）。<br>上午，阅读Infini-Attention的论文，稍微看了一下源码。这个示例代码是基于qwen2MoE模型的，迁移到qwen-chat上应该比较容易。但是话又说回来，MoE模型与chat模型的区别在哪里，还需要阅读一下源码。<br>可能的好消息是，Inifini-attention本体的代码还挺短的。<br><del>在阅读论文中的一个疑问就是：论文中Memory cache的更新，似乎是增量循环，同时在这个过程中memory没有Norm，这样会不会数值溢出？</del> 错误的，没疑问了，activation和norm发生在后面retrieval attention的计算与检索上面。  </p>
<p>下午，比较infini和qwen2-moe原生attention之间的不同；代码和思路应该是简单的，就是把额外的memory加到attn_output里面去；<del>但是 <font color="red"><strong>为 啥 qwen2-moe 的 attention 不 做 softmax 和 norm ？</strong></font></del> 铸币了，不是在这儿加add和norm的；<br>qwen这个attention，在计算完attention之后，又对它额外做了个nn.linear。<strong>这个操作的出处是哪儿？我查了llama的源码，发现它也是这么做的</strong>————哦tmd，<strong>又铸币了</strong>，原来这是MHA，多头concat到一起之后再用linear层融合一下。理论到实践之间的距离还需要去努力弥补！<br>看代码的时候突然想到，论文中做了cache消耗的对比，infini只用额外的1.5M，而memorizing transformer用了183M，为啥会差这么多？只在某一层加入cache有那么大的消耗吗？<br>看完了项目里面的model文件，对照qwen原文件之后，确定了要改的地方，基本上就是有M_Z这个参数的地方；其他的三个文件还没看。<br>晚上，和沈教授那边开了周例会，再次明确接下来的任务是无误的。<br>和SH大乱斗，输赢参半吧，就是遇到沙比选C但是C不起来挺无语的。<br>晚上，和ZZK聊了一会儿工作的事情，他现在在上海的滴滴实习，和我吐槽房子不好、天天加班、节假日也加班、实习工资低、孤独、精气神耗尽假期也不想动……我感同身受，还是那句话，大家对上班的感受都是一样的；<del>再次感受到人的天性就是厌恶工作的。</del>上完这个B班，回家往出租屋一躺，就感觉这辈子完辣😅。<br>其实，写博客、在博客中放飞自我（指瞎几把说、粗俗用语），是我在迷茫、孤独时的一种排解手段吧；给自己找一个能看见进度的、做起来不太麻烦、有点儿动力做的事情做，可以从中感到一点活着的意义。记录日常，一方面是看看自己是不是真的有进步？另一方面，也是记录一些难以复刻的经历与心境，避免自己遗忘一些东西，像是异地的格格不入、大城市的自卑之类的。  </p>
<blockquote>
<ul>
<li><strong>明日任务：看另外三个文件，尝试修改本地的模型</strong></li>
</ul>
</blockquote>
<h1 id="4-23"><a href="#4-23" class="headerlink" title="4.23"></a>4.23</h1><p>突然发现infini-attention的项目不是官方的，而是个人自己写的；github上还有几个实现的项目。好奇怪捏，为什么那篇TransformerFAM没有人写代码？<br>对照着项目，尝试修改qwen的model文件，结果发现，示例的实现代码是基于qwen1.5的，这个代码更加简洁，相比之下qwen的model更加杂乱&#x2F;冗余？我也不知道怎么说，因为还没有看到qwen1.5的其他model源码。<br>但是反过来说，修改qwen的model尽管可能更加困难，但相比之下应该更能提升我的工程能力、加深我对大模型流程和代码的理解，还是不能畏难放弃！  </p>
<p>下午，尝试修改qwen的model文件，对照infini attention、qwen源文件、qwen1.5源文件修改；qwen1在确定在哪儿融合memory上面有难度，按理说应该在实施了RoPE且加入到cache的q、k、v这一步操作之后，再利用这个cache进行memory融合，但这个代码有点杂乱，class里面为了实现attention又写了几个method，然而在qwen1.5里面就纯没有额外方法一步到位了，实现的很清爽，在1里面有些杂乱；<br>总之，废了不少力气定位了memory加入的位置后，却又发现了另一个问题：infini里面使用的是GQA，这个所需要的参数，在1.5里面通过config可以定义、是存在的。但是在1里面，config.json文件里面并没有这个参数的影子；也就是说，qwen1是不支持GQA的。这个参数是：</p>
<blockquote>
<ul>
<li>num_key_value_heads</li>
</ul>
</blockquote>
<p>在1.5里面，GQA中，需要计算group的数值，这个数值是这么计算出来的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.num_key_value_groups = self.num_heads // self.num_key_value_heads</span><br></pre></td></tr></table></figure>
<p>目前有两种思路，一是不要GQA，另一种是自己加上这个参数。在查看qwen1.5的configuration_qwen2.py之后，发现了这个新增参数它有一个默认初始化的值，初始化代码如下：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> num_key_value_heads <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    num_key_value_heads = num_attention_heads</span><br></pre></td></tr></table></figure>
<p>也就是说，如果num_key_value_heads没有定义，就默认等于num_attention_heads。我们可以直接qwen1的代码里面让它相等；这种方法实际上和不要GQA没什么区别，就是把infini的代码抄过来比较方便罢了。<br>原以为难关已经被攻破了，但在repeat_kv的时候又发现了问题：首先，qwen1里面是没这个玩意儿的，也就是说1也不支持需要repeat_kv的一系列操作；这是小事，只要去1.5里面把这个函数抄过来就行了；但关键的是1里面好像也没有对v进行cache，仅仅cache了q和k，有query_list和key_list，至少我初步看来是这样的；我得再仔细看看，真要没cache_v的话，改的工程量就太大了；还是直接去改1.5的好。<br>似乎，如果参数use_cache&#x3D;True的情况下，q、k、v是都会缓存的？而且直接就用query、key、value就行了？<br>另：infini的代码更新了，之前一些M_Z的改动似乎是不必要的，他将其删除了（这沙比owener代码有个mistake，我issue告诉他，结果他回复说“没<del>问</del>题~~”，然后立刻进行一个issue的关，结果我再看代码发现他偷偷把那个mistake给改了😅，真沙比）。<br>改好了代码，想要在本机上测试效果；结果cuda out of memory，难蚌；目前两个办法：一是换成0.5b-int4再试试，二是在服务器上部署跑一下；考虑到后续方便的问题，还是用服务器吧；但是conda create的时候报错了；解决ing……<br><strong>他妈的破案了！服务器的内存和硬盘都爆掉了！下班时间到了明日再战！</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：在服务器上测试修改后的模型效果</strong></li>
</ul>
</blockquote>
<p>晚上，在出租屋混时间，没有打LOL；而是又拾起了✌最爱的怀旧页游！（如果WQ看到，又要搁那儿骂我山猪吃不了细糠啦！哈哈！）这游戏时不时就想捡起来玩一阵子，然后突然感觉无味、因而果断抛弃，往往下定决心再也不碰，却又偶尔想起、心痒难耐。我承认我是一个很怀旧的人，我或许一直无法脱离过去的光影。  </p>
<h1 id="4-24"><a href="#4-24" class="headerlink" title="4.24"></a>4.24</h1><p>上午，按照计划、尝试在服务器上测试修改后的模型的效果；在部署环境的间歇之余，看了一下qwen1.5的model文件，开头跃入眼帘的就是rotray embedding这个class；其实，看过的model文件，大多都像这样将其放在文件的开头；而且，基本上都是一样的写法，似乎都是从llama或者什么原初模型中copy过来的；鉴于之前并没有太搞懂其原理、代码，只是了解了五六分，因而又打算再看看；花了一阵子时间，看的眼花缭乱，最终收获却不如想象之中那么大，还是没有明晰。<br>终于，在环境部署好之后，尝试运行cli_demo.py，<strong>运行成功了！问答也能正确输出！</strong> 虽然结果不是很好，但是考虑到模型本身、以及加入的attention还没有经过ft或者什么操作，因而已经很满意了！希望这个现象代表的是：修改的内容是成功的，后续是可以推进的。  </p>
<p><strong>接下来要考虑的，主要有两方面</strong>：</p>
<blockquote>
<ul>
<li><strong>input query过长时的segment和retrieval、或许还有chat history的相关操作</strong>；  </li>
<li>infini的深入理解，与微调可能；  </li>
<li>在上述做完之后，qwen1.5的修改与尝试应该就是水到渠成的事情了</li>
</ul>
</blockquote>
<p>今天中午，又发了一部分工资，上个月20号到本月20号的。  </p>
<p>下午，看了一下其他的论文。准备看streaming-llm的代码，但是这个代码结构我得梳理一下。<br>因为苹果吃完了，所以在楼下lawson买了两个饭团当晚饭；饭团还在打折，好耶！<br>到出租屋，依然混时间；和SH打了两三个消失的大乱斗。  </p>
<blockquote>
<ul>
<li><strong>明日任务：阅读streaming-llm源码，看LLM教程。</strong></li>
</ul>
</blockquote>
<h1 id="4-25"><a href="#4-25" class="headerlink" title="4.25"></a>4.25</h1><p>今天上午，又入职了一位姑娘，似乎是北大的，和上周入职的老哥认识；捏妈，百京真是卧虎藏龙。感觉自己被Top2包围了。<br>看streaming-llm的kv-cache部分的代码，结合论文看，还是有点难懂捏。  &#x3D;&#x3D;<br>中午，突然想起来昨天改的qwen-infini代码似乎有问题；去查看之后果然如此；这么想来，昨天的cuda out of memory应该是一个失误，而不是修改模型成功运行的信号。于是，下午继续琢磨修改；遇到了许多问题。下面列出最严重的问题：</p>
<blockquote>
<ul>
<li><strong>在infini-attention的初始化模块中register_buffer了M和z，同时对其进行kaiming初始化，但是后续forward的时候，却显示M和z是NoneType</strong>：如果不对M和z进行初始化，则回在计算logit的时候报错，因为logit值都为0；这边测试了一下初始化代码的写法，至少可以保证的是，init函数中的M和z是正确初始化的、正确登记的（？）<br>————<strong>原因</strong>：如果模型权重文件里面没有这个buffer，就算我初始化了，加载权重之后也会是nonetype；所以需要在使用之前再强制初始化一下。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>此外，还有似乎是因为精度问题导致的错误：probability tensor contains either inf, nan or element &lt; 0</strong>：可能量化的模型都容易出现这个问题，百川有一个<a href="https://github.com/baichuan-inc/Baichuan2/issues/291">issue</a>似乎可以解决；但是一个下午看的头昏脑胀，不想再加班的，明天再搞吧。</li>
</ul>
</blockquote>
<p>奥牛逼，工程师大哥鼠标灵敏度贼低，结果nm是LOL电一前二十？？？</p>
<blockquote>
<ul>
<li><strong>明日任务：解决问题</strong></li>
</ul>
</blockquote>
<h1 id="4-26"><a href="#4-26" class="headerlink" title="4.26"></a>4.26</h1><p>继续尝试修改错误，首先确定具体问题所在，根据报错结果，去修改代码，print报错之前的相关信息，其probs和softmax之后的结果打印如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#probs</span></span><br><span class="line">tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br><span class="line"></span><br><span class="line"><span class="comment">#prob_after_softmax：       </span></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<p>可以看到这个结果，可能是第一个token开始这个概率数值就出错了；正如昨日所说，应该是精度转化的过程中出现了问题。尝试Qwen下面一个issue的<a href="https://github.com/QwenLM/Qwen/issues/276#issuecomment-1825326365">回答</a>，结果运行了很长世界，返回结果如下：<br><img src="error.png" alt="error" title="返回值"><br>一开始运行的时候显卡风扇咔咔吹，我还以为陷入死循环了呢，没想到还是给我返回了结果。这个结果大致上符合预期吧，因为照着上面修改了代码之后，直接选择概率最大的作为decode的值，但是从一开始到最末尾，概率值都为0的情况下，自然所有decode出来的token都是相同的；qwen中这个都是0解出来就是“！”，应该是这样的。<br>总的来说，这个结果是好的，<strong>说明修改代码没有问题，只是精度转化的问题。代码是可以正常跑通、一直decode到最后的</strong>；后面要做的就是进行微调之类的东西了；后续的问题，等到实战的时候再说吧！</p>
]]></content>
  </entry>
</search>
