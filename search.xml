<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI绘画初步尝试：Stable Difussion本地部署及WebUI使用</title>
    <url>/2023/04/25/AIdrawing/</url>
    <content><![CDATA[<h2 id="1-安装conda、git、cuda"><a href="#1-安装conda、git、cuda" class="headerlink" title="1.	安装conda、git、cuda"></a>1.	安装conda、git、cuda</h2><p>Cuda版本需要首先查看自己nvida显卡参数，不能高于参数上的版本</p>
<h2 id="2-Conda创建环境，"><a href="#2-Conda创建环境，" class="headerlink" title="2.	Conda创建环境，"></a>2.	Conda创建环境，</h2><p>python版本根据<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">GitHub上的说明</a>来选择</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`conda create --name sdweb python=3.10.6`</span><br></pre></td></tr></table></figure>

<h2 id="3-进入创建的虚拟环境"><a href="#3-进入创建的虚拟环境" class="headerlink" title="3.	进入创建的虚拟环境"></a>3.	进入创建的虚拟环境</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`conda activate sdweb`</span><br></pre></td></tr></table></figure>

<h2 id="4-在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘…"><a href="#4-在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘…" class="headerlink" title="4.	在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘…."></a>4.	在虚拟环境下克隆项目，注意：命令行需要cd到虚拟环境的目录，不然会默认git到c盘….</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`</span><br></pre></td></tr></table></figure>

<p>实际上好像在哪里都无所谓？但是部署在这个位置的时候，运行.bat的时候，命令行显示的是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`Creating venv in directory D:\Anaconda3\envs\sdweb\stable-diffusion-webui\venv using python &quot;D:\Anaconda3\python.exe&quot;`</span><br><span class="line">`venv &quot;D:\Anaconda3\envs\sdweb\stable-diffusion-webui\venv\Scripts\Python.exe&quot;`</span><br><span class="line">`Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]`</span><br></pre></td></tr></table></figure>

<p>没有用虚拟环境下的python版本哎….但是存储位置好歹是对了 😇</p>
<p>不对！或许应该在虚拟环境下用命令行运行这个.bat而不是点击！</p>
<p>环境对了，但是运行时系统突然卡死一段时间……不敢动不敢动 😰<br>错误显示是Torch安装失败<img src="error1.png" alt="error" title="错误报告">，这个文章有提到把.bat的代码改一下，尝试之。</p>
<p>第六行改为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`set COMMANDLINE_ARGS=--lowvram --precision full --no-half --skip-torch-cuda-test`</span><br></pre></td></tr></table></figure>

<p>这个.bat文件是另一个.bat的参数传入文件。</p>
<p>又寄啦，而且途中很卡 😅</p>
<hr>
<h2 id="5-还是看看电子佛祖布施的法器吧嘉人们-😋"><a href="#5-还是看看电子佛祖布施的法器吧嘉人们-😋" class="headerlink" title="5.	还是看看电子佛祖布施的法器吧嘉人们 😋"></a>5.	还是看看电子佛祖布施的法器吧嘉人们 😋</h2><p> <strong>【AI绘画】Stable Diffusion整合包v4  <a href="https://www.bilibili.com/video/BV1iM4y1y7oA">BV1iM4y1y7oA</a></strong></p>
]]></content>
      <categories>
        <category>python</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>Stable Difussion</tag>
        <tag>本地部署</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/04/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>记录自己的初次搭建博客的经历——流程、路径与感悟</title>
    <url>/2023/04/24/BlogBuilding/</url>
    <content><![CDATA[<p>安装流程可以参见 <a href="https://zhuanlan.zhihu.com/p/158975269">这篇文章</a><br>部署时尤其需要注意的设置：将hexo设置里面的分支名称改成与GitHub<a href="https://zhuanlan.zhihu.com/p/345841098">一致</a>，因为前两年GitHub把main branch名字改了..</p>
<h2 id="1-网站部署的原理和逻辑是什么？"><a href="#1-网站部署的原理和逻辑是什么？" class="headerlink" title="1.网站部署的原理和逻辑是什么？"></a>1.网站部署的原理和逻辑是什么？</h2><p>在最开始，面对各种搭建网站的框架，本人其实是迷茫的，因为我一时间无法理解为什么经过这些步骤就能够产出一个可以被访问的页面？<br>直到我在站点快搭建完成时，我才突然明白其原理，或许接下来的要说理解有点不准确，但是仍然在这里记录一下：  </p>
<ol>
<li>首先，需要理解的就是，网络上所有的资源都是以二进制的方式传输的，网站也是这样，传输的是010101的数据流，而不是直接将页面呈现在电脑上。数据在经过网络传输至本地的时候，需要本地利用某种“解码软件”将其转换成更高级的、便于用户阅读、输入的模式。浏览器其实就是将01数据解析成html等代码，然后再通过渲染、代码执行，将数据以GUI的形式呈现出来的解析器。</li>
<li>那么，为什么可以将页面文件夹的一整个数据包托管至某个服务器，从而能够对外可见呢？在GitHub中，如果你建立一个仓库，并将网站数据上传至仓库，利用GitHub Pages功能就可以将个人博客发布在互联网上。实际上，通过给仓库开通Pages功能，在输入 <strong>“用户名.github.io</strong>“ 时，你实际访问的是GitHub页面下你仓库的地址（这么说不知道对不对？），然后GitHub在你访问的时候，会检查仓库中是否有index.html这一个文件，如果有，则告诉浏览器，让浏览器解析仓库内的数据资源，从而使得页面能够呈现出来。</li>
<li>为什么是index.html？因为其是所有网站建设者约定俗成的一个最初始的入口文件，就好像python包中的__init__.py一样。</li>
<li>在理清上述逻辑之后，那么我们就可以理解到一个事实：站点部署在哪里都是无所谓的，只要站点托管的地方能够提供这样一种”判断”  “解析”的服务即可，自己的电脑也可以作为server。只是PC经常保持不在线的状态，会影响用户访问。所以购买一个专门负责发送数据的服务器成为网站搭建的必要步骤，这种服务器专门处理访问网站的请求，效率上相对于PC高得多。</li>
</ol>
<h2 id="2-站点部署在哪？"><a href="#2-站点部署在哪？" class="headerlink" title="2.站点部署在哪？"></a>2.站点部署在哪？</h2><p>Github Pages，部署在这里的原因有两个：一是免费，二是顺带学习一下Github的用法</p>
<p>部署在这也有缺点，那就是国内访问不稳定</p>
<h2 id="3-部署步骤？"><a href="#3-部署步骤？" class="headerlink" title="3.部署步骤？"></a>3.部署步骤？</h2><ul>
<li>git工具的安装、npm安装、Node.js安装（？）<br>Node.js和npm分别是什么？————问问chatGPT吧！</li>
<li>使用git将本地电脑与GitHub账户绑定</li>
<li>创建一个Github仓库，命名格式严格要求为：用户名.github.io</li>
<li>使用git命令行，将本地电脑的某个地址与github仓库关联，这个地址即是博客的文件所在，我命名为Blog</li>
<li>在Blog下，用git bash安装Hexo</li>
<li>使用Hexo命令，创建、测试、部署</li>
</ul>
<h2 id="4-主题个性化修改"><a href="#4-主题个性化修改" class="headerlink" title="4.主题个性化修改"></a>4.主题个性化修改</h2><ul>
<li>选择一个主题并根据github上面提供的主题使用方法进行配置，此步骤git命令往往是在Blog这个根目录下进行</li>
<li>主题目录参见<a href="https://hexo.io/themes/">Hexo主题官方集合</a></li>
<li>经过一系列尝试，最终使用butterfly主题</li>
<li>个性化设置参见<a href="https://butterfly.js.org/">butterfly官方文档集</a>，这是创作者的博客，博文全是详细的设置如何调整</li>
</ul>
<h2 id="5-博文上传与修改"><a href="#5-博文上传与修改" class="headerlink" title="5.博文上传与修改"></a>5.博文上传与修改</h2><ul>
<li>需要注意的是，Hexo框架才是个人博客的架构，博客发表、修改等应该使用Hexo命令来执行</li>
<li>butterfly只是一个美化的框架，负责显示效果的渲染和调整</li>
<li>使用hexo命令创建新博文后，在source文件夹下可以找到。博文需要以md格式写作，本人使用VScode编写</li>
<li>md语法写作规范：<a href="https://markdown.com.cn/basic-syntax/">Markdown官方教程</a></li>
</ul>
<h2 id="6-过程中踩过的坑"><a href="#6-过程中踩过的坑" class="headerlink" title="6.过程中踩过的坑"></a>6.过程中踩过的坑</h2><h3 id="仓库中README-md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来"><a href="#仓库中README-md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来" class="headerlink" title="仓库中README.md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来"></a>仓库中README.md文件，在depoly后一直被顶替，仓库页面显示不出md的描述来</h3><p>这个问题是Hexo的设置问题，不知道官方文档有没有提到，但是解决方法是：在Hexo目录下的source根目录下添加一个README.md。修改Hexo目录下的_config.yml。将skip_render参数的值设置上。skip_render: README.md保存退出即可。使用hexo d 命令就不会在渲染 README.md 这个文件了。  </p>
<p>无法在仓库中手动添加README.md,因为部署时候会将仓库内容完全顶替为本地内容，只有在本地文件中也设置README才行。</p>
<h3 id="网站在本地测试渲染失败"><a href="#网站在本地测试渲染失败" class="headerlink" title="网站在本地测试渲染失败"></a>网站在本地测试渲染失败</h3><p>可能是缺少相应的包，重新安装一遍即可，如果不确定，就从头都试一下<br>通常而言，应该安装的包有：Hexo所需的包+所用主题渲染所需的包  </p>
<p>引出一个问题：包的作用是什么？</p>
<ol>
<li>主题包：hexo-theme-fluid 是加入主题</li>
<li>渲染包：hexo-renderer-scss 是加入渲染引擎，因为Hexo里面可能并不支持主题内的一些样式</li>
</ol>
<p>又引出一个问题：为什么不支持？Hexo究竟是什么？<br>本人尚未了解，但是可以参见<a href="https://hexo.io/zh-cn/">Hexo官方文档</a></p>
<h3 id="网站本地测试成功，部署到GitHub后远程渲染失败？"><a href="#网站本地测试成功，部署到GitHub后远程渲染失败？" class="headerlink" title="网站本地测试成功，部署到GitHub后远程渲染失败？"></a>网站本地测试成功，部署到GitHub后远程渲染失败？</h3><p>按F12，查看console，发现报错是Failed to load resource: the server responded with a status of 404 ()<br>原因在于国内与GitHub的连接犯抽……..一些文件没下载回来因而无法加载</p>
<p>因为这个错误换了好多主题….本来其他主题也蛮漂亮的唉… </p>
<p>但是新页面还有问题，可能在我不知道的地方起了作用，尽管显示没有问题：<br>Error with Permissions-Policy header: Origin trial controlled feature not enabled: ‘interest-cohort’.</p>
<p>其实在途中还遇到一个错误，稀里糊涂就没有了。</p>
<h2 id="7-后续考虑加入的功能"><a href="#7-后续考虑加入的功能" class="headerlink" title="7.后续考虑加入的功能"></a>7.后续考虑加入的功能</h2><h3 id="博客定时自动更新"><a href="#博客定时自动更新" class="headerlink" title="博客定时自动更新"></a><a href="https://hexo.io/zh-cn/docs/one-command-deployment.html">博客定时自动更新</a></h3><p>很简单，没搞是因为没必要，学习是需要时间的，不需要频繁更新…….<br><img src="Heroku.png" alt="Heroku" title="配置示例"></p>
<h3 id="动态页面"><a href="#动态页面" class="headerlink" title="动态页面"></a>动态页面</h3><h3 id="真正理解Hexo的底层原理"><a href="#真正理解Hexo的底层原理" class="headerlink" title="真正理解Hexo的底层原理"></a>真正理解Hexo的底层原理</h3><h2 id="8-一些感想"><a href="#8-一些感想" class="headerlink" title="8.一些感想"></a>8.一些感想</h2><ul>
<li>在配置过程中遇到过很多报错、和不了解的地方，直接搜索报错结果固然可以找到解决方法，但是很多可以查阅官方文档来解决。查阅固然快，但往往会使得”知其然而不知其所以然”。直接搜答案的过程和询问chatGPT很类似，工程性的问题只答复解决步骤，而不分析背后原理</li>
<li>Hexo也许是一套程序，自动提供一套框架，功能是将我们提交的文本，按照模板或者自己设定的样子渲染出来。同样功能的还要jekell，但是好像是Linux上比较好用。Hexo是静态网页框架，对于博客够用，但是动态网页框架才是未来的选择。</li>
<li>在编写md的时候还误用了中文符号导致md语法出错…….wssb</li>
<li>部署后发现页面没更新？刷新试试！再不然就开梯子刷新试试！</li>
<li>原来写正文的时候不用自动换行….否则会突然换行，很难看..</li>
<li>Markdown语法：加粗这类修饰语法，前后如果有标点符号，则要视情况加空格……</li>
<li>每次更新，clean、g、d三条命令必须走一趟啊….我还以为用更新用g、d就行了呢</li>
<li>bolg内插入图片失败…解决方法是<a href="https://zhuanlan.zhihu.com/p/542101567">这篇博客</a></li>
</ul>
]]></content>
      <categories>
        <category>markdown</category>
        <category>js</category>
        <category>css</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Blog</tag>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>YukinoのBlog</title>
    <url>/2023/04/20/bolgname/</url>
    <content><![CDATA[<h1 id="这是搭建自己第一个博客后的第一篇文章"><a href="#这是搭建自己第一个博客后的第一篇文章" class="headerlink" title="这是搭建自己第一个博客后的第一篇文章"></a>这是搭建自己第一个博客后的第一篇文章</h1>]]></content>
  </entry>
  <entry>
    <title>云服务器使用及QQ机器人搭建</title>
    <url>/2023/06/15/PcrQQbot/</url>
    <content><![CDATA[<h2 id="1-服务器选择"><a href="#1-服务器选择" class="headerlink" title="1.	服务器选择"></a>1.	服务器选择</h2><p>三个主流的：阿里云、腾讯云、华为云。萌新的我需要三选一<br>华为云没用过。阿里云学生认证可以白嫖，腾讯云学生认证是打折，看着蛮便宜，但是这个时间买是1.9折，去年11月是1.1折。</p>
<p>服务器使用阿里云，因为可以白嫖7个月，新手上路还是找免费的试一下比较好，随便造，造坏了也不心疼。</p>
<p>cpu、内存这些东西就是看钱的啦，根据需要购买合适的就行。<br>系统这个东西，不太清楚是怎么选择的。本人选择Ubuntu的依据是：尽管windows很熟悉，但是几个资深程序员的哥们儿用的都是Linux内核系统；此外，Liunx相对于windows的内存占用要小太多了，服务器还是要抠抠搜搜一点。<br>Linux提供服务好像也蛮方便的</p>
<h2 id="2-服务器操作"><a href="#2-服务器操作" class="headerlink" title="2.	服务器操作"></a>2.	服务器操作</h2><p>跟着阿里的教程走下来，基本上就会怎么操纵服务器了。远程连接服务器有两种方式：Workbench和VNC，前者似乎只能用命令行输入，后者则可以安装GUI。</p>
<p>实际上，不同云服务商对于自己云服务器的名称和操纵方式是有所不同的。</p>
<h2 id="3-图形界面的安装"><a href="#3-图形界面的安装" class="headerlink" title="3.	图形界面的安装"></a>3.	图形界面的安装</h2><p>我的一个朋友对我说，只有当你能够用命令行完成你想要的所有操作后，你才算是真正懂计算机，我觉得很有道理。目前还没有这个能力，先用带GUI的VNC试一下，后续需要理解Workbench的命令行是什么东西，如何操作。其实目前在没有GUI的情况下无法理解系统内部结构。</p>
<p>GUI安装流程的方法，阿里云本身有<a href="https://help.aliyun.com/document_detail/59330.html">提供</a>，最终选择的系统是Ubuntun20.04。<br>一开始选择的18.04的造了半天python安装还是出问题。因为机器人需求3.8的python，18.04默认是3.6，升级过程中会出现很多问题。</p>
<h2 id="4-PCR会战机器人的选择"><a href="#4-PCR会战机器人的选择" class="headerlink" title="4.	PCR会战机器人的选择"></a>4.	PCR会战机器人的选择</h2><p>之前主流是yobot，但是会战改版后用不了了，开发者不更新了。<br><a href="https://github.com/Ice9Coffee/HoshinoBot">HoshinoBot</a>是我一开始配置的，结果用不了会战功能，但是其他花里胡哨的似乎挺全。配置方法按照这个Liunx的安装流程走就是了。</p>
<p>实际上好像在哪里都无所谓？但是部署在这个位置的时候，运行.bat的时候，命令行显示的是：</p>
<p>顶级难蚌：18.04在numpy上安装疯狂失败，20.04一路绿灯😅<br><img src="success.png" alt="success" title="成功"></p>
<p>我这个服务器的带宽低，不用清华镜像的时候很慢。应该加上镜像设置的命令😇</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">`python3.8 -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`</span><br></pre></td></tr></table></figure>


<p><a href="https://github.com/eggggi/yobot_remix">yobot_remix</a>是yobot针对新版公会战的魔改，实测公会战能用。由于是基于yobot改的，<a href="https://yobot.pcrbot.com/features/">命令和部署方法</a>大体也和yobot相同</p>
<h2 id="5-机器人没响应"><a href="#5-机器人没响应" class="headerlink" title="5.	机器人没响应"></a>5.	机器人没响应</h2><p>典中典，被风控了，可以自行搜索解决方法。<br><a href="https://github.com/Mrs4s/go-cqhttp/issues/958">这个</a>里面的高赞回答很有用</p>
<hr>
<h2 id="后记阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋"><a href="#后记阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋" class="headerlink" title="后记	阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋"></a>后记	阿里云给我白嫖7个月…应该尽快学习Docker的用法，方便后续迁移😋</h2>]]></content>
      <categories>
        <category>python</category>
        <category>Liunx</category>
        <category>Ubuntu20.04</category>
      </categories>
      <tags>
        <tag>QQbot</tag>
        <tag>阿里云</tag>
        <tag>cloud_server</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次实习前的一些准备工作与知识储备</title>
    <url>/2024/03/14/%E5%AE%9E%E4%B9%A0%E5%87%86%E5%A4%87/</url>
    <content><![CDATA[<h1 id="千帆杯原生应用挑战赛"><a href="#千帆杯原生应用挑战赛" class="headerlink" title="千帆杯原生应用挑战赛"></a>千帆杯原生应用挑战赛</h1><ol>
<li><strong>大赛主旨</strong>：大赛以“创意无限·生成未来”为主题，紧密围绕当前AI技术的前沿动态和应用趋势，借助百度智能云千帆AppBuilder和ModelBuilder两大智能开发助手，鼓励参赛者打造出更多具有创新性、实用性和社会价值的AI原生应用  <ul>
<li><strong>第一期</strong>：游乐场排队规划助手：赛题聚焦春节假期游乐园排队效率问题，鼓励开发者利用 AI 能力施展“时间魔法”，打造一款具有实用性的“游乐场排队规划助手”，帮助游客更好地了解乐园的排队情况，设计个性化的游玩路线，在有限的时间内获得最“High”的体验，同时为管理者提供优化运营策略的决策支持。  <blockquote>
<ul>
<li>此大赛没有规定数据集，需求成果是使用主办方框架的应用程序。参赛者需要自己获取相关数据，如大赛第一名使用的是香港迪士尼数据</li>
</ul>
</blockquote>
</li>
<li><strong>第二期</strong>：生成一个可制作贺岁文案内容的精调模型（限定使用ERNIE Speed，通过对模型精调使其保持原有能力的同时，具备准确理解并执行文案创作中创作长度相关指令的能力）。<blockquote>
<ul>
<li>此大赛提供了少量数据集（56）条，同时要求对数据集进行扩展（最终至少需要100条数据），数据为json形式</li>
<li>与第一期不同，此期是方向特化的微调模型开发，需要使用主办方框架</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><strong>大赛形式</strong>：<ul>
<li><strong>第一期</strong>：基于AppBuilder平台提供的强大开发套件和资源环境，使用平台预置的Agent框架，以零代码的方式创建Agent智能体，自动化调用各种工具打造游乐场排队规划助手AI原生应用。<blockquote>
<ul>
<li>游客只需要输入时间预算和游玩喜好，Agent智能体就能生成并执行Python代码，求解优化问题，智能规划出游玩项目路线。<br>————————生成python，求解optimization问题的agent</li>
</ul>
</blockquote>
</li>
<li><strong>第二期</strong>：通过在千帆大模型平台使用平台上的各种模型调优工具，结合相关数据，基于ERNIE-Speed调优生成符合赛题主题要求且效果优秀的模型。  <blockquote>
<ul>
<li><strong>作品信息需包含</strong>：  <blockquote>
<ul>
<li>微调后的模型效果展示（输入输出示例截图）  </li>
<li>部署后的模型API文档（包含url地址、超参配置和步骤描述）  </li>
<li>access_token；<br></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
<h1 id="阿里天池竞赛"><a href="#阿里天池竞赛" class="headerlink" title="阿里天池竞赛"></a>阿里天池竞赛</h1><h2 id="1-基于LLM智能问答系统学习赛"><a href="#1-基于LLM智能问答系统学习赛" class="headerlink" title="1. 基于LLM智能问答系统学习赛"></a><strong>1. 基于LLM智能问答系统学习赛</strong></h2><blockquote>
<ul>
<li><strong>赛题思想：未来金融科技领域将深刻体现Agent的价值，即一个智能代理能根据用户需求进行意图识别和决策。本次大赛的赛题虽为单一，但融合了数据查询与文本理解两大任务，充分体现了Agent核心思想：根据不确定输入，判断用户意图，并调用相应服务或功能生成答案。</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>模型使用：不限制选手的模型使用，选手可以选择商业化模型或者开源模型，也可以结合多个模型，共同创建一个问答系统。可以采用Prompt Engineering方法，也可以使用外部数据对模型进行微调。推荐使用“通义千问金融大模型”或“通义千问7B模型”作为基础大模型，</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>任务目标：数据查询题——根据用户的问题，生成相应的SQL查询语句，精准查询问题结果；文本理解题：对长文本进行细致检索与解读，高效提取关键信息。</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong><a href="https://www.modelscope.cn/datasets/BJQW14B/bs_challenge_financial_14b_dataset/summary">数据集描述</a>：赛事主办方提供三类数据。一个是10张数据表（sqlite），一个是招股说明书（pdf文件，标准的招股说明书形式，没有进行数据处理），以及将招股说明书pdf解析后的txt文件。</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>大模型拓展操作：扩展金融行业词表；增量训练行业金融200B规模，涵盖中英文财报、研报、新闻、书籍、论坛等多种类型数据；</strong><br><strong><font color="#FF0000">训练上下文扩展到16K，借助NTK和LogN等技术，推理长度可以扩展到64K</font></strong></li>
</ul>
</blockquote>
<h2 id="2-通义千问AI挑战赛-Code-Qwen能力算法赛道"><a href="#2-通义千问AI挑战赛-Code-Qwen能力算法赛道" class="headerlink" title="2. 通义千问AI挑战赛 - Code Qwen能力算法赛道"></a><strong>2. 通义千问AI挑战赛 - Code Qwen能力算法赛道</strong></h2><blockquote>
<ul>
<li><strong>目的：如何通过高质量的数据微调提升基础语言模型的代码能力？——聚焦于通义千问大模型微调训练的竞赛，其主要目标是通过高质量的数据探索和拓展开源模型 Qwen 1.8B 及 Qwen 72B 的代码能力上限。高质量的数据是大模型提升效果的关键，初赛阶段主要聚焦在如何通过 SFT 提升基础模型的代码能力。需要选手基于最新开源的 Qwen 1.8 模型作为基础模型，在提供的训练框架上自行进行数据收集与微调，训练完成后将进行自动评估，返回最终结果进行排名；</strong></li>
<li><strong>数据集特征</strong><blockquote>
<ul>
<li><strong><a href="https://huggingface.co/datasets/codefuse-ai/Evol-instruction-66k">Evol-instruction-66k</a>：	基于开源open-evol-instruction-80k过滤低质量，重复和human eval相似的数据后得到的高质量代码类微调数据</strong><blockquote>
<ul>
<li><strong>e.x.</strong>{<br>  <strong>“instruction”</strong>: “Using the text file from the link, <a href="https://norvig.com/big.txt">https://norvig.com/big.txt</a>, use the SpellChecker module from the pyspellchecker package in Python to find spelling mistakes in the text file. Additionally, measure the efficiency of the implemented spell checker. Report the number of false positives and false negatives, and provide a way to visualize these instances. Also, improve the spell checker by using custom dictionaries or other natural language processing techniques to reduce these errors.”,<br>  <strong>“output”</strong>: “Here is an outline of how to address this problem:\n(Note we will need to install <code>pyspellchecker</code> and <code>requests</code> dependencies if not present already). You can install it using:\n<code>\npip install pyspellchecker requests\n</code>\n\n<code>python\nimport requests\nfrom spellchecker import SpellChecker\n\n# Get the text file\nurl = \&quot;https://norvig.com/big.txt\&quot;\nresponse = requests.get(url)\ntext = response.text\n\n# Remove punctuation and special characters\ntext = &#39;&#39;.join(e for e in text if e.isalnum() or e.isspace())\n\n# Tokenize text (naive method)\nwords = text.split()\n\n# Initialize spell checker\nspell = SpellChecker()\n\n# Find misspelled words\nmisspelled = spell.unknown(words)\n\n# Print result\nprint(f\&quot;Number of misspelled words: &#123;len(misspelled)&#125;\&quot;)\n</code>\nThis snippet will output a number of misspelled words found in the text.\n\nIn order to measure the efficiency in terms of false negatives and false positives, you need to know the true positives and true negatives, which requires the ground truth that in this case would be some kind of ‘clean’ version of the text without typos. \n\nWe can perform a counter-example with a small list that has good and bad spelled word:\n<code>python\n# Assuming we have two lists which contain correctly spelled and incorrectly spelled words.\ncorrectly_spelled = [&#39;apple&#39;, &#39;banana&#39;, &#39;pear&#39;]\nincorrectly_spelled = [&#39;aple&#39;, &#39;banan&#39;, &#39;per&#39;]\n\n# Everything the spellchecker says is fine should be in the correctly spelled list.\nfalse_negatives = [word for word in correctly_spelled + incorrectly_spelled if word not in spell.known([word])]\n\n# Everything the spellchecker says is wrong should be in the incorrectly spelled list.\nfalse_positives = [word for word in correctly_spelled + incorrectly_spelled if word not in spell.unknown([word])]\n\nprint(f\&quot;False Negatives: &#123;false_negatives&#125;\&quot;)\nprint(f\&quot;False Positives: &#123;false_positives&#125;\&quot;)\n</code>\n\nVisualizing results can be done in many ways depending on your preferences and exact needs. In simple cases, you could list the incorrectly spelled words found by the spell checker, but you could also create a plot using matplotlib, or a word cloud using the WordCloud module.\n\nTo improve the spell checker, you can add words to its dictionary. The pyspellchecker has a corpus we can add our words, as shown below:\n\n<code>python\nupdated_dictionary = set(list(spell.word_frequency.dictionary.keys()) + [\&quot;additional\&quot;, \&quot;dictionary\&quot;, \&quot;words\&quot;])\nspell.word_frequency.load_words(updated_dictionary)\n</code>\n\nIf you have a custom dictionary, you can just load this directly.\n\nPyspellchecker probabilities are based on the occurrence of a word in the text. As a result, the more domain-specific text you can provide it, the more accurate it becomes.\n\nAnother optimization could be to apply a stemming algorithm before comparing words to the dictionary since the dictionary typically includes only root words.\n\nIn addition, you can use Machine Learning or Natural Language Processing techniques to improve spell check. Techniques such as:\n1. Using a context-based spelling correction by using BERT model. \n2. Training a customized spell-check model using a dataset relevant to your problem.\n3. Using N-gram approach, where the context of the word is used for the correction (this is used in Google\u2019s spell checker as of 2010).\n4. Use of phonetic algorithms (e.g., Soundex, Metaphone, etc.) which are used to correct phonetically misspelled words.”<br>}</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li><strong><a href="https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k">CodeExercise-Python-27k</a>：	基于chatgpt生成的高质量python练习题数据</strong><blockquote>
<ul>
<li><strong>e.x.</strong>{<strong>“data_name”</strong>: “CodeExercise-Py”, <strong>“id”</strong>: 0, <strong>“chat_rounds”</strong>: [{<strong>“role”</strong>: “human”, <strong>“content”</strong>: “Write a Python function named &quot;add_numbers&quot; that takes two parameters, &quot;num1&quot; and &quot;num2&quot;, and returns the sum of the two numbers.\nInput: None”, “chat_round_id”: 0}, {<strong>“role”</strong>: “bot”, <strong>“content”</strong>: “<code>python\ndef add_numbers(num1, num2):\n    \&quot;\&quot;\&quot;\n    This function takes two numbers as input and returns their sum.\n\n    Parameters:\n    num1 (int or float): The first number.\n    num2 (int or float): The second number.\n\n    Returns:\n    int or float: The sum of num1 and num2.\n    \&quot;\&quot;\&quot;\n    return num1 + num2\n\n# Example usage\nresult = add_numbers(5, 3)\nprint(result)  # Output: 8\n\n\n</code>“, “chat_round_id”: 1}]}</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</blockquote>
<br>

<h1 id="闻达：一个大规模语言模型调用平台"><a href="#闻达：一个大规模语言模型调用平台" class="headerlink" title="闻达：一个大规模语言模型调用平台"></a><a href="https://github.com/wenda-LLM/wenda">闻达：一个大规模语言模型调用平台</a></h1><h2 id="1-知识库检索增强"><a href="#1-知识库检索增强" class="headerlink" title="1. 知识库检索增强"></a><strong>1.</strong> <strong>知识库检索增强</strong></h2><blockquote>
<ul>
<li><strong>sentence_transformers + 检索，支持faiss&#x2F;fess检索，可以选择本地&#x2F;在线检索、索引预先&#x2F;实时构建</strong></li>
<li><strong>没有提到长文本能力，仅仅是取决于model本身的功能</strong></li>
</ul>
</blockquote>
<h1 id="DeepBI"><a href="#DeepBI" class="headerlink" title="DeepBI"></a><a href="https://github.com/DeepInsight-AI/DeepBI/tree/main">DeepBI</a></h1><h2 id="1-定义-x2F-底模"><a href="#1-定义-x2F-底模" class="headerlink" title="1. 定义&#x2F;底模"></a><strong>1.</strong> <strong>定义&#x2F;底模</strong></h2><blockquote>
<ul>
<li><strong>基于OPENAI GPT4 模型开发的BI系统，可以自动生成sql与图表</strong></li>
<li><strong>核心问题是：如何在通过API的情况下优化SQL生成与结果分析？图表生成是模型生成还是调用某些库方法？</strong><blockquote>
<ul>
<li><strong>初步判断是用了python的第三方包，如pyecharts等进行图表生成；完全调用GPT4 API，似乎仅在prompt上做了一些简单的处理？</strong></li>
<li><strong>还是midware的思想；在代码生成上，没有做到交叉生成，python和sql生成是分开的</strong></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h1 id="CodeT5"><a href="#CodeT5" class="headerlink" title="CodeT5+"></a>CodeT5+</h1><ol>
<li><h2 id="模型结构：encoder-decoder结构"><a href="#模型结构：encoder-decoder结构" class="headerlink" title="模型结构：encoder-decoder结构"></a><strong>模型结构：encoder-decoder结构</strong></h2><ul>
<li><strong>编码器学习从代码&#x2F;文本序列（完整、部分或跨度屏蔽序列）中对上下文表示进行编码。</strong></li>
<li><strong>根据预训练学习任务，解码器被训练以生成不同类型的输出。</strong></li>
<li><strong>预训练任务的混合使模型能够学习代码上下文的有意义的表示，并在不同级别恢复丢失的信息：代码跨度、部分程序和完整程序。</strong></li>
</ul>
</li>
<li><h2 id="训练策略：多任务混合"><a href="#训练策略：多任务混合" class="headerlink" title="训练策略：多任务混合"></a><strong>训练策略：多任务混合</strong></h2><ul>
<li><strong>目的是增强LLM的理解能力</strong></li>
<li><strong>结合了不同类型的学习任务，包括span denoising, causal language modeling (CLM), text-code contrastive learning, and matching tasks（跨度去噪、因果语言建模（CLM）、文本代码对比学习和匹配任务）。他们发现，如此广泛的预训练任务集可以帮助模型从代码和文本数据中学习丰富的表示，并弥合各种应用中的预训练微调差距。</strong></li>
<li><strong>如何实现通过对模型进行调整加速训练？—————shallow encoder and deep decoder策略</strong>  <blockquote>
<ul>
<li><strong>keep the small encoder and the cross-attention layers trainable while freezing the deep decoder LLM. ————Such architecture is designed with the intuition that the decoder is often employed to deal with a higher level of complexity in generation tasks and requires a larger number of neural parameters.</strong></li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><h2 id="训练trick"><a href="#训练trick" class="headerlink" title="训练trick"></a><strong>训练trick</strong></h2><ul>
<li><strong>unimodal code data and bimodal code-text data.（纯代码 + 代码与注释？）</strong></li>
<li><strong>二阶段预训练策略（实践证明可以增强更丰富的文本表示）</strong><blockquote>
<ul>
<li><strong>第一阶段只采用纯代码训练：从GitHub等开源平台获得了数据（许可证）。总共采用了九种编程语言的多语言培训数据。</strong></li>
<li><strong>第二阶段采用代码+文本作为训练数据：每个数据样本都包含一个文本代码对，其中包括一个代码函数及其相应的描述函数语义的文档字符串。</strong></li>
</ul>
</blockquote>
</li>
<li><strong>Instruction Tuning</strong><blockquote>
<ul>
<li><strong>tuning the models with synthetic instruction-following tasks</strong></li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
 <br>
  <br>

<h1 id="下列为可能参考到的论文及启发点"><a href="#下列为可能参考到的论文及启发点" class="headerlink" title="下列为可能参考到的论文及启发点"></a>下列为可能参考到的论文及启发点</h1><h1 id="————LLM-agent-for-generating-code-with-python-and-SQL"><a href="#————LLM-agent-for-generating-code-with-python-and-SQL" class="headerlink" title="————LLM agent for generating code with python and SQL"></a>————LLM agent for generating code with python and SQL</h1><h2 id="1-CleanAgent-Automating-Data-Standardization-with-LLM-based-Agents"><a href="#1-CleanAgent-Automating-Data-Standardization-with-LLM-based-Agents" class="headerlink" title="1. CleanAgent: Automating Data Standardization with LLM-based Agents"></a>1. CleanAgent: Automating Data Standardization with LLM-based Agents</h2><blockquote>
<ul>
<li><strong>提出一个具有声明性、统一 API 的 Python 库，用于标准化列类型，通过简洁的 API 调用简化 LLM 的代码生成。</strong></li>
<li><strong>类似与midware的思想，加了一个中间层方便tuning，实际价值似乎不大</strong></li>
</ul>
</blockquote>
<h2 id="2-LiveCodeBench-Holistic-and-Contamination-Free-Evaluation-of-Large-Language-Models-for-Code"><a href="#2-LiveCodeBench-Holistic-and-Contamination-Free-Evaluation-of-Large-Language-Models-for-Code" class="headerlink" title="2. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"></a>2. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</h2><blockquote>
<ul>
<li><strong>一套code generation效果评价方法，MIT伯克利大学出品，或许可以一试</strong></li>
</ul>
</blockquote>
<h2 id="3-KnowCoder-Coding-Structured-Knowledge-into-LLMs-for-Universal-Information-Extraction"><a href="#3-KnowCoder-Coding-Structured-Knowledge-into-LLMs-for-Universal-Information-Extraction" class="headerlink" title="3. KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction"></a>3. KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction</h2><blockquote>
<ul>
<li><strong>中国科学院大学CAS Key Laboratory of Network Data Science and Technology出品</strong></li>
<li><strong>旨在开发一种LLM易于理解的统一模式表示，以及鼓励LLM遵循模式并准确提取结构化知识的有效学习框架（Universal Information Extraction (UIE) task）</strong></li>
<li><strong>已有UIE方法的一些不足：在类别及其归属的判定上、关联与实体的存在条件判断上不足；classification label&#x2F;自然语言 对LLM来说需要更多资源去理解；缺乏通用性</strong></li>
<li><strong>此文章实际上是用python代码来表示知识和信息，是information to python再to knowledge的过程</strong></li>
</ul>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>DailyRecord-March</title>
    <url>/2024/03/20/DailyRecord-March/</url>
    <content><![CDATA[<h1 id="3-20"><a href="#3-20" class="headerlink" title="3.20"></a>3.20</h1><p><strong>入职的第一天，接到的任务是：把闻达的demo在服务器上用docker给搭建出来。于是我在服务器上先创建了一个miniconda3的docker，然后使用conda安装了一些依赖、git项目等；其间遇到一个问题：docker+conda后，尽管bash上显示是root，但是好像是一个虚拟的root，没有sudo等的执行文件，需要重新安装一下。此外，公司的网有点差，清华源1Mb、不用清华源只有几十kb。因而进展较慢</strong><br><strong>部署步骤进行到了模型下载这一步，但是因为网不好所以连不上hugging face，需要搭梯子。下班了，明天再搞！</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：搞定梯子，下载好生成模型和embedding模型，最终完成demo，并调整远程访问展示</strong></li>
</ul>
</blockquote>
<h1 id="3-21"><a href="#3-21" class="headerlink" title="3.21"></a>3.21</h1><p><strong>今天签合同了。</strong><br><strong>搭梯子问题卡了好久😡。这国企是一天也不能待了，网差的要死。最后还是前辈哥帮忙解决的。</strong><br><strong>但是terminal的命令，下载的那些怎么都那么几把慢啊！网速是压力之源！</strong><br><strong>更悲剧的是：今日闻达demo部署至最后一步无法运行…………..</strong><br><strong>可能是docker的问题，因而将其删除从头开始搭建。如果明天仍然不行，则尝试不用docker直接在服务器上部署</strong>  </p>
<h2 id="他妈的！狗日的北京通勤太痛苦了！"><a href="#他妈的！狗日的北京通勤太痛苦了！" class="headerlink" title=" 他妈的！狗日的北京通勤太痛苦了！ "></a><font color="red"> <strong>他妈的！狗日的北京通勤太痛苦了！</strong> </font></h2><blockquote>
<ul>
<li><strong>明日任务：wenda的demo。哎我真佛了这个b服务器 + docker好难用，不能直接在服务器安装conda吗，conda创建虚拟环境不也挺方便的</strong></li>
</ul>
</blockquote>
<h1 id="3-22"><a href="#3-22" class="headerlink" title="3.22"></a>3.22</h1><p><strong>创建了一个git的container，但是在安装lfs的时候出了问题，需要用sudo等一些命令，但是apt下载很慢，似乎需要在docker file文件中就更换镜像源</strong><br><strong>了解了docker中与宿主机相互cp文件、docker在创建时需要用GPU对容器可见的命令；</strong></p>
<blockquote>
<ul>
<li>docker run -it –net&#x3D;host –gpus all –name 容器名 -e NVIDIA_DRIVER_CAPABILITIES&#x3D;compute,utility -e NVIDIA_VISIBLE_DEVICES&#x3D;all 镜像名</li>
</ul>
</blockquote>
<p><strong>闻达demo终于无报错安装起来了</strong><br><strong>但是运行的时候，模型似乎卡住&#x2F;死循环？不返回计算结果。</strong>  </p>
<h2 id="他妈的！压力一天比一天大！EMO了要！"><a href="#他妈的！压力一天比一天大！EMO了要！" class="headerlink" title=" 他妈的！压力一天比一天大！EMO了要！ "></a><font color="red"> <strong>他妈的！压力一天比一天大！EMO了要！</strong> </font></h2><h2 id="今天美好的事情："><a href="#今天美好的事情：" class="headerlink" title="今天美好的事情："></a><strong>今天美好的事情：</strong></h2><p><strong>与SL、ZYR一起在附近那啥招待所吃川菜。味道不错，就是环境有点热、菜有点辣。此外，和ZYR一起自行车骑行，他一路讲解，聊的很开心！到北京以来为数不多的快乐时间。</strong><br><strong>SL骑着他那破电瓶车，本来跟我们一起的，半路给交警罚款了，就让他回去了，唉，我好难过</strong></p>
<blockquote>
<ul>
<li><strong>下周任务：调整闻达demo</strong></li>
</ul>
</blockquote>
<h1 id="3-23-3-24-周末！"><a href="#3-23-3-24-周末！" class="headerlink" title="3.23-3.24  周末！"></a>3.23-3.24  周末！</h1><p><strong>早上乘坐地铁到清河，和NJ汇合，一起去清河</strong><br><strong>做了半小时火车到怀来辣！比北京通勤还快。下了火车有一种终于逃离压力之源的舒畅感。</strong><br><strong>中文在饭店吃了什么洋葱和羊肉炒的东西，味道不错；然后还吃了莜面、炒扁豆角；喝了营养快线味的酸奶、杏仁露。杏仁露味道不错，加热和不加热是两个味道</strong><br><strong>吃完饭和NJ一起顺路去菜市场买了点粑粑柑（最后一口没吃带回北京了）。然后不行一阵子去了宾馆，双人间一晚90，条件还挺不错….要哭了</strong><br><strong>睡了午觉，然后出去找网吧打了几把大乱斗；下了新赛季的第一把云顶，玩不明白好痛苦，老六老七出局。然后出去转了转，看了一下县城的风土人情，买了西安特产甑糕，味道也不错。天气不好，阴沉天空使我不太开心。</strong><br><strong>相对于老家，这里明显更荒凉一些，没有人、没有年轻人。</strong><br><strong>晚上吃了砂锅。酸菜白肉和竹笋炖腊肉的砂锅；点了傀儡（土豆和面的混合物作为主食？），咸咸的，有葱花。点了冰糖芦荟，和椰果罐头一个味儿。吃完又转了转。回到宾馆玩了把金铲铲，然后下去买了10斤的半个西瓜和勺子。吃了一点，大头给NJ吃了（真能吃啊这B）</strong><br><strong>睡了一晚，第二天早上吃了豆面还是什么东西，然后走路去买了个吊炉烧饼分着尝尝；NJ点了驴肉和驴肠火烧的外卖，带回北京吃了</strong><br><strong>和NJ回北京了，BYD带我在海淀区骑了好久的自行车，最后在一家新疆馆子吃了抓饭和羊肉什么馍，喝了北京的汽水（和芬达一个味），吃的挺好。就是这b人带我转来转去最后吃这种快餐属实没绷住</strong></p>
<h1 id="3-25"><a href="#3-25" class="headerlink" title="3.25"></a>3.25</h1><p><strong>闻达demo部署完毕辣！</strong><br><strong>接下来需要：尝试更换通义7B&#x2F;14B的模型；查看闻达前端技术栈，与前端做交接让其修改；部署另一个&#x2F;几个项目（似乎要加班，tmd！）</strong><br><strong>已经将底模更新为7B，但是14B可能需要双卡，还不了解他这个框架怎么放到双卡上；</strong><br><strong>前端框架不太清楚，但是有一个二次开发的前端框架正在尝试，但是服务器网不好，nodejs安装困难。</strong><br><strong>下午看了B站上的一个从零开始大模型，感觉有点收获。还有一部分没看完</strong> </p>
<blockquote>
<ul>
<li><strong>明日任务：或许需要看一下另一个agent代理的部署；看langchain和LLM的相关教程视频；RAG目前似乎有一套实践模板，基于faiss检索？</strong></li>
</ul>
</blockquote>
<h1 id="3-26"><a href="#3-26" class="headerlink" title="3.26"></a>3.26</h1><p><strong>早上看langchain的视频教程</strong><br><strong>下午部署了wenda的二次开发webui，其中有报错；能保证基本的对话，但是文档对话功能用不了</strong><br><strong>下午抽空去面了腾讯的NLP应用研究实习生，他们是做腾讯视频剧本理解与智能助手的，感觉挺有意思。花了114定了钟点房，可惜面试的时候网不好、代码题（手写交叉熵、双指针&#x2F;滑动数组解决list中满足和为n的最小连续长度）也没敲出来，BERT的损失函数、交叉熵也没回答好。</strong>  </p>
<ul>
<li><p><strong>和SH交流了一下面试的经历，他目前的实习产品经理似乎工资低但是很轻松。他说TX面试结果很快会出，就可以再投了，但是我到第二天也没看到结果</strong><br><strong>部署了langchain-chatchat，可以基本运行，但存在一些问题，如知识库检索似乎没有工作、模型更换没有前端，需要在后端尝试修改</strong>  </p>
</li>
<li><p><strong>回家，地铁上和ZJX吐槽工作的事情，交流中得到片刻的安慰；到家后，和NJ电话聊了聊，大诉苦水，得到了一些开导和建议，好兄弟！；夜里躺在床上，和LF聊了聊他的离职和走全奖去美国读PHD躺平5年。唏嘘之余，有点羡慕</strong>  </p>
</li>
<li><p><strong>似乎，上班的人状况都不是很好：ZYR精神衰弱吃药、LF压力大怼领导辞职、SX天天emo不想动，我自己的精神状态也不是很好，唉。唉！</strong></p>
<blockquote>
<ul>
<li><strong>明日任务：调整部署项目，学习</strong></li>
</ul>
</blockquote>
</li>
</ul>
<h1 id="3-27"><a href="#3-27" class="headerlink" title="3.27"></a>3.27</h1><p><strong>早上给领导演示了一下两个demo，边上的前辈哥部署的DB-GPT也看了；这个效果挺好，但是也有要调整的地方。如服务器上部署模型的API调用等</strong><br><strong>今天看阿里Qwen的vllm教程视频，大有收获</strong><br><strong>铸币吧，LLM聊天一个字一个字显示的效果原来是清屏+ 打印啊！前端或许就是&lt;div&gt;内部的反复刷新？</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：调整部署项目，学习</strong></li>
</ul>
</blockquote>
<p>晚上，和妈妈聊了聊天，倾诉了一下。得知妹妹的奶奶肿瘤晚期，突然有一种恐惧和悲伤笼罩心头。帮妈妈网上填写了检查需要的材料，相关材料也进行了保存。与SH、LMD一起打了大乱斗，片刻的开心，但是过程中没有完全放松。</p>
<h1 id="3-28"><a href="#3-28" class="headerlink" title="3.28"></a>3.28</h1><p><strong>继续学习qwen+vllm的宝藏up主。对asyncio的内容有些不理解。这个up主好像是c++、java高手，学习大模型相关内容上手也很快，我感到很挫败。</strong><br><strong>小米面试，问Qquant还是啥的量化不了解，adapter不了解，代码题一是n个全排列中k个逆序对数量，完全不会；一个是写梯度下降求算术平方根，初始化值、lr和反向传播这些都写的不太好</strong><br><strong>下午，视频看的差不多了，准备看一下Dify是个什么东西</strong><br><strong>晚上，打SH、NJ打游戏，和妈妈视频，开森</strong><br><strong>和沈晓宇老师的师姐聊了聊城市、职业规划、未来发展，感觉认知更清晰了</strong></p>
<blockquote>
<ul>
<li><strong>明日任务：学习</strong></li>
</ul>
</blockquote>
<p>悲苦萦绕心头，不能散去。可能真的是房子住的还是有点远，每天通勤1.5小时。或许我需要换租了。</p>
<h1 id="3-28-1"><a href="#3-28-1" class="headerlink" title="3.28"></a>3.28</h1><p><strong>上午看了一会儿视频，然后捣鼓4090那台机子的docker和conda，没捣鼓出东西来。</strong><br><strong>下午问了前辈，了解怎么搞了，稍微搞了一点</strong><br><strong>下午，搞了会langchain-chatchat，发现推理慢的原因可能是gpu被占了；没找到如何更换模型为qwen</strong>  </p>
<blockquote>
<ul>
<li><strong>下周任务：qwenllm&#x2F;qwen的docker image本地下载好并上传到远程服务器上，运行查看效果</strong></li>
</ul>
</blockquote>
<h2 id="又-放-假-辣-！"><a href="#又-放-假-辣-！" class="headerlink" title=" 又 放 假 辣 ！ "></a><font color="red"> <strong>又 放 假 辣 ！</strong> </font></h2><h1 id="3-30-3-31-周末！"><a href="#3-30-3-31-周末！" class="headerlink" title="3.30-3.31  周末！"></a>3.30-3.31  周末！</h1><p><strong>周六早上，和SH打了LOL，中午出去吃了吉野家；下午躺了一会儿，然后晚上和NJ去逛了古玩市场，感觉有点同质化。</strong><br><strong>之后吃了白糖汁儿的杏仁豆腐，然后吃了永和大王的快餐，赶紧赶地铁回来参加TX的笔试题。但实际上我一面已经挂了，结果现在才出来，这题就没必要做了实际上，而且五题只做出来第一题，似乎打竞赛的才能做得出来多题，唉…</strong><br><strong>又和SH打了一会儿大乱斗</strong><br><strong>看小说，睡觉！</strong>  </p>
<hr>
]]></content>
      <categories>
        <category>记录</category>
        <category>大模型</category>
        <category>实习</category>
      </categories>
      <tags>
        <tag>日记</tag>
        <tag>随想</tag>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title>DailyRecord-April</title>
    <url>/2024/04/01/DailyRecord-April/</url>
    <content><![CDATA[<h1 id="4-1"><a href="#4-1" class="headerlink" title="4.1"></a>4.1</h1><p><strong>上午，把qwenllm&#x2F;qwen的docker image放到服务器上了，但是下午发现模型没放，还要下载72b-chat的模型再放上去；而且这个东西好像要自己写应该服务端py文件？</strong><br><strong>下午复习了一下long-context的论文，看了retrieval的一些如longllama，温故知新，没跳出已有框架的同时，感觉理解更深了</strong><br><strong>晚上，和SH打了一把游戏，然后和沈老师开周会，汇报了一下自己目前的一些理解。得到下一步的研究内容是：把retrieval 的方式在大模型上都实现一下</strong><br><strong>之后，继续和SH、NJ一起打了大乱斗</strong>  </p>
<h2 id="虽然是愚人节，但是无事发生"><a href="#虽然是愚人节，但是无事发生" class="headerlink" title=" 虽然是愚人节，但是无事发生 "></a><font color="red"> <strong>虽然是愚人节，但是无事发生</strong> </font></h2><blockquote>
<ul>
<li><strong>明日任务：阅读论文；学习修改大模型的方法</strong></li>
</ul>
</blockquote>
<h1 id="4-2"><a href="#4-2" class="headerlink" title="4.2"></a>4.2</h1><h2 id="草！这一天干啥了我给忘了！原来日记漏了一天，4-8才发现！"><a href="#草！这一天干啥了我给忘了！原来日记漏了一天，4-8才发现！" class="headerlink" title=" 草！这一天干啥了我给忘了！原来日记漏了一天，4.8才发现！ "></a><font color="red"> <strong>草！这一天干啥了我给忘了！原来日记漏了一天，4.8才发现！</strong> </font></h2><p><strong>好像还是在看代码和教程？</strong></p>
<h1 id="4-3"><a href="#4-3" class="headerlink" title="4.3"></a>4.3</h1><p><strong>上午，阅读qwen的model_qwen.py文件，尝试理解模型结构，寻找修改方法。最终目的是将kv retrieval加入到模型中去；目前没什么头绪，是直接改model文件，还是写个新的继承一下？继承的话如何与已有文件保持联系和交互？</strong><br><strong>qwen的py代码没有啥注释，突然想到可以看一下transformer包中的代码，拿llama做参考，希望有注释可以好看一点</strong><br><strong>吃了好大的饼</strong><br><strong>将4090机器上的qwen-72b-chat放到A800上，尝试让它跑起来；昨天前辈哥用vllm加速一些模型，基本上没跑起来报错，不知道是什么情况；没查到解决方案</strong></p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del> 明日个P！清明节假期！</strong><br><strong><del>好吧，这个饼还是挺不错的，也许会假期学习一下。不会的好多</del></strong></li>
</ul>
</blockquote>
<h1 id="4-4-4-6"><a href="#4-4-4-6" class="headerlink" title="4.4-4.6"></a>4.4-4.6</h1><p><strong>假期第一天：上午打游戏，中午吃了昨晚下班时买的罗森便当；晚上去NJ那里，吃了川菜馆子“椒榆”（好像是这个名字），一个炒鸡、一个黄焖茄子、一个蒜泥白肉、一个小酥肉（没怎么吃，打包带回去当第二天的早饭了）；之后在附近转了转，然后坐地铁去上地一个超市逛了逛，吃了又一家杏仁豆腐，不如德和斋；打包了红豆双皮奶和一个什么奶回去第二天吃；去地铁站打道回府，路上看见北体的一个小破门，<del>另外还有漂亮MM</del>。之后到家，和SH打了会儿游戏。</strong><br><strong>假期第二天：打游戏的一天奥，无事发生！和SH、NJ打了好久的游戏；中午吃的猪脚饭，还不错；晚上吃的炸蛋螺丝混，也挺不错，熙螺湾这牌子在仙林NJU的对面也开了一家，开了不久；所以点的时候还是比较放心的。</strong><br><strong>假期第三天：中午和NJ一起去安贞门；在一家小巷子里的老馆子里面吃地地地地地地地道儿的百京菜！八大碗中的牛杂，好吃！牛肠煮的最好吃；一个炒牛尾，还行；一个羊杂砂锅，不错，但是没牛杂好吃；一个麻豆腐，挺好吃，很新奇；吃完饭，去买了芬达，合起来和大洋是有点差别，感觉大洋更好喝一点；然后沿河的公园里面有什么花展，走马观花的逛了逛。</strong><br><strong><del>忘记给NJ带德和斋杏仁豆腐了，明明前天才夸下海口…</del></strong><br><strong>三天里，没怎么学习，网络小说倒是没少看；代码什么的搞了搞但好像是在做无用功…..</strong>  </p>
<h1 id="4-7"><a href="#4-7" class="headerlink" title="4.7"></a>4.7</h1><p><strong>不想上班啊啊啊啊啊啊！</strong><br><strong>上午折腾随身wifi，顺便把qwen-72b-chat完全放到了A800的服务器上，跑了一下服务端，正常；后续工作可能就是，尝试修改模型之类的，再把后端接口什么的搞一搞？</strong><br><strong>下午，搜索阅读了一些论文，感觉收获不大；觉得是自己的代码能力不足的问题，找了篇开源代码的去读，GitHub 1k star， 但是没读懂，感觉代码写的好像不太好；遂重新阅读transformer中llama的源码；因为之前发现qwen的结构和llama很相似，希望读懂llama后能很快的触类旁通。遇到了一些问题需要记录</strong>  </p>
<blockquote>
<ul>
<li>RoPe编码的实现，看的半懂不懂，没有深究，后续视情况看是否需要深入。</li>
<li>llama attention模块中，有这两个参数，尚未搞懂作用是什么：</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cache_position: <span class="type">Optional</span>[torch.LongTensor]</span><br><span class="line">position_ids: <span class="type">Optional</span>[torch.LongTensor] = <span class="literal">None</span>  </span><br></pre></td></tr></table></figure>

<p><strong>晚上在DNF神迹上抽黑钻售货机，因为没有自动抽取的功能，所以尝试用pyautogui写了个自动抽奖的脚本，但是实际运行的时候发现移动位置是对了，但无法点击到游戏程序内部的东西，又发现自己使用鼠标一直点击倒也可以，于是又开了鼠标连点器，发现鼠标连点器也点不到游戏程序内部的东西；这两种相似的现象带来的启发是：这种基于GUI模拟点击的程序，或许没有聚焦到应用程序内部，亦或者是DNF.exe的特殊性之类的。想来之前应该有一个获取游戏窗口的方法，但是不想搞了，就这样吧。</strong></p>
<blockquote>
<ul>
<li><strong>明日任务：继续阅读llama的源码</strong></li>
</ul>
</blockquote>
<h1 id="4-8"><a href="#4-8" class="headerlink" title="4.8"></a>4.8</h1><p><strong>今天，勉强把qwen的model文件看完，仍然是半懂不懂的状况，尤其是generate()和chat()这两个函数。logit的值映射到id再映射到文字，思路很好懂但代码有点复杂</strong><br><strong>后续重新看了GitHub上qwen的项目，以及modelscope上qwen7b模型的具体内容和文件。本来以为modelscope上面的是运行文件之类的，原来它真的只是模型，这个模型安装一定方式组织，transformer还是huggingface依照generation_config.json和config.json来完整的读取模型、运行之类的。运算、生成之类的函数确实是写在这个模型里面的，github上面的是介绍、示例的demo、各个数据集上的测试文件。</strong><br><strong>也就是说，如果要在模型中加入cache的retrieval，就要在modeling_qwen.py这个文件中直接修改？（但是我更希望的是，写个继承的文件然后互不影响？？）</strong><br><strong>得看看论文和论文代码去</strong><br><strong>晚上，和沈教授开了组会，表明目前的问题是不知道怎么改代码一头雾水，直接在模型文件上改感觉不稳妥；他建议我直接改试试</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：阅读recurrent-memory-transformer，开始修改qwen代码文件</strong></li>
</ul>
</blockquote>
<h1 id="4-9"><a href="#4-9" class="headerlink" title="4.9"></a>4.9</h1><p><strong>工程师大哥入职噜！希望能带我飞！</strong><br><strong>阅读recurrent-memory-transformer的源码，思考其结构和代码作用</strong><br><strong>modeling_rmt文件，大致上应该是模型文件，其中定义了两个类，MemoryCell——似乎是负责向原有model中加入MemoryCell的类；RecurrentWrapper——似乎是可以将原有model的输出再处理之后的输出，这样看来，好像可以直接将其运用到qwen中去？</strong><br><strong>下午和GG聊了一阵子，到出租屋后又和他聊了大概一个小时；和NJ打了几把大乱斗</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：继续阅读recurrent-memory-transformer，开始修改qwen代码文件</strong></li>
</ul>
</blockquote>
<h1 id="4-10"><a href="#4-10" class="headerlink" title="4.10"></a>4.10</h1><p>今日，新入职了一个本地学校的做数据的实习生<br><strong>上午，稍微思考了一下RMT的代码结构和用法；然后尝试在PC上跑一下千问的小模型查看效果</strong><br><strong>采用Int4版本，结果报错：</strong>  </p>
<blockquote>
<ul>
<li>CUDA extension not installed.</li>
</ul>
</blockquote>
<p>尝试安装cudatoolkit，结果依然没有解决<br>破案了，byd一直安装的cpu版本的torch之类的，可能是版本没对齐&#x2F;清华源的问题<br>来自HXD的支援：  </p>
<blockquote>
<ul>
<li>建议用pip<br>别用conda<br>conda不会检查环境里的冲突直接装<br>pip会先看有没有装</li>
</ul>
</blockquote>
<p><strong>他妈的破案了，python和torch版本太高了</strong>    </p>
<blockquote>
<ul>
<li><strong>明日任务：改千问暂停，协助跑通RAGFLOW</strong></li>
</ul>
</blockquote>
<h1 id="4-11"><a href="#4-11" class="headerlink" title="4.11"></a>4.11</h1><p>RAGFlow昨天下午工程师跑了，它最方便的功能还是直接调用api，但工程师尝试搞明白阿里的api怎么申请怎么用却失败了；从示例图上开RAgflow可以支持本地模型，但是在我们跑通的结果来看那个图上面的选项消失了，今天工程师问ragflow的群主得知，已经用ollama直接取代了。<br>似乎ollam调用更方便？准备部署ollama试试，但是由于4090显存不够了，因而要将其部署到A800上；镜像的上传又是折磨<br>踩了一个坑：  </p>
<blockquote>
<ul>
<li>scp命令指定远程端口，-p其实要大写成-P才是正确命令</li>
</ul>
</blockquote>
<p>导入之后，显示名称什么的为none，需要使用docker tag命令自己命名<br>继续琢磨怎么加mem， 想搜RMT的解析文章，结果搜出来发现说它就是Transformer-XL，然后去搜transoformer-XL，发现它是2019年的文章被拒稿了？后续改进的XLNet，这个已经被加入到了transformer官方包里面<br>查看了一些代码，感觉加不进去；但似乎渐渐得到了一些理解：这些网络结构什么的已经定好了，包括qwen，这些放在src&#x2F;model下的modeling文件定义了模型，而huggingface或者什么地方可以找到模型的一些与训练好的参数。那么已知的是qwen确实没法改模型结构？最多只能在推理的时候采取一些不影响它模型本身流动的trick，如lora&#x2F;adpter之类的东西。<br>qwen1.5采用了滑动窗口attn，可以提高推理效率；能否提高长文本能力尚不明晰。<br>似乎可行的两个思路：  </p>
<blockquote>
<ul>
<li>使用adapter，直接将之前的kv cache揉到attn里面？  </li>
<li>直接在模型推进的某一层，对kv cache和外部向量数据库 进行向量检索，然后揉进去？</li>
</ul>
</blockquote>
<p><strong>研究方向暂时转变</strong>：研究ReAct + CoT的应用；唉人在江湖身不由己，但好在是这回有人指导了，希望这段时间代码能力能有突破；越来越觉得LLM尤其需要强工程能力  </p>
<p>用千问的api，跑RAGFlow成功了，后续工作是更改成本地模型；此外，还有一些功能待添加：聊天界面可以输入图片&#x2F;文件，支持连接sql并发挥BI功能  </p>
<p>尝试用langchain，将React和CoT结合起来  </p>
<blockquote>
<ul>
<li><strong>明日任务：langchain的ReAct和CoT；了解一下CoT</strong></li>
</ul>
</blockquote>
<h1 id="4-12"><a href="#4-12" class="headerlink" title="4.12"></a>4.12</h1><p>今天没什么印象深刻的任务，在读一篇新的论文BPO，阅读它的代码。由于工程能力不足，阅读代码总是吃力半懂不懂；或许，LLM学习困难的原因在于实验条件高，我平时真的很难上手去改一些东西并快速查看效果。  </p>
<p>晚上，与ZYR和SL一起吃了聚宝源（西直门店）的涮肉；说实话感觉一般，那个B麻酱根本不香！感觉不如一般的火锅涮肉，亦或者之前在西安吃过的冰煮羊  </p>
<p>吃完饭，一起去紫什么公园逛了大概一小时；<br>今天与朋友们聊了一些求职、读博方面的事情，聊了一些高中同校、学弟学妹们的一些发展；再次感叹人外有人，每次聊天总是能感受到自己的不足，希望我能赶上去！实习实现工程能力的巨大提高！！！！  </p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del> 假期噜！明天和LT，NJ一起聚一下，吃筋头巴脑！</strong>  </li>
<li><strong>下周任务： 读懂BPO代码，复现</strong></li>
</ul>
</blockquote>
<h1 id="4-13-4-14"><a href="#4-13-4-14" class="headerlink" title="4.13-4.14"></a>4.13-4.14</h1><p>周六中午，和LT，NJ一起在西二旗吃了湘菜（<del>筋头巴脑无了</del>），280的套餐，大众点评好评送手撕包菜；有臭鳜鱼、擂椒皮蛋、小炒肉、辣炒茶干、肘子肉（应该不是这个，但我想不起来具体名字了）和鸡蛋烧的一个菜；喝了可乐；<br>吃撑了，LT饭量没衰减，而我已经吃不动了，唉。<br>吃完饭，随便逛了逛；路上飞虫进眼，忙忙糟糟虚惊一场；简单粗暴地买了瓶矿泉水冲了一下眼睛。<br>和LT一起去天坛公园逛逛。路上闻到一些味道，结果发现是沙比LT的衣服馊了；想到自己当年也穿过馊的衣服而不自知，还是亲娘发现的，不免有些好笑。<br>天坛公园要购票，15一张。和LT随便逛了逛，由于他要赶火车因而走马观花，只看了很小的一部分；聊了聊过去、现在、未来。似乎朋友们没怎么变，似乎又有点变化。但朋友身边总归还是安心一点。<br>实际上，和朋友们聊天、自己写博客记录，是我迷茫的一种体现，我或许希望在交流中明心见性、获得奋斗下去的动力。  </p>
<p>下午回去，打发时间；到了10.40，SH上号叫我打LOL；周日纯躺家一天，下午1点多和SH打了两小时大乱斗。NJ撺掇我玩dota2，未果；我暂时不想花费脑子在游戏上了，新游戏的学习在上学的时候可能是快乐，但现在我没有动力。  </p>
<h1 id="4-15"><a href="#4-15" class="headerlink" title="4.15"></a>4.15</h1><p>上午又入职了一位员工。开了个会，妈的不知道现在要干啥，云里雾里的。  </p>
<p>上午看了一些东西CoT的一些东西，结果下午又告诉我回去搞长文本。这是否有点……<br>下午，被告知搞一个长文本工程思路的PPT，于是梳理了一下想法，工程思路分为三类，每个类对应目前的一个具体例子。突然发现，之前的LongMem好像可以借鉴，于是重新去看它的代码。<br>这一看不得了，之前的认识还是太浅薄了，现在感觉他的代码思路都很明晰，然后虽然用的fairseq这个我没接触过的package，但是只是为了memory的方便，因而使用它的incremental decoder类。<br>从它的eval.py、memory以及fuse等文件中，隐隐感觉到可以用在qwen上，但我需要更详细的去理解代码。这个项目中一个memory bank模块，似乎可以很方便的迁移。<br>我必须立刻发动同调！<br>沈教授今晚有事，预定的周会推迟一天噜。<br>晚上，和NJ和SH大乱斗；和SH输麻了。</p>
<blockquote>
<ul>
<li><strong>明日任务：呱！我要狠狠地使用longmem呀！</strong></li>
</ul>
</blockquote>
<h1 id="4-16"><a href="#4-16" class="headerlink" title="4.16"></a>4.16</h1><p>上午继续阅读longmem的源码；它虽说进行了解耦，但从网络结构来看，是自定义了一个层数为底模一半的网络，然后输入、训练、生成memory bank的结果，然后一直存着不动，同时底模向前运行推进，最后再加一个融合层？也许是我理解不到位，代码看错了；总之是有点搞不懂的。感觉代码好像挺不错，但转化到qwen中有难度，因为他主要用的fairseq，而我还没学过，感觉这是一个深度学习的库，自定义模型什么的。<br>下午，尝试跑一下qwen的memory增强；首先跑了基础的底模，啥都没改，让它从西游记的一段话中抽取一个信息，应该是一个大海捞针的任务，没回答出来，那我的思路就是调整模型文件来查看效果。<br>其实，下午也看了一阵子评测数据集的东西，但是也是粗略的看看没大搞懂，可能主要因为看的评测数据集与我预想中的有所区别，C-EVAL什么的是通用，但是长文本&#x2F;超长文本可能还要那种名著数据集什么的，我记得之前看过有，但是一时间忘记去找了；倒也无所谓，因为这个qwen-1.8b-chat模型本身就没有长文本能力，ntk和logn缩放都无，只有7b和14b好像支持这个；因而我采用了上面的一个测试数据。<br>修改模型还是没什么头绪，longmem究竟如何加进去？  </p>
<p>晚上，开了周例会；得到指点，预计看三篇论文、使用faiss检索并将检索结果加入prompt中作为self-RAG的baseline，需要跑通这个实验  </p>
<blockquote>
<ul>
<li><strong>明日任务：学习faiss，将输入query先切片、再faiss、再构建prompt，再输入到模型中查看效果</strong></li>
</ul>
</blockquote>
<h1 id="4-17"><a href="#4-17" class="headerlink" title="4.17"></a>4.17</h1><p>今日，尝试将query 按句子切片；结果使用nltk、langchain等切片都未果，不知什么情况；于是使用简单的split暂作切分；结果发现是这个标点符号好像有点特殊，应该是另一种编码还是什么东西？但是NLTK应该确实是不能进行中文分词的。<br>尝试了langchain的文本chunk方法，也未果，到底是怎么回事？<br>终究，实现了将query进行faiss，然后加入到prompt中去，返回的结果不是很好，可能是由于模型本身的能力不足，也有可能是prompt模板需要优化。<br>他妈的！公司的网还是没接上！我都没办法测试真正的模型的效果、没法保证自己调整的可用性。</p>
<p>晚上回家，打大乱斗；输麻了，这b游戏能不能死一死。目前寻找其他可以和朋友一起玩的游戏中……但这个更换游戏的限制其实在于朋友之间共识的达成。  </p>
<blockquote>
<ul>
<li><strong>明日任务：阅读周会上的论文，尝试优化prompt，构建模板</strong></li>
</ul>
</blockquote>
<h1 id="4-18"><a href="#4-18" class="headerlink" title="4.18"></a>4.18</h1><p>上午，稍微调整了一下prompt，之前的prompt格式有点不正确；然后测试了西游记第一章其中一节的效果，似乎有提升，但是感觉不明显，可能是因为模型不太能处理这种半古文的原因。<br>于是，切换了内部的测试数据，运行查看效果，确实提升了模型的性能。  </p>
<blockquote>
<ul>
<li><strong>思考：</strong><br><strong>首先说一下目前存在的不足</strong>：目前，对于query的faiss处理，没有实现问题与前置信息的分离，仅仅是将整个query进行cut&#x2F;chunk，然后使用faiss检索、返回topk的结果，利用其构建一个增强的query；prompt注入方法的缺点已经很明显了，这里要说的主要是未来可能的faiss&#x2F;其他向量库检索结果错误&#x2F;不匹配的问题，正如之前提到的，没有实现问题和背景的分离；此外，就算实现了分离，问题和背景知识的向量也不一定能够匹配上去。<br>目前的实践，仅仅是一个简单的test，还有许多细节需要调整：history与历史faiss向量库的保留筛选机制；faiss向量在内存中的废弃与删除；未来的应用场景应该是去理解长文本乃至超长文本，或许还要涉及生成，因而超长文本下token、input、cache等的限制也需要考虑，当然这是后面的课题。<br><strong>其次，说一下确实实现结果优化的可能原因</strong>：正如之前提到的，这种实践的方法论与理论基础比较不踏实，但为什么实现了结果优化？可能的原因在于，最后的问题，由于关键词还是语义之类的，在向量化之后确实与背景信息的某一部分高度相关，从而在query的cut与向量化之后增强了相关的向量（sentence_transformer的原理似乎在于，计算句子向量与全文向量的相关程度，从而返回topk），使得大多数情况下相关背景信息能够被 <strong>“大海捞针”</strong>，这部分topk信息，加入到prompt中去，再输入到LLM中，在LLM内部又会因为重点信息的二次&#x2F;多次出现实现attention的增强，从而增强了返回结果。</li>
</ul>
</blockquote>
<p>下午阅读TransoformerFAM的论文，但是尚未理解透彻，不太能构想出来这种结构应该怎么在代码里实现。论文中说，可以不影响之前的权重参数，也就是说应该定义一个class来作为cache？<br>内网接入许可批下来了，可以使用服务器上的模型了。</p>
<blockquote>
<ul>
<li><strong>明日任务：阅读TransformerFAM，测试服务器上的模型的prompt优化效果</strong></li>
</ul>
</blockquote>
<h1 id="4-19"><a href="#4-19" class="headerlink" title="4.19"></a>4.19</h1><p>尝试将内外网访问走的通道分离，不然每次访问内网都要断wifi就很傻逼。<br>下列命令需要cmd以管理员身份运行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">route -p add 172.17.39.189 mask 255.255.255.0 173.17.39.254   </span><br></pre></td></tr></table></figure>
<p>三个地址分别是内网ip、内网掩码、内网网关。<del>（幸好网络接入需要mac验证，不然我这命令还不好放上来）</del>。<br>网上搜的教程还额外所有路由全走了一遍内网，纯沙比做法来的，只要这一个命令其实就可以了。  </p>
<p>突然发现自己鞋子穿的不是同一双，好笑之余，又有点感慨。或许我仍然没有长大，仍然毛毛燥燥地去面对这社会的一切，或许我就像穿上大人西装的蜡笔小新，在假装成熟。上次发生类似的事情，似乎是袜子还是什么东西穿的不是同一双，已经记不太清了。  </p>
<p>下午，整了一下午curl访问那个b服务器模型，都失败了；官方文档的命令试过了一点用也没有；但是，使用langchain的命令却成功了？？？？？基本上两小时浪费了，我真是无语子。但好消息是，这个prompt优化的效果，在7b上也很明显。<br>下午还看了一会儿TransformerFAM，似乎有一点实现的思路；不知周末是否会开始写一下 <del>（基本不可能）</del><br>今天发了一部分工资，上个月20号到本月10号的。</p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del>  哇袄！高一下至高二末尾的112寝室！三分之二的四人再聚首！吃一口地地地地道儿的百京八大碗！没毛病奥老铁们！</strong>  </li>
<li><strong>下周任务：后续复现捏~</strong></li>
</ul>
</blockquote>
<h1 id="4-20-4-21"><a href="#4-20-4-21" class="headerlink" title="4.20-4.21"></a>4.20-4.21</h1><p>周六，和SL、NJ一起吃之前和NJ在安贞门吃过的那个小馆子，地道儿！点了麻豆腐、炸蘑菇、八大碗之牛杂、豆泡（这个b豆泡和麻辣烫那个味道一样，真是血亏！）、扒肉条、羊肉烧卖、蘑菇和莴苣炒的一个素菜。吃的挺好。<br>啥比LT用他之前在群里吹的向老板请假的理由来应付我们😅，家人们真是一整个无语住了。<br>啥比SL坐地铁坐反了😅，坐到他妈的朝阳门去了，多等了半小时才吃饭，百京✌坐车主打一个随心所欲说是。<br>快下地铁的时候，NJ告诉我附近有cosplay的什么展子，因为他在地铁口看见了好几个穿的奇奇怪怪的人🤣；我出地铁口的时候，看见了一个JK的背影，是那种日式JK，因为那衣服的质感啥的一看就是cosplay的。吃完饭，NJ猜测cosplay的场地是在北投购物公园，于是我们走过去看看热闹。<br>在那个购物广场里面逛了第一次，我没看见，宝可梦道馆也被查封了，但在找宝可梦道馆的路上看见了一群coser，看背影认识一个钉琦野蔷薇；出了商场到附近的地方又稍微转了转，没什么好看的；于是原路返回，想去河边的公园邹洲、看一看花，结果这时候在购物广场里面看见了一大批coser，居然有<strong>小鸟游六花</strong>，泪目，细想已经12年了；SL说看见了约尔太太；当然还看见了二字游戏的萤妹（wc！O！）；其他零零散散的coser记不清了。<br>出了购物广场，又看见两个coser进广场，看来是真有展子；其中一个是miku（我去！初音未来！），可惜听NJ说是个坦克；我看的这些coser都没看见正脸，全是背影。在b站会员购上查了查，发现真是个展子，门票80的coser展，纯cosplay交流之类的，无同人志啥的，感觉没意思，也没有啥声优给我看（**<del>樋口円香你带我走吧😭</del>*<em>），就不去看了；去河边公园逛了逛，歇了歇，帮SL看一下他碧蓝航线的装备和配队；差不多14点的时候，各自打到回府了。<br>下午，NJ发了淘宝链接给我，是大窑打折，3</em>450ml只要8.9r，于是速速下单；又问他还有无推荐，给我推荐了7月临期的盐汽水，买了；<br>到出租屋之后，躺、看小说、刷视频、打劣质游戏。<br>晚上10点多，查公主链接EX5的作业，查到一个好像能抄，遂决心战斗；耗费2k母猪石、8E玛娜，一个多小时、SL十几次，终于通关EX5；  </p>
<p>周日，混；中午吃的烤肉饭外卖，晚上吃的螺 吸 混；和SH打大乱斗；<br><strong>他妈的！返回百京的票没抢到！全是候补！五一节回不了家什么的那种事情不要啊啊啊啊啊啊啊啊啊啊啊啊！如果没办法，只能请一两天的假延迟回百京了</strong><br>晚上8、9点的时候感觉眼睛有点不舒服，干涩，又有点困，于是早早睡了；</p>
<h1 id="4-22"><a href="#4-22" class="headerlink" title="4.22"></a>4.22</h1><p>周末的时候，华为的移动wifi到了，加上之前网管过来接网线但啥事儿都没解决反而把服务器的外网权限给断了😅（<del>百京的国企✌就是爷！</del>），因而服务器和PC都会通过这个wifi相连（终于不用我开热点了！）。<br>上午，阅读Infini-Attention的论文，稍微看了一下源码。这个示例代码是基于qwen2MoE模型的，迁移到qwen-chat上应该比较容易。但是话又说回来，MoE模型与chat模型的区别在哪里，还需要阅读一下源码。<br>可能的好消息是，Inifini-attention本体的代码还挺短的。<br><del>在阅读论文中的一个疑问就是：论文中Memory cache的更新，似乎是增量循环，同时在这个过程中memory没有Norm，这样会不会数值溢出？</del> 错误的，没疑问了，activation和norm发生在后面retrieval attention的计算与检索上面。  </p>
<p>下午，比较infini和qwen2-moe原生attention之间的不同；代码和思路应该是简单的，就是把额外的memory加到attn_output里面去；<del>但是 <font color="red"><strong>为 啥 qwen2-moe 的 attention 不 做 softmax 和 norm ？</strong></font></del> 铸币了，不是在这儿加add和norm的；<br>qwen这个attention，在计算完attention之后，又对它额外做了个nn.linear。<strong>这个操作的出处是哪儿？我查了llama的源码，发现它也是这么做的</strong>————哦tmd，<strong>又铸币了</strong>，原来这是MHA，多头concat到一起之后再用linear层融合一下。理论到实践之间的距离还需要去努力弥补！<br>看代码的时候突然想到，论文中做了cache消耗的对比，infini只用额外的1.5M，而memorizing transformer用了183M，为啥会差这么多？只在某一层加入cache有那么大的消耗吗？<br>看完了项目里面的model文件，对照qwen原文件之后，确定了要改的地方，基本上就是有M_Z这个参数的地方；其他的三个文件还没看。<br>晚上，和沈教授那边开了周例会，再次明确接下来的任务是无误的。<br>和SH大乱斗，输赢参半吧，就是遇到沙比选C但是C不起来挺无语的。<br>晚上，和ZZK聊了一会儿工作的事情，他现在在上海的滴滴实习，和我吐槽房子不好、天天加班、节假日也加班、实习工资低、孤独、精气神耗尽假期也不想动……我感同身受，还是那句话，大家对上班的感受都是一样的；<del>再次感受到人的天性就是厌恶工作的。</del>上完这个B班，回家往出租屋一躺，就感觉这辈子完辣😅。<br>其实，写博客、在博客中放飞自我（指瞎几把说、粗俗用语），是我在迷茫、孤独时的一种排解手段吧；给自己找一个能看见进度的、做起来不太麻烦、有点儿动力做的事情做，可以从中感到一点活着的意义。记录日常，一方面是看看自己是不是真的有进步？另一方面，也是记录一些难以复刻的经历与心境，避免自己遗忘一些东西，像是异地的格格不入、大城市的自卑之类的。  </p>
<blockquote>
<ul>
<li><strong>明日任务：看另外三个文件，尝试修改本地的模型</strong></li>
</ul>
</blockquote>
<h1 id="4-23"><a href="#4-23" class="headerlink" title="4.23"></a>4.23</h1><p>突然发现infini-attention的项目不是官方的，而是个人自己写的；github上还有几个实现的项目。好奇怪捏，为什么那篇TransformerFAM没有人写代码？<br>对照着项目，尝试修改qwen的model文件，结果发现，示例的实现代码是基于qwen1.5的，这个代码更加简洁，相比之下qwen的model更加杂乱&#x2F;冗余？我也不知道怎么说，因为还没有看到qwen1.5的其他model源码。<br>但是反过来说，修改qwen的model尽管可能更加困难，但相比之下应该更能提升我的工程能力、加深我对大模型流程和代码的理解，还是不能畏难放弃！  </p>
<p>下午，尝试修改qwen的model文件，对照infini attention、qwen源文件、qwen1.5源文件修改；qwen1在确定在哪儿融合memory上面有难度，按理说应该在实施了RoPE且加入到cache的q、k、v这一步操作之后，再利用这个cache进行memory融合，但这个代码有点杂乱，class里面为了实现attention又写了几个method，然而在qwen1.5里面就纯没有额外方法一步到位了，实现的很清爽，在1里面有些杂乱；<br>总之，废了不少力气定位了memory加入的位置后，却又发现了另一个问题：infini里面使用的是GQA，这个所需要的参数，在1.5里面通过config可以定义、是存在的。但是在1里面，config.json文件里面并没有这个参数的影子；也就是说，qwen1是不支持GQA的。这个参数是：</p>
<blockquote>
<ul>
<li>num_key_value_heads</li>
</ul>
</blockquote>
<p>在1.5里面，GQA中，需要计算group的数值，这个数值是这么计算出来的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.num_key_value_groups = self.num_heads // self.num_key_value_heads</span><br></pre></td></tr></table></figure>
<p>目前有两种思路，一是不要GQA，另一种是自己加上这个参数。在查看qwen1.5的configuration_qwen2.py之后，发现了这个新增参数它有一个默认初始化的值，初始化代码如下：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> num_key_value_heads <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    num_key_value_heads = num_attention_heads</span><br></pre></td></tr></table></figure>
<p>也就是说，如果num_key_value_heads没有定义，就默认等于num_attention_heads。我们可以直接qwen1的代码里面让它相等；这种方法实际上和不要GQA没什么区别，就是把infini的代码抄过来比较方便罢了。<br>原以为难关已经被攻破了，但在repeat_kv的时候又发现了问题：首先，qwen1里面是没这个玩意儿的，也就是说1也不支持需要repeat_kv的一系列操作；这是小事，只要去1.5里面把这个函数抄过来就行了；但关键的是1里面好像也没有对v进行cache，仅仅cache了q和k，有query_list和key_list，至少我初步看来是这样的；我得再仔细看看，真要没cache_v的话，改的工程量就太大了；还是直接去改1.5的好。<br>似乎，如果参数use_cache&#x3D;True的情况下，q、k、v是都会缓存的？而且直接就用query、key、value就行了？<br>另：infini的代码更新了，之前一些M_Z的改动似乎是不必要的，他将其删除了（这沙比owener代码有个mistake，我issue告诉他，结果他回复说“没<del>问</del>题~~”，然后立刻进行一个issue的关，结果我再看代码发现他偷偷把那个mistake给改了😅，真沙比）。<br>改好了代码，想要在本机上测试效果；结果cuda out of memory，难蚌；目前两个办法：一是换成0.5b-int4再试试，二是在服务器上部署跑一下；考虑到后续方便的问题，还是用服务器吧；但是conda create的时候报错了；解决ing……<br><strong>他妈的破案了！服务器的内存和硬盘都爆掉了！下班时间到了明日再战！</strong>  </p>
<blockquote>
<ul>
<li><strong>明日任务：在服务器上测试修改后的模型效果</strong></li>
</ul>
</blockquote>
<p>晚上，在出租屋混时间，没有打LOL；而是又拾起了✌最爱的怀旧页游！（如果WQ看到，又要搁那儿骂我山猪吃不了细糠啦！哈哈！）这游戏时不时就想捡起来玩一阵子，然后突然感觉无味、因而果断抛弃，往往下定决心再也不碰，却又偶尔想起、心痒难耐。我承认我是一个很怀旧的人，我或许一直无法脱离过去的光影。  </p>
<h1 id="4-24"><a href="#4-24" class="headerlink" title="4.24"></a>4.24</h1><p>上午，按照计划、尝试在服务器上测试修改后的模型的效果；在部署环境的间歇之余，看了一下qwen1.5的model文件，开头跃入眼帘的就是rotray embedding这个class；其实，看过的model文件，大多都像这样将其放在文件的开头；而且，基本上都是一样的写法，似乎都是从llama或者什么原初模型中copy过来的；鉴于之前并没有太搞懂其原理、代码，只是了解了五六分，因而又打算再看看；花了一阵子时间，看的眼花缭乱，最终收获却不如想象之中那么大，还是没有明晰。<br>终于，在环境部署好之后，尝试运行cli_demo.py，<strong>运行成功了！问答也能正确输出！</strong> 虽然结果不是很好，但是考虑到模型本身、以及加入的attention还没有经过ft或者什么操作，因而已经很满意了！希望这个现象代表的是：修改的内容是成功的，后续是可以推进的。  </p>
<p><strong>接下来要考虑的，主要有两方面</strong>：</p>
<blockquote>
<ul>
<li><strong>input query过长时的segment和retrieval、或许还有chat history的相关操作</strong>；  </li>
<li>infini的深入理解，与微调可能；  </li>
<li>在上述做完之后，qwen1.5的修改与尝试应该就是水到渠成的事情了</li>
</ul>
</blockquote>
<p>今天中午，又发了一部分工资，上个月20号到本月20号的。  </p>
<p>下午，看了一下其他的论文。准备看streaming-llm的代码，但是这个代码结构我得梳理一下。<br>因为苹果吃完了，所以在楼下lawson买了两个饭团当晚饭；饭团还在打折，好耶！<br>到出租屋，依然混时间；和SH打了两三个消失的大乱斗。  </p>
<blockquote>
<ul>
<li><strong>明日任务：阅读streaming-llm源码，看LLM教程。</strong></li>
</ul>
</blockquote>
<h1 id="4-25"><a href="#4-25" class="headerlink" title="4.25"></a>4.25</h1><p>今天上午，又入职了一位姑娘，似乎是北大的，和上周入职的老哥认识；捏妈，百京真是卧虎藏龙。感觉自己被Top2包围了。<br>看streaming-llm的kv-cache部分的代码，结合论文看，还是有点难懂捏。<br>昨天在服务器上创建了老长时间的环境给nm不知道是谁给删了😅😅😅😅。<br>中午，突然想起来昨天改的qwen-infini代码似乎有问题；去查看之后果然如此；这么想来，昨天的cuda out of memory应该是一个失误，而不是修改模型成功运行的信号。于是，下午继续琢磨修改；遇到了许多问题。下面列出最严重的问题：</p>
<blockquote>
<ul>
<li><strong>在infini-attention的初始化模块中register_buffer了M和z，同时对其进行kaiming初始化，但是后续forward的时候，却显示M和z是NoneType</strong>：如果不对M和z进行初始化，则回在计算logit的时候报错，因为logit值都为0；这边测试了一下初始化代码的写法，至少可以保证的是，init函数中的M和z是正确初始化的、正确登记的（？）<br>————<strong>原因</strong>：如果模型权重文件里面没有这个buffer，就算我初始化了，加载权重之后也会是nonetype；所以需要在使用之前再强制初始化一下。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><strong>此外，还有似乎是因为精度问题导致的错误：probability tensor contains either inf, nan or element &lt; 0</strong>：可能量化的模型都容易出现这个问题，百川有一个<a href="https://github.com/baichuan-inc/Baichuan2/issues/291">issue</a>似乎可以解决；但是一个下午看的头昏脑胀，不想再加班的，明天再搞吧。</li>
</ul>
</blockquote>
<p>奥牛逼，工程师大哥鼠标灵敏度贼低，结果nm是LOL电一前二十？？？</p>
<blockquote>
<ul>
<li><strong>明日任务：解决问题</strong></li>
</ul>
</blockquote>
<h1 id="4-26"><a href="#4-26" class="headerlink" title="4.26"></a>4.26</h1><p>继续尝试修改错误，首先确定具体问题所在，根据报错结果，去修改代码，print报错之前的相关信息，其probs和softmax之后的结果打印如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#probs</span></span><br><span class="line">tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br><span class="line"></span><br><span class="line"><span class="comment">#prob_after_softmax：       </span></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<p>可以看到这个结果，可能是第一个token开始这个概率数值就出错了；正如昨日所说，应该是精度转化的过程中出现了问题。尝试Qwen下面一个issue的<a href="https://github.com/QwenLM/Qwen/issues/276#issuecomment-1825326365">回答</a>，结果运行了很长时间，返回结果如下：<br><img src="error.png" alt="error" title="返回值"><br>一开始运行的时候显卡风扇咔咔吹，我还以为陷入死循环了呢，没想到还是给我返回了结果。这个结果大致上符合预期吧，因为照着上面修改了代码之后，直接选择概率最大的作为decode的值，但是从一开始到最末尾，概率值都为0的情况下，自然所有decode出来的token都是相同的；qwen中这个都是0解出来就是“！”，应该是这样的。<br>总的来说，这个结果是好的，<strong>说明修改代码没有问题，只是精度转化的问题。代码是可以正常跑通、一直decode到最后的</strong>；后面要做的就是进行微调之类的东西了；后续的问题，等到实战的时候再说吧！<br>下午，借用同时的设备，接入了公司的网，可以使用A800了。目前收到的任务是，在A800上，对修改后的模型文件进行调整、评测效果对比。<br>同时接入了wifi和以太网，花了好长去搜如何让ssh默认走以太网而非wifi，结果失败了，暂时只能物理切换了。<br>下载qwen-7b-chat的模型文件，修改model文件，<del>还需要从本地copy到远程服务器上，因为那儿不能连接外网。</del> 没事儿了，下载一会儿突然想起来modelscope不是外网，可以直接下；对于服务器里面没有git-lfs，也无法使用root权限调用apt-get命令进行安装的问题，可以使用conda安装：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install git-lfs</span><br></pre></td></tr></table></figure>
<p>感谢之前的自己把conda什么的国内镜像源都换好了🥰🥰<br>此外，没想到之前搞得环境有问题，安装不了faiss，<strong>原因在于python版本太高了</strong>。重写搞了个环境，幸好速度很快！搞好了环境之后，没量化的版本返回结果是这样的：<br><img src="error2.png" alt="error2" title="返回值2"><br>乱说一通！但确实是量化的问题奥！没毛病奥老铁们。  </p>
<blockquote>
<ul>
<li><strong><del>明日任务：</del>  我恨调休！放不起就别放！明天躺尸一天！</strong>  </li>
<li><strong><del>下周</del>后天任务：A800上的模型训练~</strong></li>
</ul>
</blockquote>
<h1 id="4-27"><a href="#4-27" class="headerlink" title="4.27"></a>4.27</h1><p>周末，在出租屋躺尸一天。本来打算中午睡个午觉、下午再出去骑行一两小时，结果下午百京竟然下雨了，计划泡汤。<br>中午和SH打了几把大乱斗，好像一直在赢。<br>昨晚和今天，看《魔都精兵的奴隶》，主要是看漫画，动漫也看了一点；不得不说很合我的胃口，纯爱卖肉番suki捏🥰🥰。看到了118话，禁漫上面的更新到此为止了，似乎还可以继续看十几二十话来着，但是我看小说、追剧之类的时候，总喜欢留一点，有种松鼠存松果的那种意思，好看的不能看完，得存着留点念想（除非忍不住）。<br>怀旧页游游玩过程中遇到了一点小挫折捏。  </p>
<h1 id="4-28"><a href="#4-28" class="headerlink" title="4.28"></a>4.28</h1><p><strong>他妈的调休！我！要！杀！你！一！万！遍！也！不！够！！</strong><br>看qwen的finetune文件，看看这个参数怎么冻结、怎么调。<br>我登记了一个param，名为beta，两个buffer，名为M和z；在一开始，我以为要训练beta，和M、z的初始分布，其他的参数进行冻结，于是我尝试查看这两个参数在哪儿。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看param</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.requires_grad)</span><br><span class="line"><span class="comment"># 查看buffer</span></span><br><span class="line"><span class="keyword">for</span> name, buf <span class="keyword">in</span> model.named_buffers():</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br></pre></td></tr></table></figure>
<p>结果发现，param列表里面有beta，但是buffer列表里面没有M和z。我感到很奇怪，为什么M和z没有加上去？难道是我哪里写错了？<br>于是，去搜集如何进行finetune的教程，终于发现，只有param是可以调整的参数，buffer只是常数罢了；因而，正确的调整方法应该是：调整每一层的beta参数和所有transformer结束后的全连接层。<br>那么，问题是，为什么我会觉得M和z是可以调参的呢？这个误区的原因在于，在infini代码中，M和z是一个类似RNN的更新机制，这个更新实际上是根据每一层的Q、K、V进行的；之前，在加载模型的时候，因为预训练模型中没有M和z、<br>因而被设置为None了，所以我在代码里面又加入了初始化M和z的分布；当时想的是，后面正常训练好的模型，M和z应该是有一个分布的；但是不是这样的，正如上面所言，M和z是Q、K、V基于类似RNN的方法得出的，是固定的生成流程；不固定的，只有门控参数beta罢了；<br>但是，现在还有一个可能的问题就是，既然M和z在推理过程中被设置为None，那么我训练的时候会不会也是None、因而无法操作？<br>在服务器端，将再次初始化的操作删除后，运行果然报错；但是，在将init里面的初始化中的kaiming分布去掉后，全用zero的分布反而不会报错正常运行，只是结果不太好罢了，但是也不会乱说一通。<br>于是，在finetune.py文件中，在模型加载和后加入以下代码，理论上就可以运行qwen自带的bash脚本进行调参：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span>(<span class="string">&#x27;beta&#x27;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&#x27;ln_f&#x27;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&#x27;lm_head&#x27;</span> <span class="keyword">in</span> name):</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(name, param.requires_grad)</span><br></pre></td></tr></table></figure>
<p>在训练数据集的选取上，则需要进行斟酌：qwen提到自己对多个数据集进行了打榜；那么，这些数据集是可以参考的，目前可能何时的，是C-EVAl以及中文长文本数据集；<del>但是长文本方面，对输入进行segment的代码还没写，因而暂定使用C-EVAL进行训练和评测。</del><br>仔细思考了一下，现在是模型的基础能力因为新加入参数和计算的原因有所下降，因而需要训练其基本的问答能力，至于评测集，我认为应该在基础能力满足之后进行进一步的调整。<br>使用什么数据集呢？我直接开始摇人！沈教授推荐了指令微调数据集COIG-PC。一开始在检索的时候我被导向了同一个机构下的早期数据集COIG，这个数据集比较小；而COIG-PC足足有126G。COIG-PC这个数据集还需要申请，是gated的，幸好申请的流程很简单。<br>由于模型太大，因而我希望直接在服务器上下载，但是服务器上又连不上外网，于是搜索huggingface镜像站。数据集地址在<a href="https://hf-mirror.com/datasets/BAAI/COIG-PC/tree/main">这里</a>；此外，由于数据集是gated的，因而需要在本站上申请一个access token，在命令行的时候把这个传进去，才能git下来。相关教程在网站首页上有。<br>总的来说，命令应该是这样的：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line">huggingface-cli download --token hf_access --resume-download --repo-type dataset BAAI/COIG-PC --save-dir COIG-PC --local-dir-use-symlinks False</span><br></pre></td></tr></table></figure>
<p>下吧！126G呢！该说不说服务器墙内下载速度还是快的。<br><img src="download.png" alt="download" title="服务器下载"><br><strong>下完之后，梳理了这个数据集的情况</strong>：<br>Hugging Face上面的datacard显示说，文件是jsonl、还给出了相关文件的命名；起初，我以为直接对jsonl文件处理即可，但事实不是。下载下来的文件全是parquet格式，是pandas的数据格式，而且命名与datacard中的完全不一样！；列名里面有filename，但我完全搞不懂这个玩意儿的意义何在：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">filename                 <span class="number">00001</span><span class="number">-001</span><span class="number">-000</span>-named_entity_recognition.jsonl</span><br></pre></td></tr></table></figure>
<p>总之，借助GPT大人的力量，写了一个处理数据的脚本，用于遍历文件、提取train且是用于’推理’数据、并形成最终的训练数据格式。理论上来讲，这一步搞好之后，直接调用训练的sh脚本就可以了。唯一担心的问题就是RAM可能不够，但好像是我多虑了，白白花了一些时间纠结分段存储。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyarrow.parquet <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># 获取所有符合条件的文件路径</span></span><br><span class="line">file_paths = glob.glob(<span class="string">&#x27;data/*train*.parquet&#x27;</span>)</span><br><span class="line"></span><br><span class="line">samples_list = []</span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line">query = <span class="string">&#x27;&#x27;</span></span><br><span class="line">answer = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_sample</span>(<span class="params">idx, query, answer</span>):</span><br><span class="line">    sample = &#123;</span><br><span class="line">        <span class="string">&quot;id&quot;</span>: idx,</span><br><span class="line">        <span class="string">&quot;conversations&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;from&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">                <span class="string">&quot;value&quot;</span>: query</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;from&quot;</span>: <span class="string">&quot;assistant&quot;</span>,</span><br><span class="line">                <span class="string">&quot;value&quot;</span>: answer</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sample</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每个文件并读取数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># f_idx = 0</span></span><br><span class="line"><span class="keyword">for</span> file_path <span class="keyword">in</span> file_paths:</span><br><span class="line">    <span class="comment"># 读取 Parquet 文件</span></span><br><span class="line">    <span class="comment"># print(&#x27;正在读取文件：&#123;&#125;&#x27;.format(file_path[5:]))</span></span><br><span class="line">    <span class="comment"># file_path = file_paths[-5]</span></span><br><span class="line">    data = pd.read_parquet(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 发现数据，构建数据</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在遍历文件：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(file_path[<span class="number">5</span>:]))</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> data.iterrows():</span><br><span class="line">        <span class="comment"># print(row)</span></span><br><span class="line">        <span class="comment"># print(idx,row)</span></span><br><span class="line">        is_correct_file = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> r_str <span class="keyword">in</span> row[<span class="string">&#x27;task_type&#x27;</span>][<span class="string">&#x27;major&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;推理&#x27;</span> <span class="keyword">in</span> r_str:</span><br><span class="line">                <span class="comment"># print(&#x27;find!&#x27;)</span></span><br><span class="line">                query = row[<span class="number">0</span>] + row[<span class="number">1</span>]</span><br><span class="line">                answer = row[<span class="number">2</span>]</span><br><span class="line">                sample = update_sample(idx, query, answer)</span><br><span class="line">                samples_list.append(sample)</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line">                is_correct_file = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="string">&#x27;信息抽取&#x27;</span> == r_str:</span><br><span class="line">                query = row[<span class="number">0</span>] + row[<span class="number">1</span>]</span><br><span class="line">                answer = row[<span class="number">2</span>]</span><br><span class="line">                sample = update_sample(idx, query, answer)</span><br><span class="line">                samples_list.append(sample)</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line">                is_correct_file = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_correct_file:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_correct_file:</span><br><span class="line">            <span class="keyword">break</span>     </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;文件：&#123;&#125;处理完毕！&#x27;</span>.<span class="built_in">format</span>(file_path[<span class="number">5</span>:]))</span><br><span class="line">    <span class="comment"># break</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;train_data_2.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    json.dump(samples_list, file, ensure_ascii=<span class="literal">False</span>,indent=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>或许基于file_name这个东西能更轻松？我不知道，反正这样就行了，能实现目的，<del>花费几十分钟也不太慢</del>草！得四个多小时！。<br>加班，等数据处理好到21点。我草！忘记import json了！尼玛白等了！<br>为了快速，观察到每个文件里面数据的用途似乎都是相同的；简单地从第一条数据开始判断吧！没有就跳过！但是失败了，结果是空的！此外，还犯了严重错误：没有copy变量而是直接在变量上修改，导致最后list全是一个变量！  </p>
<blockquote>
<ul>
<li><strong>明日任务：解决问题，准备好数据！开始训练！</strong></li>
</ul>
</blockquote>
<h1 id="4-29"><a href="#4-29" class="headerlink" title="4.29"></a>4.29</h1><p>首先，昨天确定的思路是：</p>
<blockquote>
<ul>
<li>查看昨晚的json数据是否正确</li>
<li>如果正确，则开始尝试训练模型；不正确，则改写正确的脚本，同时先用数个测试数据构成训练数据集，测试ft是否正常运行。</li>
<li>根据昨天晚上，运行训练的sh脚本失败的问题，排查原因。</li>
</ul>
</blockquote>
<p>遗憾的是，昨天的代码还有个小错误，导致数据未正确提取；这也是昨天“从第一条数据开始判断该文件是否是所需数据集的文件，返回结果为空的原因”——str in list只会查看list元素，而不会查看元素中是否含有相关字符串。<br>总之，数据总算是正确提取了！尝试运行bash脚本，报错信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FAILED: multi_tensor_adam.cuda.o</span><br><span class="line">/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -I/home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include/TH -isystem /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/torch/include/THC -isystem /home/zhangshuo/.conda/envs/QwenMem/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options &#x27;-fPIC&#x27; -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /home/zhangshuo/.conda/envs/QwenMem/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o</span><br><span class="line">nvcc fatal   : Unknown option &#x27;-generate-dependencies-with-compile&#x27;</span><br><span class="line">ninja: build stopped: subcommand failed.</span><br></pre></td></tr></table></figure>
<p>报错看不懂….头晕捏。<br>手忙脚乱了一下午，在病急乱投医的情况下，运行了查看cuda的命令:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$CUDA_HOME</span></span><br></pre></td></tr></table></figure>
<p>结果发现输出为空，同时又注意到nvcc -V显示的cuda是根目录而非虚拟环境下的cuda，这是否意味着，虚拟环境的cuda没有安装&#x2F;安装失败？<br>安装cuda 的命令，搜集了半天，踩了不少坑，最终确定是这个：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install cuda -c nvidia/label/cuda-12.1.0</span><br></pre></td></tr></table></figure>
<p>虽然echo的结果还是空，但是nvcc -V显示的cuda版本是12.1，对了！<br>再次尝试运行，结果报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">KeyError: <span class="string">&#x27;conversations&#x27;</span></span><br></pre></td></tr></table></figure>
<p>搜了一下，说是数据格式的问题。接下来要调整数据格式。结果发现自己铸币，dict中的conversations少打了一个s😅。<br>一切调整好之后，运行终于成功了！让它在A800上跑着吧！<br>不对，10天太长了！我应该先抽取一点数据，暂定4w条吧，进行训练试试；数据集这一块儿的选取其实还需要仔细斟酌。<br>抽取了数据，训练大概需要7小时；明天看一下结果！</p>
<p><strong>好吧！来自沈教授的场外支援！我超参数用的默认的，这这不对。现在全数据也只要训17小时！明天中午就好了！</strong><br><img src="training.png" alt="training" title="正在训练"> </p>
<blockquote>
<ul>
<li><strong>明日任务：查看训练结果，思考后续优化方向（更好的数据集？）</strong></li>
</ul>
</blockquote>
<h1 id="4-30"><a href="#4-30" class="headerlink" title="4.30"></a>4.30</h1><p>梳理一下目前的思路吧：</p>
<blockquote>
<ul>
<li>首先，对于目前长文本的解决思路，有两种，一是采用faiss之类的向量库，对输入的query进行重构、检索、增强；这种方法见效快，但是受到模型本身功能的限制；如果模型长文本能力、理解与提炼能力本身不足的话，这个方法遇到瓶颈之后就无法提升了。其次，则是对模型底层进行重构、修改，这种方法理论上可能性是无限的，但是，在实践过程中的问题在于，大模型对我们来说是一个黑箱。正如上面所提到的，在加入infini-attn的时候，模型的基本对话能力大大下降了；新增的参数，会对预训练好的权重造成影响，因而需要进行ft，这里的问题在于：训练数据如何选取？如何训练？评测集是什么？  </li>
<li>一个重点是，上面提到的两种方法，都需要使用到：对于input query的segment&#x2F;cut，以及之后的融合。原因在于，对于前者来说，query长度当然是固定的，我们所追求的目标是在有限的query长度中构建最好的prompt，对于长文本，则可能需要分段精炼再整合、循环往复；而对于后者，尽管不需要在prompt阶段耗费大量经历，但是模型本身的输入也是有长度限制的。目前不知道这个长度限制能否设置去改变，以Qwen-7B-Chat为例，它的输入长度就是512个token。加入infini之后，理论上可以接受无限长的数据，但是需要在generate&#x2F;chat或者哪个传入query的时候对query进行slice，一段段的喂入。效果上来说和全部输入没有区别，但是因为模型限制，需要一段段传入（细想一下不应该这样，或许可以设置输入无限长？）</li>
</ul>
</blockquote>
<p>今早的坏消息是：昨天的train的东西，似乎在我这边ssh断开之后就停止了？总而言之，预定的中午训练完成是没有的。我只好再次从里面提取更少的30w条数据，进行训练，需要三小时，预计今天中午完成。<br>今早，出于同事们接入A800服务器的需求，突然想到可以把我目前在用的转接的这个eth给接到服务器上，这样服务器可以直连公司外网和A800；同时，我们的eth虽然不能访问公司外网，但是可以通过内网连接到服务器。也就是说，只要这样做，就可以都正常使用A800了！试了试果然成功了！<br>昨天调用train.sh的时候，一个有趣的现象是：安装flash-attention后脚本会报错；这是因为我在model里面没改flash-attention的函数，因而加入的参数矩阵似乎因为shape不匹配无法相乘、使用flash-attn加速，卸载之后就好了。当时想要用flash的原因在于，10天的训练时间太长辣！但是后来多亏沈教授的支援，让我在实践中对LLM中ft的超参和作用有了更深的理解。<br>顺便记录一下后续应该会常用的命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># nvidia-smi实时刷新</span></span><br><span class="line">watch -n 5 nvidia-smi</span><br><span class="line"><span class="comment"># 查看文件夹下文件及其大小</span></span><br><span class="line"><span class="built_in">ls</span>  -lht</span><br></pre></td></tr></table></figure>
<p>只会显示窗口大小内可以放下的gpu状态，如缩略状态是0-3卡，全屏化只会是0-7卡。<br>查看qwen的各种文件，主要是config文件和chat方法，查看query的处理流程，似乎模型里面query进行split不太难？<br>午休过后，模型跑完了，准备加载模型并检验效果，结果出现了如下报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ValueError: Trying to <span class="built_in">set</span> a tensor of shape torch.Size([64, 32, 128, 128]) <span class="keyword">in</span> <span class="string">&quot;M&quot;</span> (<span class="built_in">which</span> has shape torch.Size([32, 128, 128])), this look incorrect.</span><br></pre></td></tr></table></figure>
<p>对此，GPT大人的答复是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这个错误提示表明你正在尝试将一个形状为 [64, 32, 128, 128] 的张量设置到一个形状为 [32, 128, 128] 的张量中</span><br></pre></td></tr></table></figure>
<p>琢磨了一个小时不得其解。与同事讨论之后，去上了个厕所，急躁的心略有平静。灵光一现，问题解决了！<br>首先，最奇怪的是，这个M是register_buffer的cache，按理来说不是一个可学习的参数，在init中设置的初始形状就是三维的。而这里显示的是四维tensor载入到M。这带来的问题是：M不应该被保存到模型中，而是应该在每次query输入&#x2F;对话开始的时候被重新初始化。<br>同时，这个四维tensor的第一维度大小是64，这个64是batch size，是我昨天调整的时候设置的。那么，结合上面的逻辑，这个64维出现在这里，应该是训练的时候，batch中有64个样本，对应64个M，因而最后，register_buffer保存的是这个形状的tensor。在加载模型的时候，M初始化应该是三维的，但是模型文件里面它想从safetensor里面加载M出错了，原因在于形状不符合。<br>那么，问题的关键在于，register buffer的这两个参数不应该被记录到模型文件中去。解决方法很简单，persistent参数设置为False即可。<br>解决这个问题后，用一个数据进行模型的训练、保存，并最后进行模型加载与对话，成功了！<br>重新用之前准备好的数据去训练模型的时间已经不够了，于是我重新准备了COIG-PC中命名实体识别领域的10w条数据，只需要70分钟即可训练完成。同时，由于COIG-PC中这个数据集样本的文本都比较长，感觉也可以同时训练一点长文本和对话能力。一个参考样本数据如下：</p>
<pre><code>当给一个没有标注的中文文本添加实体标记时，可以按照以下简明指示进行操作： 阅读并理解待标注的文本，识别出其中的实体，包括时间、地点、人名、组织名、公司名和产品名。 根据实体的类型，在实体文本两侧使用双大括号（&#123;&#123;&#125;&#125;）进行标记。 在双大括号内部，使用实体类型和冒号分隔实体文本，格式为：&#123;&#123;实体类型: 实体文本&#125;&#125;。 根据实际情况，使用以下标记类型来表示不同的实体： 时间：&#123;&#123;time: 2023年5月29日&#125;&#125; 地点：&#123;&#123;location: 北京&#125;&#125; 人名：&#123;&#123;person_name: 张三&#125;&#125; 组织名：&#123;&#123;org_name: 中国共产党&#125;&#125; 公司名：&#123;&#123;company_name: 腾讯&#125;&#125; 产品名：&#123;&#123;product_name: iPhone 13&#125;&#125; 在整个文本中，通过添加实体标记将所有识别出的实体进行标注。 确保实体标记的格式统一、准确，并与实体文本对应。 请按照如上指示标记下面文本：浙江在线杭州4月25日讯（记者 施宇翔 通讯员 方英）毒贩很“时髦”，用微信交易毒品。没料想警方也很“潮”，将计就计，一举将其擒获。记者从杭州江干区公安分局了解到，经过一个多月的侦查工作，江干区禁毒专案组抓获吸贩毒人员5名，缴获“冰毒”400余克，毒资30000余元，扣押汽车一辆。黑龙江籍男子钱某长期落脚于宾馆、单身公寓，经常变换住址。他有一辆车，经常半夜驾车来往于杭州主城区的各大宾馆和单身公寓，并且常要活动到凌晨6、7点钟，白天则在家里呼呼大睡。钱某不寻常的特征，引起了警方注意。禁毒大队通过侦查，发现钱某实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。
</code></pre>
<p>训完之后，可以在对话中直接用这个样本测试一下效果。<br>好吧，由于测试数据很单一，结果不好（测试数据是句法分析好像是）<br><img src="badoutput.png" alt="badoutput" title="句法分析"><br>我将会用全数据去调一下，看看效果。理论上说，这个infini-attn的结构，用于单一场景应该是见效很快的。训练数据可能需要多样性、包含对话之类的。这个结构说实话不太有趣。我希望能尽量不对已有参数造成影响，而是增强。<br>130G全数据A800也放不下，只能用Top200的数据，多跑几个epoch了。</p>
<blockquote>
<ul>
<li><strong>明日任务：查看训练结果，思考后续优化方向（更好的数据集？）</strong></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>记录</category>
        <category>大模型</category>
        <category>实习</category>
      </categories>
      <tags>
        <tag>日记</tag>
        <tag>随想</tag>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title>DailyRecord-May</title>
    <url>/2024/05/02/DailyRecord-May/</url>
    <content><![CDATA[<h1 id="5-1-——-5-5"><a href="#5-1-——-5-5" class="headerlink" title="5.1 —— 5.5"></a>5.1 —— 5.5</h1><p>5.1 五一假期的第一天。再说一遍：我恨调休！我恨调休！<br>昨天晚上21：46的火车，早上6：35火车到站。只有站票，我和LT带了马扎上去，结果发现能放马扎的地方也很少。在4月中旬的时候，我和LT说一起回家，聊到站票买马扎的事情，我说我买两个，他不用买直接过来就行，结果他忘了自己也买了一个。于是多出了个小马扎。我是14车，LT是15车，他在15车帮我占了个位置，结果发现是厕所门口，而这个厕所的门还坏了，乘务员过来修反而把门直接给搞下来了；中途，LT试图将多余的马扎以15R推销出去，结果失败了🤣🤣🤣🤣。<br>每节车厢都带个厕所，因而站票大抵是都要在厕所附近窝着的我是不想当老八的，正好听闻说餐车那边有空位，因而我拎着我的两个小马扎过去找位子。到那儿发现餐车坐满了，估计都是站票过来买饭送座位的。餐车的尽头比较空旷，只有一个哥们儿在地上铺了个纸躺着。我在这儿蹲了下来，LT怕被赶走，让我看看情况，半小时没人赶就叫他。确实没人赶，然后他就过来了，过来一合计，吃点东西，然后去问有没有空位。乘务员腾了两个相邻的位子给我们，然后96r，没得选择，两人吃鱼香肉丝饭。我吃了一碗半，LT吃了两碗半。吃完后，可以名正言顺的占用餐桌。<br>由于跨月，因而碧蓝航线要刷一下大世界的剩余体力，玩手机到1点，LT默默陪着我一起玩，我放下手机趴下他也趴下了。说实在话，虽然有坐，但是不安静，乘务员嘈杂，他们不关心你是不是在睡觉；尤其是2点多乘务员集体早餐，我们那个位置需要暂时让一下给乘务员吃饭，吃完饭一般立刻就走了，但是有不走的留在桌子上、东百口音的男的在吹牛逼，不压低声音。总而言之，到4、5点的时候，我们也只睡了质量很低的断断续续的2小时左右。然后到了开早饭的时候，被乘务员撵走了，我们到了餐车末尾的地方蹲了一小时，又被撵走了。<br>总之，6.35到车站，然后LT他妈妈开车来带我们，把我送到家。妈妈在家等我，我洗了个澡，然后七点多开始睡觉，一下睡到中午12.30。然后去门市吃午饭，牛肉啥的，家常菜。啊！家的味道！<br>下午，LSH叫我出去上网，我们把LT叫上了。线上叫了道哥，以及我的研究生同门SH，凑了五黑，大大乱斗。下午两点打到8点，十几把输了两把，爽赢！网吧有茶水售卖，喝了红茶。<br>然后，我骑的自行车留在了网吧门口，LT开车带我、LSH、SXZ一起去步行街吃龙虾。喝啤酒、吃下酒菜，倾诉一下，聊聊在北京一个月的一些经历和感想。啤酒喝晕了，他们驾着我打车去浙江商城那儿的KTV唱歌。我真不知道自己是怎么过去的，只知道自己被人驾着。到那儿，躺到了0：30，醒了，唱歌到2点，走路回家睡觉。<br>和朋友们喝酒是开心的、唱歌是开心的。<br>5.2 五一假期第二天。中午去我爹那儿吃午饭。我的大爷、我爸的新对象和那个女儿也来了，喝了点啤酒，聊了聊一些烦恼。但是在这里接收到的只有“努力！拼搏！”一类的大道理、大男子&#x2F;大家长主义的说教。挫败，再次决定不应该向这里倾诉一些东西了，在母亲那儿更放松一点。但这个家族氛围就是这样，总体上还是和睦的、较好的，一些东西应该学会去忍受。<br>下午，依旧昨天的配置、昨天的阵容，今天点了枸杞菊花茶，因为感觉这个好像是明目的，最近感觉视力有所下降。一开始连跪，沉默不语，然后连胜了，最后打到18点多，光荣下播！<br>骑着自行车到门市吃饭捏，家的味道！*2🥰🥰<br>5.3 五一假期的第三天。中午去我爹那儿吃饭，下午还是和他们一起去网吧。晚上去门市吃饭，之后去了舅爹舅奶家。看了看菜地、菜地边上舅爹晒太阳的破沙发。我妈看生菜长得好要了点回去。之后，在地下室的屋子里面坐着聊天，临走时，我把装在信封里面的、从银行取出来的新钱2k给了舅爹舅奶，实习工资的一部分，很有纪念意义。感到舅爹舅奶有点手足无措、又有点失落。<br>5.4 五一假期第四天。中午去门市吃饭，然后在家里躺了一会儿。下雨了，NZY告诉我他在初中部那儿和初中老师吃饭。LT开车去带了他，还带来了HL。我们一起在网吧打了一两盘，他们就都走了。HL感觉没啥精气神儿了，这班上的。NZY也好像有点心事，感觉他不像以前那样开心了。晚上，到门市吃饭，和妈妈一起回家。<br>5.5 今天回百京。下雨，早起坐上去宿迁的车，大概9点到高铁站。13点多到百京，点了个东北铁盒饭外卖，到出租屋的时候吃。下午和SH打了大乱斗，一直输。在出租屋没事干，混日子呗。  </p>
<p>家里真舒服啊！真舒服啊！</p>
<h1 id="5-6"><a href="#5-6" class="headerlink" title="5.6"></a>5.6</h1><p>复工。查看五一期间跑的训练怎么样了。发现训练没跑完，然后vscode远程sshA800也连不上了，但是bash可以连。百思不得其解。终于发现原因是保存的checkpoint文件太多、把硬盘填满了（本来剩下几百G）。测试checkpoint的效果，正常问答和指令问答效果都很差。看样子需要重新斟酌。需要仔细选取数据，进行调整。或者直接放弃改底层模型，因为这导致训好的模型被利用的很差。  </p>
<p>此外，加载模型时候有这个提示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Some weights of the model checkpoint at output_qwen/checkpoint-2000 were not used when initializing QWenLMHeadModel: [<span class="string">&#x27;transformer.h.0.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.1.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.10.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.11.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.12.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.13.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.14.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.15.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.16.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.17.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.18.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.19.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.2.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.20.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.21.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.22.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.23.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.24.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.25.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.26.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.27.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.28.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.29.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.3.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.30.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.31.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.4.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.5.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.6.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.7.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.8.attn.bias&#x27;</span>, <span class="string">&#x27;transformer.h.9.attn.bias&#x27;</span>]</span><br><span class="line"></span><br><span class="line">- This IS expected <span class="keyword">if</span> you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</span><br><span class="line"></span><br><span class="line">- This IS NOT expected <span class="keyword">if</span> you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</span><br><span class="line"></span><br><span class="line">Some weights of QWenLMHeadModel were not initialized from the model checkpoint at output_qwen/checkpoint-2000 and are newly initialized: [<span class="string">&#x27;transformer.h.0.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.1.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.10.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.11.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.12.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.13.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.14.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.15.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.16.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.17.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.18.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.19.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.2.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.20.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.21.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.22.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.23.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.24.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.25.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.26.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.27.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.28.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.29.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.3.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.30.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.31.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.4.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.5.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.6.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.7.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.8.attn.beta&#x27;</span>, <span class="string">&#x27;transformer.h.9.attn.beta&#x27;</span>]</span><br><span class="line"></span><br><span class="line">You should probably TRAIN this model on a down-stream task to be able to use it <span class="keyword">for</span> predictions and inference.</span><br></pre></td></tr></table></figure>
<p>attention的bias和我后面新加的beta参数，没有从模型中加载到。为什么？bias这个好像不影响。但是beta是核心参数，没了直接乱说一气。</p>
<p>开发大哥那边需求连接服务器，需要服务器接上wifi，同时能够连接A800。本以为是个改路由的大工程，但是没想到直接接入网络就行了。也许是接入网络先后顺序的问题？搞不懂。  </p>
<p>改了infini里面的结构，本来是融合后在projection，我把projection放前面了。因为我不想动这个projection的参数，只想训练融合的参数信息。不知道结果会怎样。如果后融合的话，projection的参数也要变，模型本身能力直接寄。</p>
<p>晚上，开了例会。前辈哥给出了问题的可能解决方法，明天需要测试一下：</p>
<blockquote>
<ul>
<li>检查checkpoint目录下的config.json、modeling_qwen.py文件是否与预期一致</li>
<li><strong>super().<strong>init</strong>(config)</strong> 移到最后，检查是否是继承类的初始化问题</li>
</ul>
</blockquote>
<p>此外，对于c_proj是否需要调整位置的问题，建议还是先不调整试试，一般影响不大。   </p>
<p>晚上，和LT、SH一起大乱斗，赢！  </p>
<blockquote>
<ul>
<li><strong>明日任务：测试解决方案是否可行</strong></li>
</ul>
</blockquote>
<h1 id="5-7"><a href="#5-7" class="headerlink" title="5.7"></a>5.7</h1><p>测试昨晚商讨的解决方案。无效果。<br>尝试打印模型结构和参数，结果显示attn已经修改，且模型参数里面有beta：<br><img src="model_arc.png" alt="model_arc" title="模型结构"><br><img src="beta.png" alt="beta" title="beta存在"><br>没办法，上不去下不来，卡在这儿了😅<br>看了一下相关的issue，似乎是加载模型的问题，好像要改from_pretrained。  </p>
<p>来自ZYR的场外支援！开了个腾讯会议，排查了两小时，确定是from_pretrain的问题！我到这不知道该怎么办，他给出了<strong>另一种加载模型的方式</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig</span><br><span class="line"><span class="keyword">from</span> safetensors.torch <span class="keyword">import</span> load_model</span><br><span class="line">config = AutoConfig.from_pretrained(<span class="string">&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_config(config)</span><br><span class="line">ckpt_file_1 = <span class="string">&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00001-of-00004.ckpt&quot;</span></span><br><span class="line">model.load_state_dict(torch.load(ckpt_file_1), strict=<span class="literal">False</span>)</span><br><span class="line">ckpt_file_2 = <span class="string">&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00002-of-00004.ckpt&quot;</span></span><br><span class="line">model.load_state_dict(torch.load(ckpt_file_2), strict=<span class="literal">False</span>)</span><br><span class="line">ckpt_file_3 = <span class="string">&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00003-of-00004.ckpt&quot;</span></span><br><span class="line">model.load_state_dict(torch.load(ckpt_file_3), strict=<span class="literal">False</span>)</span><br><span class="line">ckpt_file_4 = <span class="string">&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem/model-00004-of-00004.ckpt&quot;</span></span><br><span class="line">model.load_state_dict(torch.load(ckpt_file_4), strict=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>在此之前，需要将safetensor逐个转化为ckpt文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> safetensors.torch <span class="keyword">import</span> load_file</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">path = sys.argv[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;path not exists&#x27;</span>)</span><br><span class="line">    sys.exit(<span class="number">0</span>)</span><br><span class="line">device = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">weights = load_file(path, device=device)</span><br><span class="line">weights[<span class="string">&quot;state_dict&quot;</span>] = weights</span><br><span class="line">torch.save(weights, os.path.splitext(path)[<span class="number">0</span>] + <span class="string">&#x27;.ckpt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>将safetensor转化成ckpt，然后加载。加载的比from_pretrain慢好多！但是问题解决了！我必须立刻去研究一下这个加载方式的原理！<br><strong>真得给我朱哥磕一个吧，帮我太多了！</strong><br>safetensor是安全的，也就是说为了确保安全性、不包含恶意代码之类的，就要限制其中的东西，因而不让改动，也可以理解。 我定位到了问题，但没能解决的原因，在于对权重文件及其加载了解不深入，实际上这对我是傻瓜式的操作、黑箱。<br>晚上没有战斗，混日子捏……感觉不到啥拼搏的动力，浑浑噩噩月光族也是活着……  </p>
<blockquote>
<ul>
<li><strong>明日任务：测试训练好模型的效果</strong></li>
</ul>
</blockquote>
<h1 id="5-8"><a href="#5-8" class="headerlink" title="5.8"></a>5.8</h1><p>昨天的代码，默认是在cpu上加载模型的，需要改成如下格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">torch.load(ckpt_name, map_location=device) </span><br><span class="line">model.to(device)  <span class="comment"># 这一步不知道究竟是否需要</span></span><br></pre></td></tr></table></figure>
<p>对话效果烂了。我决定照着原论文的结构来复现，再训练一遍，看看到底怎么回事。<br>看了过程中tensor的shape，似乎之前的代码shape不对？也许效果不好的原因就在于此。改了之后再跑训练看看效果叭！<br>趁着训练的时候，查看ckpt和safetensor的区别。<br><del>ckpt这种文件似乎是torch的官方保存方式，以dict形式存储torch.nn.module的模型结构及其对应的参数，在加载的时候使用torch.load加载。这个过程是很直观简单的。（正常训练结束的文件，好像应该是.pt或者.pth？）</del><br>以下内容来自网络：  </p>
<blockquote>
<ul>
<li><strong>.ckpt文件是Pytorch Lightning框架中使用的模型文件格式之一。Pytorch Lightning是一个基于Pytorch的轻量级深度学习框架，它提供了更简单和更高层次的API，用于训练和管理深度学习模型。.ckpt文件保存了模型的参数和优化器的状态，并且通常还包含训练的元数据信息。</strong></li>
<li><strong>.pth文件是Pytorch中最常见的模型文件格式之一。它是一个二进制文件，包含了模型的参数和状态。.pth文件保存了模型的权重和各层的参数，可以方便地用于加载和恢复模型。通过保存模型为.pth文件，我们可以在需要时重新加载模型，并使用它进行预测或继续训练。</strong></li>
</ul>
</blockquote>
<p>也就是说，pth是base，ckpt是进阶，safetensor是再进阶？知乎上<a href="https://zhuanlan.zhihu.com/p/686570419">这篇</a>解释safetensor还不错。<br>看了一些讲解，了解到，不同类型的模型、训练方法会自定义save和load的方法，文件名之类的也有差别，这种要具体情况具体分析。  </p>
<p>修改模型之后，重新训练，结果还是像以前那样，只能回答第一句话，后面的话无了。其原因在哪儿？需要print以下tensor的情况是否存在之类的。目前的猜想是：首先，不可能是训练数据只有单轮对话的原因，因为这无非是输入长短的问题，不应该是一点输出都没有。在多轮对话中，一点结果都不返回的话，说明第二轮及以后的decode都是&lt;!i’m end&gt;这种东西；这个问题的原因，可能在于魔改模型烂了，M和z对后续输出造成了很坏的负面应用，这个方法不通；也有可能是ft的方法、量级不对。亦或者，一个原因是M和z使用、更新的顺序出问题，导致后续decode无法进行之类的。<strong>因而，目前首先需要print所有decode的结果，而非对话</strong><br><img src="bug.png" alt="decode" title="decode"><br>输入之后，print出make_context中raw_context的结果，发现decode的结果是直接无了。思考这个现象出现的原因，应该是魔改模型+ft导致模型本身的能力被破坏殆尽；训练数据中都是单轮对话的数据，因而无法decode出多轮对话的结果，它输入进去的都只是一个system+user的对话，过拟合导致只能输出一轮对话的结果，后面直接decode成end了。<br>这个猜想的可能性是很大的。目前，不能确定自己魔改的代码在思路上是没有问题的，因而：  </p>
<blockquote>
<ul>
<li>可能需要再仔细斟酌一下代码修改，需要查看这个decode过程中的tensor(句子、M、z)；</li>
<li>也有可能代码没问题，纯因为这个方法需就是烂，水出来的；</li>
<li>也有可能是需要大量数据喂进去才能看到效果。 </li>
<li>也有可能，是safetensor向ckpt转化过程中损失了参数；因为似乎ckpt to safetensor是可以无损的，而反过来不行，因为safetensor是一种压缩、精简的格式；</li>
</ul>
</blockquote>
<p>查看M和z在forward过程中的数值更新：<br><img src="Mz.png" alt="Mz" title="Mz"><br>查看M和z的具体数字后，发现其过大，没收敛在一个固定范围内😡。但是、似乎、好像、Attn的结果，是M和z除；因而Attn不会受到太大影响才对？<br><strong>感觉脑子不够用了，一天能认真思考的时间好短捏。思考多了头晕。</strong>（tmd不是因为我节食了营养这一块儿没人给我补吧？）<br>好吧，确定一下思路：修正M和z的更新公式，使得其稳定，需要参考<a href="https://github.com/Beomi/InfiniTransformer/blob/main/infini_llama/modeling_infini_llama.py">另一个论文实现的开源项目</a>，这个项目适配了llama和Gemma，应该是好的。  </p>
<blockquote>
<ul>
<li><strong>明日任务：修正M和z的更新公式，力求将其限定在一定范围内波动，避免数值溢出。</strong></li>
</ul>
</blockquote>
<h1 id="5-9"><a href="#5-9" class="headerlink" title="5.9"></a>5.9</h1><p>查看上面说的开源项目llama版本的改动，发现其在初始化阶段只初始化了beta，且beta是一个和MHA的shape相匹配的tensor而非单独的数值。这样，每个头有一个参数，似乎更加符合论文中的做法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.gate = nn.Parameter(torch.full((<span class="number">1</span>, self.num_heads, <span class="number">1</span>, <span class="number">1</span>), <span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<p>modeling的代码中有past_kv，查看这个tensor是否存在、shape是否合理；按理来说这个应该是自增的。<br><img src="kvpast.png" alt="kvpast" title="kvpast"><br>确实是随着token的逐步decode递增的，第二个维度是query_len，输入+prompt大差不差是这个数。<br>又回顾了M和z的具体数值，发现<strong>主要是z过大</strong>，那么最简单的方法就是对z做norm，但是这个操作没出现在论文中，最好还是再分析一下代码是否有问题。<br>再去看一下论文：<br><img src="%E5%85%AC%E5%BC%8F.png" alt="公式" title="公式"><br>原来z只是归一化的分母，它就应该是大的，无所谓；但是M不应该是大的，这个问题出现的原因，可能是代码实现中没有关注 <strong>“element-wise”</strong> 这个操作<br>修改完之后，再次训练，预计下班后能训好。这次训练查看效果，还是重点关注两方面的内容：</p>
<blockquote>
<ul>
<li>对话是否正常</li>
<li>如果对话不正常，则查看M和z，尤其是M，需要控制其在一定范围内</li>
</ul>
</blockquote>
<p>M为什么会突发性的过大？这个M的公式上看，增量更新应该没问题，也许是代码写的不对？<br>查看代码，感觉没什么问题，再仔细看一会儿，真没问题那就没办法了。<br>训练中断，加载checkpoints-1000的参数看看能否多轮对话，似乎有多轮对话的能力；但是没有看M和z的具体数值。又改了代码。把beta改成了shape为[1, 1, self.num_heads, 1]的tensor，再次训练，看看效果。预计明天看看今天训的两个模型的效果和M、z的具体数值。  </p>
<blockquote>
<ul>
<li><strong>明日任务：查看两次修改的ft效果，明确是否是数据量不足的原因；思考限制M和z范围的方法；</strong></li>
</ul>
</blockquote>
<h1 id="5-10"><a href="#5-10" class="headerlink" title="5.10"></a>5.10</h1><p>昨晚跑一半服务器断了，今早发现连不上服务器了，ssh也不行。<br><strong>静观其变，相机而动！</strong><br>继续看一下streaming-llm的论文和源代码，这个项目的star涨的好快，现在已经有6.2k了，看样子很有可取之处。这个论文的核心思想还是sparse attention机制，只不过是在观察了attention logits的数值分布特征之后，提出的针对这个特征的sparse方案。<br><strong>另</strong>：win11上配置ssh server的<a href="https://www.sysgeek.cn/openssh-windows/">教程</a>，就是nm几mb的东西为什么安装的那么慢？而且还安装失败？————好吧，似乎是梯子的问题，把梯子关了之后正常安装了。和pip换清华源之后不能开梯子install有异曲同工之妙。  </p>
<p>streaming-llm对positio embedding也做了调整，感觉如果改qwen1的话会很麻烦，因为它的代码不明晰。</p>
<p>分析一下streaming-llm种modify_llama.py文件的思路：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_pos_emb_single</span>(<span class="params">x, cos, sin, position_ids</span>):</span><br><span class="line">    <span class="comment"># The first two dimensions of cos and sin are always 1, so we can `squeeze` them.</span></span><br><span class="line">    cos = cos.squeeze(<span class="number">1</span>).squeeze(<span class="number">0</span>)  <span class="comment"># [seq_len, dim]</span></span><br><span class="line">    sin = sin.squeeze(<span class="number">1</span>).squeeze(<span class="number">0</span>)  <span class="comment"># [seq_len, dim]</span></span><br><span class="line">    cos = cos[position_ids].unsqueeze(<span class="number">1</span>)  <span class="comment"># [bs, 1, seq_len, dim]</span></span><br><span class="line">    sin = sin[position_ids].unsqueeze(<span class="number">1</span>)  <span class="comment"># [bs, 1, seq_len, dim]</span></span><br><span class="line">    x_embed = (x * cos) + (rotate_half(x) * sin)</span><br><span class="line">    <span class="keyword">return</span> x_embed</span><br></pre></td></tr></table></figure>
<p>这个函数的作用，顾名思义是进行RoPE；而“single”的意义在于，指定位置进行RoPE，而不是一整个raw context。这个函数的作用正好和论文中的 <strong>“根据cache进行位置编码”</strong> 相对应。通过这个函数，可以自由选定、构建token组进行RoPE。<br>我也查看了qwen1中的RoPE，应该就是这样，没有问题。  </p>
<p>下午nm给拉出去干杂活，说明天早起去那儿打下手现在去踩点😅，挤在出租车上有点晕；然后nm到中关村会议中心那儿啥事儿没干，蹲了一会儿出去找饭吃，吃了31的大盘鸡拌面，味道一般，比不上南京一根；然后吃一半打电话来说不用再回去了直接回家😅😅😅😅😅，纯dirty work了属于是。实习生牛马忍气吞声ing。  </p>
<blockquote>
<ul>
<li><strong>明日任务：当杂活、群演、牛马😅😅</strong></li>
</ul>
</blockquote>
<h1 id="5-11"><a href="#5-11" class="headerlink" title="5.11"></a>5.11</h1><p>今日，在中关村会议中心当群演和迎宾。纯牛马。<br>中午包饭，食堂比公司的好，比较有意思的一点就是酸奶居然都是一样的，而且都是摞起来给你自取。可惜之前吃了个卷饼和豆浆，不太饿了。<br>开会议，无事发生。大佬讲的“低空经济”这个词儿有点意思。  </p>
<blockquote>
<ul>
<li><strong>下周任务：看ft效果；改模型</strong></li>
</ul>
</blockquote>
<h1 id="5-12"><a href="#5-12" class="headerlink" title="5.12"></a>5.12</h1><p><strong>再说一遍：我恨调休！我恨调休！</strong><br>放一天真的啥事儿也不想干，也不想出去玩、吃饭，纯在出租屋躺一天。<br>晚上和SH、LSH、LT一起大乱斗，今天赢得比较多。<br>ZZK从上海的滴滴实习跑到了北京的快手实习，5.10入的职。这几天和他聊了聊工作怎么样啥的。他是早上10:30，晚上9点还是10点来着。在西二旗那边。预期下周和他吃个饭，他本科就是北京的，也在百京实习过，因而游刃有余一点。<br>聊到WXF、ZW的实习，询问过后发现他们还在找。唉、后端是不好活，ZXQ也在学校躺尸没找到实习……  </p>
<h1 id="5-13"><a href="#5-13" class="headerlink" title="5.13"></a>5.13</h1><p><strong>再说一遍：我恨调休！我恨调休！</strong><br>早上测试了A800还连不上😅。<br>接下来继续分析一下modify_llama.py文件的函数作用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">llama_pos_shift_attention_forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">    attention_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    position_ids: <span class="type">Optional</span>[torch.LongTensor] = <span class="literal">None</span>, <span class="comment"># 这个参数应该是新增的</span></span></span><br><span class="line"><span class="params">    past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    use_cache: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Optional</span>[torch.Tensor], <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]]]:</span><br><span class="line">    bsz, q_len, _ = hidden_states.size()  <span class="comment">#获取batch_size和query_len</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.config.pretraining_tp &gt; <span class="number">1</span>:   <span class="comment"># 是否并行，tensor parallel</span></span><br><span class="line">        key_value_slicing = (</span><br><span class="line">            self.num_key_value_heads * self.head_dim</span><br><span class="line">        ) // self.config.pretraining_tp</span><br><span class="line">        query_slices = self.q_proj.weight.split(</span><br><span class="line">            (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line">        key_slices = self.k_proj.weight.split(key_value_slicing, dim=<span class="number">0</span>)</span><br><span class="line">        value_slices = self.v_proj.weight.split(key_value_slicing, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 对query、key、value进行切分，目的是并行计算； 计算tensor的slice大小，然后对各个切片进行proj，然后结果concat起来</span></span><br><span class="line">        query_states = [</span><br><span class="line">            F.linear(hidden_states, query_slices[i])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.pretraining_tp)</span><br><span class="line">        ]</span><br><span class="line">        query_states = torch.cat(query_states, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        key_states = [</span><br><span class="line">            F.linear(hidden_states, key_slices[i])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.pretraining_tp)</span><br><span class="line">        ]</span><br><span class="line">        key_states = torch.cat(key_states, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        value_states = [</span><br><span class="line">            F.linear(hidden_states, value_slices[i])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.pretraining_tp)</span><br><span class="line">        ]</span><br><span class="line">        value_states = torch.cat(value_states, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>: <span class="comment">#不并行 简单的proj一下即可</span></span><br><span class="line">        query_states = self.q_proj(hidden_states)</span><br><span class="line">        key_states = self.k_proj(hidden_states)</span><br><span class="line">        value_states = self.v_proj(hidden_states)</span><br><span class="line"></span><br><span class="line">    query_states = query_states.view(</span><br><span class="line">        bsz, q_len, self.num_heads, self.head_dim</span><br><span class="line">    ).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    key_states = key_states.view(</span><br><span class="line">        bsz, q_len, self.num_key_value_heads, self.head_dim</span><br><span class="line">    ).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    value_states = value_states.view(</span><br><span class="line">        bsz, q_len, self.num_key_value_heads, self.head_dim</span><br><span class="line">    ).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    kv_seq_len = key_states.shape[-<span class="number">2</span>]  <span class="comment"># key_states的seq_len，上面transpose一下之后，shape应该是[bsz, self.num_key_value_heads, q_len, self.head_dim]</span></span><br><span class="line">    <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        kv_seq_len += past_key_value[<span class="number">0</span>].shape[-<span class="number">2</span>]  <span class="comment"># 从这可以看出，past kv的shape应该也是[bsz, self.num_key_value_heads, q_len, self.head_dim]，或者至少倒数第二个是q_len？</span></span><br><span class="line">    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)  <span class="comment"># RoPEcos和sin参数</span></span><br><span class="line">    <span class="comment">### Shift Pos: query pos is min(cache_size, idx)</span></span><br><span class="line">    <span class="comment"># query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)</span></span><br><span class="line">    query_states = apply_rotary_pos_emb_single(query_states, cos, sin, position_ids)</span><br><span class="line">    <span class="comment">###  此处的shape应该没变</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># reuse k, v, self_attention</span></span><br><span class="line">        key_states = torch.cat([past_key_value[<span class="number">0</span>], key_states], dim=<span class="number">2</span>) </span><br><span class="line">        value_states = torch.cat([past_key_value[<span class="number">1</span>], value_states], dim=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 对past进行concat，结果应该得到q_len的states，因为当前的时间步只会处理当前的token，要得到完整的q_len还得要concat前面的cache。</span></span><br><span class="line">    past_key_value = (key_states, value_states) <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Shift Pos: key pos is the pos in cache</span></span><br><span class="line">    key_position_ids = torch.arange(kv_seq_len, device=position_ids.device).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    key_states = apply_rotary_pos_emb_single(key_states, cos, sin, key_position_ids)</span><br><span class="line">    <span class="comment">### 上面改的RoPE在此进行了应用</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># repeat k/v heads if n_kv_heads &lt; n_heads   GQA的做法，tensor shape是(batch, num_key_value_heads * n_rep, slen, head_dim)</span></span><br><span class="line">    key_states = repeat_kv(key_states, self.num_key_value_groups)</span><br><span class="line">    value_states = repeat_kv(value_states, self.num_key_value_groups)</span><br><span class="line"></span><br><span class="line">    attn_weights = torch.matmul(query_states, key_states.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(</span><br><span class="line">        self.head_dim</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):  <span class="comment"># 后两维的shape</span></span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;Attention weights should be of size <span class="subst">&#123;(bsz, self.num_heads, q_len, kv_seq_len)&#125;</span>, but is&quot;</span></span><br><span class="line">            <span class="string">f&quot; <span class="subst">&#123;attn_weights.size()&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> attention_mask.size() != (bsz, <span class="number">1</span>, q_len, kv_seq_len):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Attention mask should be of size <span class="subst">&#123;(bsz, <span class="number">1</span>, q_len, kv_seq_len)&#125;</span>, but is <span class="subst">&#123;attention_mask.size()&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line">        attn_weights = attn_weights + attention_mask</span><br><span class="line"></span><br><span class="line">    <span class="comment"># upcast attention to fp32</span></span><br><span class="line">    attn_weights = nn.functional.softmax(attn_weights, dim=-<span class="number">1</span>, dtype=torch.float32).to(</span><br><span class="line">        query_states.dtype</span><br><span class="line">    )</span><br><span class="line">    attn_output = torch.matmul(attn_weights, value_states) <span class="comment"># (batch, num_key_value_heads * n_rep, slen, head_dim) 应该也是这个shape</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;`attn_output` should be of size <span class="subst">&#123;(bsz, self.num_heads, q_len, self.head_dim)&#125;</span>, but is&quot;</span></span><br><span class="line">            <span class="string">f&quot; <span class="subst">&#123;attn_output.size()&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    attn_output = attn_output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.config.pretraining_tp &gt; <span class="number">1</span>: <span class="comment"># 如果并行，则切分后计算</span></span><br><span class="line">        attn_output = attn_output.split(</span><br><span class="line">            self.hidden_size // self.config.pretraining_tp, dim=<span class="number">2</span></span><br><span class="line">        )</span><br><span class="line">        o_proj_slices = self.o_proj.weight.split(</span><br><span class="line">            self.hidden_size // self.config.pretraining_tp, dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        attn_output = <span class="built_in">sum</span>(</span><br><span class="line">            [</span><br><span class="line">                F.linear(attn_output[i], o_proj_slices[i])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.pretraining_tp)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_output = self.o_proj(attn_output) <span class="comment"># 否则直接proj</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> output_attentions:</span><br><span class="line">        attn_weights = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> attn_output, attn_weights, past_key_value  <span class="comment"># 输出没啥好说的，标准输出罢了</span></span><br></pre></td></tr></table></figure>
<ul>
<li>这个函数的作用是，将前面的修改后的、带position_id的位置信息的RoPE应用到attention模块的forward上。从命名中可以看出这个用法。这个关键在于，对forward新增了一个position_ids参数，然后RoPE的函数重构一下，其他流程是和原生一样的。目前，没有在这个函数里面看见基于cache的RoPE，也没有看见sink attn的设置。</li>
</ul>
<p><strong>草！A800连不上的原因是安徽那边的看机器的人用root账户把东西搞烂了，现在正在修，环境啥的都没了，都要重新配置，真NM无语了，有这种事啊？</strong>  </p>
<p>在分析上面代码的过程中，突然对自己之前修改的model产生了质疑：tensor的shape是否正确？我需要查看原生的output tensor shape和修改后的output tensor shape是否一致。此外，还需要输出一下past_key_value的shape，<del>这我之前咋没想到看呢？也许是model_qwen里面没有?</del>好吧，之前看过，但是时间太长我给忘了，嘻🤗🤗。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">1</span>, <span class="number">407</span>, <span class="number">16</span>, <span class="number">128</span>])  <span class="comment"># past_key_value[0][0].shape  这个也许是k的shape，如果是[1][0]则是v的shape？</span></span><br></pre></td></tr></table></figure>
<p>而原生model，print attn_output的结果是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attn_output.shape:torch.Size([<span class="number">1</span>, <span class="number">404</span>, <span class="number">2048</span>])</span><br><span class="line">attn_output.shape:torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2048</span>])</span><br></pre></td></tr></table></figure>
<p>前面q_len一直是404，后面一直是1。但是这个没分开num_heads，捏妈我改的代码不会就是因为这个所以效果不好的吧？<br>赶紧查看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attn_output before c_proj.shape:torch.Size([<span class="number">1</span>, <span class="number">419</span>, <span class="number">16</span>, <span class="number">128</span>])</span><br></pre></td></tr></table></figure>
<p>虚惊一场，代码里面attn一直是这个形状，中间的merge后的形状是context_layer这个中间参数；modeling代码里面的shape也没问题。</p>
<p>modify_llama.py三个函数中的最后一个函数是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">enable_llama_pos_shift_attention</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> <span class="built_in">reversed</span>(model._modules.items()):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>(module.children())) &gt; <span class="number">0</span>:</span><br><span class="line">            enable_llama_pos_shift_attention(</span><br><span class="line">                module,</span><br><span class="line">            ) <span class="comment"># 一个递归，似乎是为了处理一个网络中的所有层；transformer的网络结构大抵是nn.module及其嵌套。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, LlamaAttention):</span><br><span class="line">            model._modules[name].forward = types.MethodType(</span><br><span class="line">                llama_pos_shift_attention_forward, model._modules[name]</span><br><span class="line">            ) <span class="comment"># 找到网络中的LlamaAttention，替换forward函数</span></span><br></pre></td></tr></table></figure>
<p>这个函数的作用很简单。  </p>
<p>看下来之后，感觉论文中提到的sink和基于cache的RoPE似乎没有实现？此外，softmax的修改也没体现。  </p>
<p>服务器上面的代码失去了，我这里没有备份；幸好一些关键代码不多，我还记得：  </p>
<ul>
<li><p>safetensor逐个转化为ckpt文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> safetensors.torch <span class="keyword">import</span> load_file</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line">DEFAULT_CKPT_PATH = <span class="string">&#x27;Qwen-7B-Chat&#x27;</span></span><br><span class="line">CUDA_VISIBLE_DEVICES = <span class="string">&#x27;2,3,4,5,6,7&#x27;</span> <span class="comment"># 0、1卡已经部署了研究院那边sft好的模型的api</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line">file_paths = glob.glob(os.path.join(DEFAULT_CKPT_PATH, <span class="string">&#x27;/*.safetensors&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> file_paths:</span><br><span class="line">    weights = load_file(path, device=device)</span><br><span class="line">    weights[<span class="string">&quot;state_dict&quot;</span>] = weights</span><br><span class="line">    torch.save(weights, os.path.splitext(path)[<span class="number">0</span>] + <span class="string">&#x27;.ckpt&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>构建训练数据make_train_data.py：<br><strong>此代码在四月份的记录中已存，具体是4.28的记录</strong></p>
</li>
<li><p>修改cli_demo.py中的加载模型部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig</span><br><span class="line"><span class="keyword">from</span> safetensors.torch <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">config = AutoConfig.from_pretrained(<span class="string">&quot;/home/zhangshuo/Qwen-main/Qwen-7B-Chat-Mem&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_config(config)</span><br><span class="line"></span><br><span class="line">DEFAULT_CKPT_PATH = <span class="string">&#x27;Qwen-7B-Chat-Mem&#x27;</span></span><br><span class="line">file_paths = glob.glob(os.path.join(DEFAULT_CKPT_PATH, <span class="string">&#x27;/*.ckpt&#x27;</span>))</span><br><span class="line">CUDA_VISIBLE_DEVICES = <span class="string">&#x27;2,3,4,5,6,7&#x27;</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> file_paths:</span><br><span class="line">    model.load_state_dict(torch.load(path), strict=<span class="literal">False</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><strong>明日任务：阅读kv_cache.py，吃透总体上的代码逻辑</strong></li>
</ul>
</blockquote>
</li>
</ul>
<h1 id="5-14"><a href="#5-14" class="headerlink" title="5.14"></a>5.14</h1><p>昨天夜里领导突然告诉我，让我到昌平区的那个地方的公司去帮忙，说那边任务紧急。<br>然后就过去了，早上坐了一个多小时的地铁，9点才将将赶上到那个园区里面，早饭也没吃。到那儿，在找中心B座的时候遇见做数据的实习兄弟S，原来他也被派来了。俺们在园区里面迷了一阵子路，最后找到了。<br>到那儿，给我们安排到一楼的工位，似乎是因为公司部门在三楼的工位不够了。说是有下载huggingface数据的任务，但是一楼工位又给我们申请了工作账号，用来派发数据校验审查的任务，通俗来说就是打赛博螺丝😅😅。这边的人倒是挺好的，给我们讲解怎么做，然后时不时来问问有没有遇到问题，但是跑大老远的紧急任务就是打赛博螺丝真是让人无大语。<br>中午，也不知道有啥地方吃饭，公司在这边带我们的F大哥的话，我和S兄跟着人流，到了园区外面的饭馆。哪儿是一排面馆的聚集地，但是也卖盖浇饭什么的，总之是接地气的快餐。人很多，我点了辣子鸡丁盖浇饭，等了好久才吃上，味道和分量都不错。价格也比较便宜，比宣武门公司和出租屋那边都便宜，而且是现炒的，有锅气。<br>吃完饭，买了瓶饮料，回去工位，勉勉强强睡了一阵子。下午继续打螺丝。临到下班问F大哥，数据集下载的硬盘在哪儿，大哥说明天给😅😅，合着今天来真是就打赛博螺丝啊！<br>哎！宣武门那边工位上的键盘、被子、药、手机充电器都没带来！<br>周边的绿化倒是挺好的，楼顶很高、楼很大、给人空旷开阔的感觉，毕竟是郊区。看了看周围的房价，发现自如上的不便宜。然后NJ找的58同城上的巨便宜，还是单间！不贵！比现在出租屋那边条件好太多了！我草tmd自如！！！如果要呆在这边时间长，那肯定是要搬家的。但是一切未定。问到底在这干多久，领导也说不确定😅😅😅😅😅😅。实习生牛马呗！<br>快下班的时候告诉我明天回宣武门那边聊一下到底怎么做、后续做啥、是在哪边的公司😅😅。真是折腾人！不能今天正常上班跟我聊吗！<br>怎么说呢，今天的经历，给了我很强的跑路理由，之前顾虑到沈老师师姐这边，总想着忍一下吧，忍忍过去了！但是今天发现，正常的研究都不打算让我做了，国企这边管理混乱、没有主线。那我跑起来是心安理得，心情反倒感到轻松了不少🤗🤗。<br>琢磨琢磨后面能投哪儿的公司捏~~反正不想待百京了</p>
<blockquote>
<ul>
<li><strong>明日任务：寄！开摆喽！</strong></li>
</ul>
</blockquote>
<h1 id="5-15"><a href="#5-15" class="headerlink" title="5.15"></a>5.15</h1><p>关于sink attn和cache RoPE的部分，实现似乎在kv_cache.py中，这里记录并分析一下这个代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slice2d</span>(<span class="params">x, start, end</span>):</span><br><span class="line">    <span class="keyword">return</span> x[:, :, start:end, ...]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slice3d</span>(<span class="params">x, start, end</span>):</span><br><span class="line">    <span class="keyword">return</span> x[:, :, :, start:end, ...]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slice1d</span>(<span class="params">x, start, end</span>):</span><br><span class="line">    <span class="keyword">return</span> x[:, start:end, ...]</span><br><span class="line"><span class="comment"># 这三个是对tensor的某个维度进行cut，似乎就是论文中对attn实行sink+window中window的部分</span></span><br><span class="line"></span><br><span class="line">DIM_TO_SLICE = &#123;</span><br><span class="line">    <span class="number">1</span>: slice1d,</span><br><span class="line">    <span class="number">2</span>: slice2d,</span><br><span class="line">    <span class="number">3</span>: slice3d,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StartRecentKVCache</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        start_size=<span class="number">4</span>,  <span class="comment"># sink大小 4 + 508构成总window，论文里是这样写的，但代码实现好像是4+512=516？</span></span></span><br><span class="line"><span class="params">        recent_size=<span class="number">512</span>, <span class="comment"># window大小</span></span></span><br><span class="line"><span class="params">        k_seq_dim=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        v_seq_dim=<span class="number">2</span>, <span class="comment"># tensor的shape应该是[batch, num_heads, seq_len, head_dim]</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;StartRecentKVCache: <span class="subst">&#123;start_size&#125;</span>, <span class="subst">&#123;recent_size&#125;</span>&quot;</span>)</span><br><span class="line">        self.start_size = start_size</span><br><span class="line">        self.recent_size = recent_size</span><br><span class="line">        self.cache_size = start_size + recent_size</span><br><span class="line">        self.k_seq_dim = k_seq_dim</span><br><span class="line">        self.v_seq_dim = v_seq_dim</span><br><span class="line">        self.k_slice = DIM_TO_SLICE[k_seq_dim] <span class="comment"># 结合上面定义的函数和dict，这里确定seq所在的dim，从而选好slice函数</span></span><br><span class="line">        self.v_slice = DIM_TO_SLICE[v_seq_dim]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, past_key_values</span>):</span><br><span class="line">        <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        seq_len = past_key_values[<span class="number">0</span>][<span class="number">0</span>].size(self.k_seq_dim)  <span class="comment"># 这行代码的作用是计算past_key_values中第一个元素的第一个子tensor的大小，并将结果存储在seq_len变量中。具体地说，它使用size方法来获取特定维度上的尺寸。例如，假设past_key_values[0][0]是一个形状为(3, 4, 5)的3D张量，且k_seq_dim的值为2，则该代码将计算第2个维度（从0开始计数）上的尺寸，即5，并将结果存储在seq_len变量中。</span></span><br><span class="line">        <span class="keyword">if</span> seq_len &lt;= self.cache_size: <span class="comment"># 如果目前的seq长度小于等于我们预先定义的sink+window的总大小，则不需要对past_key_values进行任何处理，直接返回。</span></span><br><span class="line">            <span class="keyword">return</span> past_key_values</span><br><span class="line">        <span class="keyword">return</span> [ <span class="comment"># 否则，我们需要对past_key_values进行处理，每个seq维度对应的kv都剪切sink+window的长度，其余都删去，最后concat起来</span></span><br><span class="line">            [</span><br><span class="line">                torch.cat(</span><br><span class="line">                    [</span><br><span class="line">                        self.k_slice(k, <span class="number">0</span>, self.start_size),</span><br><span class="line">                        self.k_slice(k, seq_len - self.recent_size, seq_len),</span><br><span class="line">                    ],</span><br><span class="line">                    dim=self.k_seq_dim,</span><br><span class="line">                ),</span><br><span class="line">                torch.cat(</span><br><span class="line">                    [</span><br><span class="line">                        self.v_slice(v, <span class="number">0</span>, self.start_size),</span><br><span class="line">                        self.v_slice(v, seq_len - self.recent_size, seq_len),</span><br><span class="line">                    ],</span><br><span class="line">                    dim=self.v_seq_dim,</span><br><span class="line">                ),</span><br><span class="line">            ]</span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> past_key_values</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evict_for_space</span>(<span class="params">self, past_key_values, num_coming</span>):  <span class="comment"># 逐出_for_space</span></span><br><span class="line">        <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        seq_len = past_key_values[<span class="number">0</span>][<span class="number">0</span>].size(self.k_seq_dim)</span><br><span class="line">        <span class="keyword">if</span> seq_len + num_coming &lt;= self.cache_size: <span class="comment"># 如果seq + 预备加入的token数量 小于等于cache大小</span></span><br><span class="line">            <span class="keyword">return</span> past_key_values</span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            [</span><br><span class="line">                torch.cat(</span><br><span class="line">                    [</span><br><span class="line">                        self.k_slice(k, <span class="number">0</span>, self.start_size),</span><br><span class="line">                        self.k_slice(</span><br><span class="line">                            k, seq0._<span class="built_in">len</span> - self.recent_size + num_coming, seq_len  <span class="comment"># 和上面的__call__相比，不同在于 + num_coming</span></span><br><span class="line">                        ),</span><br><span class="line">                    ],</span><br><span class="line">                    dim=self.k_seq_dim,</span><br><span class="line">                ),</span><br><span class="line">                torch.cat(</span><br><span class="line">                    [</span><br><span class="line">                        self.v_slice(v, <span class="number">0</span>, self.start_size),</span><br><span class="line">                        self.v_slice(</span><br><span class="line">                            v, seq_len - self.recent_size + num_coming, seq_len <span class="comment"># 同上</span></span><br><span class="line">                        ),</span><br><span class="line">                    ],</span><br><span class="line">                    dim=self.v_seq_dim,</span><br><span class="line">                ),</span><br><span class="line">            ]</span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> past_key_values</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evict_range</span>(<span class="params">self, past_key_values, start, end</span>):</span><br><span class="line">        <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        seq_len = past_key_values[<span class="number">0</span>][<span class="number">0</span>].size(self.k_seq_dim) <span class="comment"># 这一步三个函数都是一样的。</span></span><br><span class="line">        <span class="keyword">assert</span> start &lt;= end <span class="keyword">and</span> end &lt;= seq_len</span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            [</span><br><span class="line">                torch.cat(</span><br><span class="line">                    [</span><br><span class="line">                        self.k_slice(k, <span class="number">0</span>, start),</span><br><span class="line">                        self.k_slice(k, end, seq_len), <span class="comment"># 这个方法应该是自定义从哪儿到哪儿的方法。</span></span><br><span class="line">                    ],</span><br><span class="line">                    dim=self.k_seq_dim,</span><br><span class="line">                ),</span><br><span class="line">                torch.cat(</span><br><span class="line">                    [</span><br><span class="line">                        self.v_slice(v, <span class="number">0</span>, start),</span><br><span class="line">                        self.v_slice(v, end, seq_len),</span><br><span class="line">                    ],</span><br><span class="line">                    dim=self.v_seq_dim,</span><br><span class="line">                ),</span><br><span class="line">            ]</span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> past_key_values</span><br><span class="line">        ]</span><br></pre></td></tr></table></figure>
<p>阅读完毕之后，发现还是有点搞不懂他这个代码是怎么运行的，还得看一下它这个运行的run_streaming_llama.py的代码。需要明晰的是，这个总体上的思路应该是：写class、method满足基于cache进行RoPE的需求、动态维护一个sink + window的cache需求，然后在llama模型中运用这个方法，在生成的forward过程中，替换成sink + window</p>
<p>查看一下官方实现的以streaming形式运行llama的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> streaming_llm.utils <span class="keyword">import</span> load, download_url, load_jsonl</span><br><span class="line"><span class="keyword">from</span> streaming_llm.enable_streaming_llm <span class="keyword">import</span> enable_streaming_llm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_generate</span>(<span class="params">model, tokenizer, input_ids, past_key_values, max_gen_len</span>): <span class="comment"># qwen里面的QwenLMHeadModel里面有generate函数，llama里面没有，这个greedy_generate应该是类似qwen里面的功能？</span></span><br><span class="line">    outputs = model(    <span class="comment"># 这个关键应该是在input_ids这个参数上？通过这个参数来进行..？</span></span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        past_key_values=past_key_values,</span><br><span class="line">        use_cache=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    past_key_values = outputs.past_key_values</span><br><span class="line">    pred_token_idx = outputs.logits[:, -<span class="number">1</span>, :].argmax(dim=-<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>这里我去看了一下qwen里面的logits的相关数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(lm_logits)</span><br><span class="line"><span class="built_in">print</span>(lm_logits.shape)</span><br><span class="line"><span class="built_in">print</span>(lm_logits[..., :-<span class="number">1</span>, :])</span><br><span class="line"><span class="built_in">print</span>(lm_logits[..., :-<span class="number">1</span>, :].shape)</span><br><span class="line">tensor([[[  <span class="number">7.6367</span>,   <span class="number">4.5312</span>,   <span class="number">4.6406</span>,  ...,  -<span class="number">2.9141</span>,  -<span class="number">2.9141</span>,  -<span class="number">2.9141</span>],</span><br><span class="line">        [  <span class="number">7.6797</span>,   <span class="number">4.5391</span>,   <span class="number">4.5664</span>,  ...,  -<span class="number">2.9512</span>,  -<span class="number">2.9512</span>,  -<span class="number">2.9512</span>],</span><br><span class="line">        [ -<span class="number">5.9531</span>, -<span class="number">11.9766</span>,  -<span class="number">8.3281</span>,  ...,  -<span class="number">1.5859</span>,  -<span class="number">1.5869</span>,  -<span class="number">1.5869</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [ -<span class="number">4.7500</span>,  -<span class="number">3.5156</span>,  -<span class="number">4.6289</span>,  ...,  -<span class="number">2.9043</span>,  -<span class="number">2.9043</span>,  -<span class="number">2.9062</span>],</span><br><span class="line">        [  <span class="number">3.0527</span>,   <span class="number">6.7930</span>,  -<span class="number">0.4011</span>,  ...,  -<span class="number">3.6289</span>,  -<span class="number">3.6289</span>,  -<span class="number">3.6289</span>],</span><br><span class="line">        [  <span class="number">3.1152</span>,   <span class="number">7.9453</span>,   <span class="number">0.1495</span>,  ...,  -<span class="number">2.8750</span>,  -<span class="number">2.8750</span>,  -<span class="number">2.8750</span>]]],</span><br><span class="line">    device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">404</span>, <span class="number">151936</span>])</span><br><span class="line"></span><br><span class="line">tensor([[[  <span class="number">7.6367</span>,   <span class="number">4.5312</span>,   <span class="number">4.6406</span>,  ...,  -<span class="number">2.9141</span>,  -<span class="number">2.9141</span>,  -<span class="number">2.9141</span>],</span><br><span class="line">        [  <span class="number">7.6797</span>,   <span class="number">4.5391</span>,   <span class="number">4.5664</span>,  ...,  -<span class="number">2.9512</span>,  -<span class="number">2.9512</span>,  -<span class="number">2.9512</span>],</span><br><span class="line">        [ -<span class="number">5.9531</span>, -<span class="number">11.9766</span>,  -<span class="number">8.3281</span>,  ...,  -<span class="number">1.5859</span>,  -<span class="number">1.5869</span>,  -<span class="number">1.5869</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [ -<span class="number">7.8203</span>,  -<span class="number">4.4961</span>,  -<span class="number">4.9688</span>,  ...,  -<span class="number">0.6299</span>,  -<span class="number">0.6323</span>,  -<span class="number">0.6323</span>],</span><br><span class="line">        [ -<span class="number">4.7500</span>,  -<span class="number">3.5156</span>,  -<span class="number">4.6289</span>,  ...,  -<span class="number">2.9043</span>,  -<span class="number">2.9043</span>,  -<span class="number">2.9062</span>],</span><br><span class="line">        [  <span class="number">3.0527</span>,   <span class="number">6.7930</span>,  -<span class="number">0.4011</span>,  ...,  -<span class="number">3.6289</span>,  -<span class="number">3.6289</span>,  -<span class="number">3.6289</span>]]],</span><br><span class="line">    device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">403</span>, <span class="number">151936</span>])</span><br><span class="line"></span><br><span class="line">tensor([[[ <span class="number">1.8477</span>,  <span class="number">1.7461</span>,  <span class="number">1.9453</span>,  ..., -<span class="number">2.3418</span>, -<span class="number">2.3418</span>, -<span class="number">2.3418</span>]]],</span><br><span class="line">    device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.float16)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">151936</span>])</span><br><span class="line"></span><br><span class="line">tensor([], device=<span class="string">&#x27;cuda:0&#x27;</span>, size=(<span class="number">1</span>, <span class="number">0</span>, <span class="number">151936</span>), dtype=torch.float16)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">0</span>, <span class="number">151936</span>])</span><br></pre></td></tr></table></figure>
<p>一开始seq_len维度的是query的长度，然后[…, :-1, :]这个操作正如之前所说的，减了一个维度，只把最后一个时间步作为预测结果。<br>在第二个时间步之后，可以看到print的结果，seq这个维度是1，削减之后是0；之后就一直是[1][0]循环了。[…, :-1, :]就纯没有结果。<br>那么，也就是说，内部这个lm_logits在第一步输入的时候会进行一个整的计算，然后到了生成token的时候，这个lm_logits就是一个单词表长度的概率分布了，直接从这个概率softmax再decode啥的出一个token。<br>那么pred_token_idx就是根据logits而decode出来的token了。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">generated_ids = [pred_token_idx.item()]   <span class="comment"># 这里取出预测出来的id</span></span><br><span class="line">pos = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_gen_len - <span class="number">1</span>):  <span class="comment"># 上面那个似乎是单独预测第一个token？这里预测后面的token直到到达模型设置的max_gen_len为止。为啥要分开呢？难道是为了初始化list？那为啥不用空list呢？</span></span><br><span class="line">    outputs = model(</span><br><span class="line">        input_ids=pred_token_idx,</span><br><span class="line">        past_key_values=past_key_values,</span><br><span class="line">        use_cache=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    past_key_values = outputs.past_key_values</span><br><span class="line">    pred_token_idx = outputs.logits[:, -<span class="number">1</span>, :].argmax(dim=-<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    generated_ids.append(pred_token_idx.item())  <span class="comment"># 新decode出来的token对应的id加入了</span></span><br><span class="line">    generated_text = (  </span><br><span class="line">        tokenizer.decode( <span class="comment"># decode成token， id -&gt; token </span></span><br><span class="line">            generated_ids,</span><br><span class="line">            skip_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br><span class="line">            spaces_between_special_tokens=<span class="literal">False</span>,</span><br><span class="line">        )</span><br><span class="line">        .strip()</span><br><span class="line">        .split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    now = <span class="built_in">len</span>(generated_text) - <span class="number">1</span> <span class="comment"># 好吧，单独生成第一个token是为了这一步的循环</span></span><br><span class="line">    <span class="keyword">if</span> now &gt; pos:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot; &quot;</span>.join(generated_text[pos:now]), end=<span class="string">&quot; &quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        pos = now</span><br><span class="line">    <span class="comment"># 预测出终止符</span></span><br><span class="line">    <span class="keyword">if</span> pred_token_idx == tokenizer.eos_token_id:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>.join(generated_text[pos:]), flush=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">return</span> past_key_values  <span class="comment"># 下面那个method需要这个返回值。迭代的kv更新也需要past_key_values这个参数。</span></span><br></pre></td></tr></table></figure>
<p>总的来说，这个method是一个获取model输出概率并进行greedy decode的过程。没有看见之前改的kv cache在此有应用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">streaming_inference</span>(<span class="params">model, tokenizer, prompts, kv_cache=<span class="literal">None</span>, max_gen_len=<span class="number">1000</span></span>):</span><br><span class="line">    past_key_values = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> idx, prompt <span class="keyword">in</span> <span class="built_in">enumerate</span>(prompts): <span class="comment"># 迭代式单轮对话？</span></span><br><span class="line">        prompt = <span class="string">&quot;USER: &quot;</span> + prompt + <span class="string">&quot;\n\nASSISTANT: &quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span> + prompt, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        input_ids = tokenizer(prompt, return_tensors=<span class="string">&quot;pt&quot;</span>).input_ids <span class="comment"># 通过tokenizer把prompt转成pt（pytorch）形式的tensor，然后从中提取input_ids</span></span><br><span class="line">        input_ids = input_ids.to(model.device)</span><br><span class="line">        seq_len = input_ids.shape[<span class="number">1</span>] <span class="comment"># 第二个维度是seq_len</span></span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 如果存在kv cache，即不是第一个token</span></span><br><span class="line">            space_needed = seq_len + max_gen_len</span><br><span class="line">            past_key_values = kv_cache.evict_for_space(past_key_values, space_needed) <span class="comment"># 在这里实现past_key_values的sink + window的cache的维护。</span></span><br><span class="line"></span><br><span class="line">        past_key_values = greedy_generate( <span class="comment"># 调用上面那个method进行greedy decoding</span></span><br><span class="line">            model, tokenizer, input_ids, past_key_values, max_gen_len=max_gen_len</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model_name_or_path = args.model_name_or_path</span><br><span class="line">    model, tokenizer = load(model_name_or_path)</span><br><span class="line">    test_filepath = os.path.join(args.data_root, <span class="string">&quot;mt_bench.jsonl&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading data from <span class="subst">&#123;test_filepath&#125;</span> ...&quot;</span>)</span><br><span class="line">    <span class="comment"># 导入模型和数据</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_filepath):</span><br><span class="line">        download_url(</span><br><span class="line">            <span class="string">&quot;https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl&quot;</span>,</span><br><span class="line">            args.data_root,</span><br><span class="line">        )</span><br><span class="line">        os.rename(os.path.join(args.data_root, <span class="string">&quot;question.jsonl&quot;</span>), test_filepath)</span><br><span class="line"></span><br><span class="line">    list_data = load_jsonl(test_filepath)</span><br><span class="line">    prompts = []</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> list_data:</span><br><span class="line">        prompts += sample[<span class="string">&quot;turns&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.enable_streaming:</span><br><span class="line">        kv_cache = enable_streaming_llm(</span><br><span class="line">            model, start_size=args.start_size, recent_size=args.recent_size</span><br><span class="line">        )<span class="comment"># 这个method的作用大概是：替换llama中的RoPE以及forward部分，从而可以实现sink + windows，以及cache RoPE的功能，然后会返回一个应用了上面那些修改的kv cache，即sink + window</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kv_cache = <span class="literal">None</span> <span class="comment"># 如果不启用streaming，为啥kv_cache设置为none？原因在于，单轮对话不需要cache，如果是多轮，则有history参数传入，作为query加到我们当前轮的对话中，也就是说kv cache没必要存。  </span></span><br><span class="line">        <span class="comment"># 但是，在streaming的情况下，似乎可以把max_gen_len设置成很大，一直无限地对话下去。</span></span><br><span class="line"></span><br><span class="line">    streaming_inference(</span><br><span class="line">        model,</span><br><span class="line">        tokenizer,</span><br><span class="line">        prompts,</span><br><span class="line">        kv_cache,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--model_name_or_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;lmsys/vicuna-13b-v1.3&quot;</span></span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--data_root&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;data/&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--enable_streaming&quot;</span>, action=<span class="string">&quot;store_true&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--start_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--recent_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">2000</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<p>实现streaming-llm，主要是对模型的kv_cache及其forward机制进行修改，对model的forward也会进行修改；此外，在generate上也需要进行相应的调整。<br>而像是qwen中的cli_demo.py，其对话的print之类的东西也要调整，总的来说感觉这个调整是全面的，而非单一改算法。  </p>
<p><strong>那么，项目全部阅读完毕之后，确定一下modeling_qwen修改的思路：总的来说，各种文件大抵都可以结合modeling_qwen.py的源文件相关部分进行一些调整；greedy_generate这个修改，在qwen中应该是chat&#x2F;chat_stream，而非generate</strong>  </p>
<p>中午，和Z哥一起去吃沙县小吃，颇有感慨。想起自己初中的时候，很喜欢吃小区门口那家沙县小吃的肉丝饭，11块，经常去吃，有时候家里做了饭，但馋那一口，也会去吃；记得有一次，我已经过世的奶奶做好了饭叫我吃，我说不吃，去买沙县小吃吃，买回来她看到，说“就这个东西啊”。哈哈，她也没有逼我吃家里的饭。倒是现在，想吃一口，终究也是吃不上了。<br>上了高中，由于学业压力大，加之没啥钱，有钱会去上网吧，似乎就再也没吃过那家沙县。大学再去，发现已经关门了，吃其他沙县也找回不了记忆里的感觉。想来，沙县或许是我初中生活的一个缩影，有和朋友在沙县吃的记忆、有坐在电脑前打游戏吃沙县的莫名其妙的很舒适的感觉。然而一切都已过去，我也找不回那种懵懂、纯粹快乐的感觉。   </p>
<p>晚上，线上会议，纯看客。开完和LT，NZY和她对象打大乱斗。捏妈，向N开炮似乎给人姑娘吓着了，后面麦都没啥声音😨😨。</p>
<blockquote>
<ul>
<li><strong>明日任务：将streaming-llm实现到qwen中  or  接手今早才跟我说的多路召回检索优化项目</strong></li>
</ul>
</blockquote>
<h1 id="此处记录streaming-llm应用到qwen上的一些具体修改"><a href="#此处记录streaming-llm应用到qwen上的一些具体修改" class="headerlink" title="此处记录streaming-llm应用到qwen上的一些具体修改"></a>此处记录streaming-llm应用到qwen上的一些具体修改</h1><blockquote>
<p><strong>enable_streaming_llm.py</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在elif新增：</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&quot;qwen&quot;</span> <span class="keyword">in</span> model.config.model_type: <span class="comment"># qwen结构和llama类似，应该copy llama的设置就行。</span></span><br><span class="line">        k_seq_dim = v_seq_dim = <span class="number">1</span></span><br><span class="line">        <span class="keyword">from</span> .pos_shift.modify_falcon <span class="keyword">import</span> (</span><br><span class="line">            enable_falcon_pos_shift_attention,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        enable_falcon_pos_shift_attention(model)</span><br><span class="line"><span class="comment"># qwen结构和llama类似，但是llama中 attn_output.size的shape是(bsz, self.num_heads, q_len, self.head_dim)</span></span><br><span class="line"><span class="comment"># 而正如之前所提到的，qwen里面应该是(bsz, q_len, self.num_heads, self.head_dim)，因而seq_dim应该是1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此外，根据目录，from streaming_llm.kv_cache import StartRecentKVCache 这种或许要改成 from .kv_cache import StartRecentKVCache</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>kv_cache.py + utils.py</strong>  </p>
</blockquote>
<p>似乎不需要改动；kv_cache.py是感觉没有需要改的；utils.py是下载、加载数据的class，暂时没打算用这个。而且应该可以照搬过来用</p>
<blockquote>
<p><strong>pos_shift&#x2F;modify_llama.py</strong>  </p>
</blockquote>
<p>这个文件夹下面，针对每个model文件都写了一个modify_xxx.py，那么qwen需要在这下面写个文件modify_qwen.py，应该是在llama的基础上进行修改即可。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 尚未做完，qwen1的model文件太混乱了，有很多多此一举的举动，有很多操作直接使用pytorch的原生方法就可以直接完成，它非要定义一个函数；传参什么的跟llama很不相同；而qwen1.5的model文件就和llama很近似。  </span></span><br><span class="line"><span class="comment"># 暂时没时间改了，工作内容有变。效果尚未验证。</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..modeling_qwen <span class="keyword">import</span> (</span><br><span class="line">    QWenAttention,</span><br><span class="line">    _rotate_half,</span><br><span class="line">    apply_rotary_pos_emb,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> types</span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">&quot;enable_qwen_pos_shift_attention&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_pos_emb_single</span>(<span class="params">x, cos, sin, position_ids</span>):</span><br><span class="line">    <span class="comment"># The first two dimensions of cos and sin are always 1, so we can `squeeze` them.</span></span><br><span class="line">    cos = cos.squeeze(<span class="number">1</span>).squeeze(<span class="number">0</span>)  <span class="comment"># [seq_len, dim]</span></span><br><span class="line">    sin = sin.squeeze(<span class="number">1</span>).squeeze(<span class="number">0</span>)  <span class="comment"># [seq_len, dim]</span></span><br><span class="line">    cos = cos[position_ids].unsqueeze(<span class="number">1</span>)  <span class="comment"># [bs, 1, seq_len, dim]</span></span><br><span class="line">    sin = sin[position_ids].unsqueeze(<span class="number">1</span>)  <span class="comment"># [bs, 1, seq_len, dim]</span></span><br><span class="line">    x_embed = (x * cos) + (_rotate_half(x) * sin)</span><br><span class="line">    <span class="keyword">return</span> x_embed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">qwen_pos_shift_attention_forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    hidden_states: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.FloatTensor]],</span></span><br><span class="line"><span class="params">    rotary_pos_emb_list: <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">List</span>[torch.Tensor]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    layer_past: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    use_cache: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Optional</span>[torch.Tensor], <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]]]:</span><br><span class="line">    bsz, q_len, _ = hidden_states.size()</span><br><span class="line"></span><br><span class="line">    mixed_x_layer = self.c_attn(hidden_states)</span><br><span class="line"></span><br><span class="line">    query, key, value = mixed_x_layer.split(self.split_size, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class="line">    key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class="line">    value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line">    past_len = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        past_len = layer_past[<span class="number">0</span>].shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="keyword">if</span> rotary_pos_emb_list <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># cur_len = query.shape[1]</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(rotary_pos_emb_list) == <span class="number">1</span>:</span><br><span class="line">            rotary_pos_emb = rotary_pos_emb_list[<span class="number">0</span>]</span><br><span class="line">            rotary_pos_emb = [i[:, -past_len:, :, :] <span class="keyword">for</span> i <span class="keyword">in</span> rotary_pos_emb]</span><br><span class="line">            rotary_pos_emb = (rotary_pos_emb,) * <span class="number">2</span></span><br><span class="line">            q_pos_emb, k_pos_emb = rotary_pos_emb</span><br><span class="line">            <span class="comment"># Slice the pos emb for current inference</span></span><br><span class="line">            query = apply_rotary_pos_emb(query, q_pos_emb)</span><br><span class="line">            key = apply_rotary_pos_emb(key, k_pos_emb)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            query_list = []</span><br><span class="line">            key_list = []</span><br><span class="line">            <span class="keyword">for</span> i, rotary_pos_emb <span class="keyword">in</span> <span class="built_in">enumerate</span>(rotary_pos_emb_list):</span><br><span class="line">                rotary_pos_emb = [i[:, -past_len:, :, :] <span class="keyword">for</span> i <span class="keyword">in</span> rotary_pos_emb]</span><br><span class="line">                rotary_pos_emb = (rotary_pos_emb,) * <span class="number">2</span></span><br><span class="line">                q_pos_emb, k_pos_emb = rotary_pos_emb</span><br><span class="line">                <span class="comment"># Slice the pos emb for current inference</span></span><br><span class="line">                query_list += [apply_rotary_pos_emb(query[i:i+<span class="number">1</span>, :, :], q_pos_emb)]</span><br><span class="line">                key_list += [apply_rotary_pos_emb(key[i:i+<span class="number">1</span>, :, :], k_pos_emb)]</span><br><span class="line">            query = torch.cat(query_list, dim=<span class="number">0</span>)</span><br><span class="line">            key = torch.cat(key_list, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.use_cache_quantization:</span><br><span class="line">        key = quantize_cache_v(key.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">                                    bits=<span class="number">8</span>,</span><br><span class="line">                                    qmin=self.cache_qmin,</span><br><span class="line">                                    qmax=self.cache_qmax)</span><br><span class="line">        value = quantize_cache_v(value.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">                                        bits=<span class="number">8</span>,</span><br><span class="line">                                        qmin=self.cache_qmin,</span><br><span class="line">                                        qmax=self.cache_qmax)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        past_key, past_value = layer_past[<span class="number">0</span>], layer_past[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.use_cache_quantization:</span><br><span class="line">            <span class="comment"># use_cache_quantization:</span></span><br><span class="line">            <span class="comment"># present=((q_key,key_scale,key_zero_point),</span></span><br><span class="line">            <span class="comment">#          (q_value,value_scale,value_zero_point))</span></span><br><span class="line">            key = (torch.cat((past_key[<span class="number">0</span>], key[<span class="number">0</span>]), dim=<span class="number">2</span>),</span><br><span class="line">                    torch.cat((past_key[<span class="number">1</span>], key[<span class="number">1</span>]), dim=<span class="number">2</span>),</span><br><span class="line">                    torch.cat((past_key[<span class="number">2</span>], key[<span class="number">2</span>]), dim=<span class="number">2</span>))</span><br><span class="line">            value = (torch.cat((past_value[<span class="number">0</span>], value[<span class="number">0</span>]), dim=<span class="number">2</span>),</span><br><span class="line">                        torch.cat((past_value[<span class="number">1</span>], value[<span class="number">1</span>]), dim=<span class="number">2</span>),</span><br><span class="line">                        torch.cat((past_value[<span class="number">2</span>], value[<span class="number">2</span>]), dim=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># not use_cache_quantization:</span></span><br><span class="line">            <span class="comment"># present=(key,value)</span></span><br><span class="line">            key = torch.cat((past_key, key), dim=<span class="number">1</span>)</span><br><span class="line">            value = torch.cat((past_value, value), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_cache:</span><br><span class="line">        present = (key, value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        present = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    key_size = key[<span class="number">0</span>].size(<span class="number">2</span>) <span class="keyword">if</span> self.use_cache_quantization <span class="keyword">else</span> key.size(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> key_size &gt; self.seq_length <span class="keyword">and</span> self.use_logn_attn <span class="keyword">and</span> <span class="keyword">not</span> self.training:</span><br><span class="line">        <span class="keyword">if</span> self.use_cache_quantization:</span><br><span class="line">            seq_start = key[<span class="number">0</span>].size(<span class="number">2</span>) - query.size(<span class="number">1</span>)</span><br><span class="line">            seq_end = key[<span class="number">0</span>].size(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            seq_start = key.size(<span class="number">1</span>) - query.size(<span class="number">1</span>)</span><br><span class="line">            seq_end = key.size(<span class="number">1</span>)</span><br><span class="line">        logn_tensor = self.logn_tensor[:, seq_start:seq_end, :, :].type_as(query)</span><br><span class="line">        query = query * logn_tensor.expand_as(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (</span><br><span class="line">        self.use_flash_attn</span><br><span class="line">        <span class="keyword">and</span> flash_attn_unpadded_func <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">not</span> self.is_fp32</span><br><span class="line">        <span class="keyword">and</span> query.is_cuda</span><br><span class="line">    ):</span><br><span class="line">        q, k, v = query, key, value</span><br><span class="line">        attn_output = self.core_attention_flash(q, k, v, attention_mask=attention_mask)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        key_size = key[<span class="number">0</span>].size(<span class="number">2</span>) <span class="keyword">if</span> self.use_cache_quantization <span class="keyword">else</span> key.size(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> query.size(<span class="number">1</span>) == key_size:</span><br><span class="line">            causal_mask = torch.tril(</span><br><span class="line">                torch.ones((key_size, key_size), dtype=torch.<span class="built_in">bool</span>, device=query.device)</span><br><span class="line">            ).view(<span class="number">1</span>, <span class="number">1</span>, key_size, key_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            causal_mask = <span class="literal">None</span></span><br><span class="line">        query = query.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.use_cache_quantization:</span><br><span class="line">            key = key.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">            value = value.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            causal_mask <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> self.use_flash_attn</span><br><span class="line">            <span class="keyword">and</span> flash_attn_unpadded_func <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> <span class="keyword">not</span> self.is_fp32</span><br><span class="line">            <span class="keyword">and</span> <span class="keyword">not</span> query.is_cuda</span><br><span class="line">        ):</span><br><span class="line">            <span class="keyword">raise</span> Exception(_ERROR_INPUT_CPU_QUERY_WITH_FLASH_ATTN_ACTIVATED)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.use_cache_quantization <span class="keyword">and</span> SUPPORT_TORCH2:</span><br><span class="line">            <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                attention_mask = attention_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, query.size(<span class="number">2</span>), -<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> causal_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    attention_mask = attention_mask.masked_fill(~causal_mask, torch.finfo(query.dtype).<span class="built_in">min</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                attention_mask = causal_mask</span><br><span class="line">            attn_output = F.scaled_dot_product_attention(</span><br><span class="line">                query, key, value, attn_mask=attention_mask</span><br><span class="line">            ).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            attn_weight = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_weight = self._attn(</span><br><span class="line">                query, key, value, causal_mask, attention_mask, head_mask</span><br><span class="line">            )</span><br><span class="line">    context_layer = self._merge_heads(</span><br><span class="line">        attn_output, self.num_heads, self.head_dim</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># print(&#x27;attn_output before c_proj.shape:&#123;&#125;&#x27;.format(attn_output.shape))</span></span><br><span class="line">    attn_output = self.c_proj(context_layer)</span><br><span class="line">    <span class="comment"># print(&#x27;attn_output.shape:&#123;&#125;&#x27;.format(attn_output.shape))</span></span><br><span class="line"></span><br><span class="line">    outputs = (attn_output, present)</span><br><span class="line">    <span class="keyword">if</span> output_attentions:</span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            self.use_flash_attn</span><br><span class="line">            <span class="keyword">and</span> flash_attn_unpadded_func <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">and</span> <span class="keyword">not</span> self.is_fp32</span><br><span class="line">        ):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Cannot output attentions while using flash-attn&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> self.use_cache_quantization <span class="keyword">and</span> SUPPORT_TORCH2:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Cannot output attentions while using scaled_dot_product_attention&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs += (attn_weight,)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">enable_qwen_pos_shift_attention</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> <span class="built_in">reversed</span>(model._modules.items()):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>(module.children())) &gt; <span class="number">0</span>:</span><br><span class="line">            enable_llama_pos_shift_attention(</span><br><span class="line">                module,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, LlamaAttention):</span><br><span class="line">            model._modules[name].forward = types.MethodType(</span><br><span class="line">                llama_pos_shift_attention_forward, model._modules[name]</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>

<h1 id="5-16"><a href="#5-16" class="headerlink" title="5.16"></a>5.16</h1><p>今天继续尝试将streaming-llm适配qwen1，但是进展不理想，具体内容如上面的代码注释所示。问题出现在streaming-llm的核心代码，其中使用的基于cache的RoPE在qwen中应用难度较大。因为qwenattention的forward中没用position_id这个参数，于是去参考另一个modify_falcon.py文件，因为这个的model文件也是没有position id的。但是，结合其modify文件和model文件之后，没发现有很明显的应用基于cache的RoPE做法。仅仅是基于past len做了一些调整。<br>或许，modeling_qwen的修改，在attention部分，应该也是像falcon中进行past_len这个参数的设置和应用。然后，对于kv_cache.py这个文件直接使用，然后再改写chat方法，基于改写后的chat方面写cli_demo.py。正如之前所说，这个流程应该是一个综合的过程。或许qwen1里面只要应用kv_cache.py对它这个模型的kv_cache进行修改后，自动RoPE就是水到渠成的基于cache了。</p>
<blockquote>
<ul>
<li><strong>明日任务：应用今天的思路，修改qwen</strong></li>
</ul>
</blockquote>
<h1 id="5-17"><a href="#5-17" class="headerlink" title="5.17"></a>5.17</h1><p>把RoPE中cur_len的部分，参照modify_falcon.py中，修改成了past_len。但是应该最好还是qwen1.5修改为佳。<br>这个项目还有一个问题就是greedy decoding，没有温度、随机性啥的。  </p>
<p>无奈了，终究不能搞长文本了。领导要看到效果，立刻转战多路召回检索优化。本以为逐渐稳定步入正轨，终究是一厢情愿。人在江湖，身不由己！  </p>
<p>看了项目已经写好的基于llamaindex + fastapi的代码，但是因为没有readme，代码也没用注释，所以看的一头雾水。  </p>
<blockquote>
<ul>
<li><strong>下周任务：人在江湖噜~~研究多路召回的实现，从读懂代码开始！没开发规范真是烦人！草台班子说是！</strong></li>
</ul>
</blockquote>
<h1 id="5-18-5-19"><a href="#5-18-5-19" class="headerlink" title="5.18 - 5.19"></a>5.18 - 5.19</h1><p>周六，本来和ZZK越好下午三点半在北京动物园碰面，结果我一点睡午觉睡过了，还是他打电话过来我才醒。急急忙忙地赶过去，已经四点半了。他去买了饮料，然后我们刷票进动物园。<br>进熊猫馆的时候，发现ZZK买的是不带熊猫馆的票，结果他又买了一份，然后尝试退之前的票，失败；尝试退刚买的票，失败；因为两张票都已经被实际上使用过了🤣🤣，铸币吧。<br>下午，这个时间动物们都没什么精神，熊猫趴在那儿睡觉，看不见正脸。有好长的队在排一个好像是网红熊猫的馆，我们嫌太长就没凑热闹。幸运的是，在熊猫馆里面逛了一阵子，正好遇见一个大熊猫打盹结束出来逛逛，拍下来了。<br>之后，在动物园里面其他地方逛，边逛边和ZZK聊天。看了鹦鹉、狼、羚羊、北极熊啥的。都是懒趴趴的。<br>走到大概六点多，逛累了，于是我们走去德凯mall吃一家绿茶餐厅，好像是杭州菜😅，真是明知美食荒漠，偏向荒漠行呀。点了荔枝虾球和抹茶饼两个甜品，可惜这个虾球很一般，不是我记忆力高三暑假和ZZC一起去杭州在新周记吃的那种。招牌菜一个糖醋口的排骨，难吃😅；烤鸡、烤肉片、竹笋这三道菜中规中矩，但这tm好像也不是杭州菜，无语子……<br>铸币ZZK一开始还想点西湖醋鱼，幸好给我拦住了。<br>之后，在商场-1层的超市逛了逛，然后准备回去了。在luckyin买了两杯咖啡，中途聊到WXF，和他打电话，约了下周俺们三一起去爬八达岭长城噜。<br>晚上到出租屋，和LSH、LT、SH打游戏，十点多的时候把咖啡喝完了。结果，晚上，不知道是热的还是因为咖啡，睡不着，开空调舒服点，但是也睡不着。迷迷糊糊到三点多似乎才睡着，结果六点多好像就醒了。捏妈的，一上午都状态不太对，还好不上班！<br>周日，混。改了简历，投了TX实习。晚上和LSH、SH、LT、NZY、WS打大乱斗。感觉得控制一下自己的脾气的，唉，严于律人宽以待己。<br>感觉没有什么外卖点了，附近的外卖没有想吃的。不知道什么情况，提不起劲，愁啊！  、</p>
<h1 id="5-20"><a href="#5-20" class="headerlink" title="5.20"></a>5.20</h1><p>早上看昨天投的结果，秒挂了尼玛的，简历秒挂！<br>继续看项目代码，似乎有点明白结构和用法了。<br>下午发现被tx捞了，捏妈明天面试。  </p>
<p>似乎懂了一点这个项目的结构。llamaindex似乎可以直接导入预先构建的索引，然后设置为model chat的参数，就可以实现rag。那么，我就要在这个index的生成上做手脚？具体的明天再说吧。  </p>
<p>晚上他们打游戏，我没去。看leetcode上的hot 100中的最大正方形面积，写了一种解法，结果和之前的一样，都只打败了5%的人。看来要看题解了，唉……<br>给SH开了个腾讯会议，讲了一下LLM的知识和细节，给他产品经理AI方向的补一下盲。</p>
<blockquote>
<ul>
<li><strong>明日任务：多路召回</strong></li>
</ul>
</blockquote>
<h1 id="5-21"><a href="#5-21" class="headerlink" title="5.21"></a>5.21</h1><p>测试项目里面fastapi的功能，发现这个吊项目基本上跑不通，document的向量化以及基于知识库的chat都nm跑不通。问开发的，也支支吾吾的，草！这还让我加班这周把这多路召回跑通呢？这不纯坑人呢嘛！<br>下午有面试，中午回出租屋，发现门吸到了，安上之后，似乎有点效果，但比较麻烦；美美睡了一觉。<br>2.30面试，半小时结束，面试官摄像头都没开。让我写二叉树先序遍历，捏妈的非递归形式没写出来，还是得刷题！感觉是kpi面，草他妈滴！<br>面完，吃了个沙县小吃鸭腿饭，然后回班上干活。  </p>
<p>全是坑啊兄弟们！没有开发规范，没有产品文档，我真是操了！  </p>
<p>一个现在才明白的小知识:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># a/b/c.py中</span></span><br><span class="line"><span class="comment"># .代表b目录，可以./xxx可以访问b目录下所有文件；from . import是从b目录引入</span></span><br><span class="line"><span class="comment"># ..代表a目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># power_system_aiqa-develop/server/configs/db_config.py中</span></span><br><span class="line">local_embedding_model_path = <span class="string">&quot;../embed_models/m3e&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果在server下的文件里面，运行程序，间接调用了这个config，无论嵌套多少回，都会以pwd = server进行上级目录寻址和返回；也就是说，这个情况下则会访问power_system_aiqa-develop/embed_models/m3e </span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><strong>明日任务：多路召回</strong></li>
</ul>
</blockquote>
<h1 id="5-22-5-23"><a href="#5-22-5-23" class="headerlink" title="5.22 - 5.23"></a>5.22 - 5.23</h1><p>没啥好说的，改那个项目的多路召回部分。加班改。发现没跑通的问题在于，似乎是官方实现的代码有缺陷，chromaDB转化为vector store的时候转化不完全&#x2F;没法正确转化，因而进行检索的时候是空集。<br>只能写死了，指定文档库的文件夹，然后直接从文档转化为vector index然后进行后续操作。<br>实际上，更精细的操作，还需要直接从document加载成node，然后像是summary index、graph index都可以从这个node创建。这样，就可以构建不能由vector index直接转化的index。后续，可以在chat里面加载各种index，构建多样化的retriever，然后整合起来。  </p>
<blockquote>
<ul>
<li><strong>明日任务：多路召回</strong></li>
</ul>
</blockquote>
<h1 id="5-24"><a href="#5-24" class="headerlink" title="5.24"></a>5.24</h1><p>早上改retriever，之前其他类型的index to retriever的部分没有跑通，可能是不兼容或者数据格式不对？<br>开了个会。发现在国企里面开发只需要做应声虫就可以了，提的意见和想法人家都当没听到。纯按照先入为主的理解在聊、派任务。反正我实习生不背锅，你咋说我咋做呗！<br>反正是准备跑路了，这边本来说的卡现在也没了，科研也不让做了，做开发还nm没人带，没有培养机制，还nm催进度。我的评价是草台班子一拖四。  </p>
<p>回顾这段时间，本来刚来的时候和沈老师那边会开会，也有一些指导，但是渐渐会也不开了。本来说沈老师那边的学生们能一起聊一聊，结果那边的学生也就开了一两次会，就再也没来。由于距离遥远，感觉沟通的时候也很费劲。总而言之，刚开始的时候还兢兢业业，慢慢发现这边是个草台班子、研发队伍和管理机制混乱、缺陷较大。虽然学到了点东西，但是感觉不多。需要一个新环境、正式的、有明确培养机制的环境。  </p>
<p><strong>预计六月初辞职，然后面面其他的吧唉。总归是一步慢、步步慢。理想情况下是一月份就开始实习，然后4月辞职或是请假什么的投暑期实习，然后中途休息一下、旅个游啥的，然后暑期实习回来投秋招。但是现在，无可奈何地晚了两个月，只能步步紧逼，却又总是赶不及……</strong><br><strong>回顾从高中毕业到现在的时间，因为懒惰、愚蠢而导致的信息差，错过了许多机会、有许多可以更好的选择没有做出。悲叹没有意义了，只能吃更多的苦……</strong>  </p>
<hr>
<p><img src="documentsummary.png" alt="docsummary" title="docsummary"> </p>
<p>向项目里面加入了document summary retriever，这个需要调用llm解析文档，因而慢，或许换成embedding会好一些。<br>换成embedding的写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">retriever = DocumentSummaryIndexEmbeddingRetriever(</span><br><span class="line">    doc_summary_index,</span><br><span class="line">    <span class="comment"># similarity_top_k=1,</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>但是没有测试，下周再搞！<br>llm解析时运行时报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ValueError: invalid literal <span class="keyword">for</span> int() with base 10: <span class="string">&#x27;Doc&#x27;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li><strong>下周任务：多路召回</strong></li>
</ul>
</blockquote>
<h1 id="5-25-5-26"><a href="#5-25-5-26" class="headerlink" title="5.25 - 5.26"></a>5.25 - 5.26</h1><p>前两天，想起ZZK的推荐，在美团的小象超市上买了康师傅方便面、一桶酸奶、一包王中王火腿、一个2L冰红茶。<br>周六，下雨，预计的长城之旅移到明天。在出租屋歇着，玩玩垃圾页游，中午点了鸡公煲，油油的，吃到一半，开了一包前两天在美团小象超市上买的康师傅方便面加了进去，吃撑了。下饭的片是《哆啦A梦：大雄的宇宙小战争》。下午和LT、SH、WS、NZY打了大乱斗。<br>晚上，还吃那个鸡公煲。这次感觉不太行了，也许是我终于吃腻了，下饭的片是《哆啦A梦：大雄与机器人王国》，但是只看了大概三分之一？<br>今天一天爽喝冰镇冰红茶，简直就是仙酿！<br>周日，早起，去北京北坐高铁到八达岭。与ZZK、WXF汇合。书包里面带了昨晚在小象超市买的三个三明治和一个面包，以及三瓶盐汽水，一瓶大瓶怡宝。到站饿了先吃一个面包，在上长城的路上经过商区，看见有肯德基、蜜雪冰城……蜜雪冰城稍微贵一两块钱，神！但是饮料带够了，没买。昨晚还买了两板酸奶，但是忘带了，嘻嘻。<br>过了九曲十八弯的安检，又走了一段，才验票进入城墙。向上爬。或许是由于昨天下了一天的雨、亦或许是因为处于山林之间，长城上的北风很大，在城墙口吹的很凉爽，足以抵消烈日。城墙上人很多，慢慢地走、慢慢地爬。<br><img src="%E9%95%BF%E5%9F%8E.jpg" alt="长城" title="长城"><br>可惜的是，ZZK似乎因为低血糖，爬了一阵子就撑不住，发晕呕吐；也有可能是爬长城前刚吃东西所以运动导致胃部不适应？总之，在爬到的好汉石那里，大概是北四段？（总共是12段），我们必须下去了。花了80r一人买了下山的缆车、抑或是过山车的票？哐哐哐一顿冲，没几分钟就到出口了。在出口，看看了一下黑熊，这个出口正好是“八达岭黑熊乐园”。说是乐园，只有三四只小黑熊懒懒的趴在那里晒太阳。<br>坐上了回去的公交，把预定的两点多的高铁票退了。唉，这两天为了这长城的来回，推了三回票，-11r。<br>公交一个多小时后到了啥门，忘了，我也不想去瞅一眼，总而言之是在恭王府附近。在附近找了饭店，排了队，一点多才吃上饭，饭店名字好像是长亭酒馆。点了韭菜炒虾米，炒一个啥菇，这俩菜是家里的口味。还有牛杂煲、生菜、小酥肉，我的评价是团购经典套餐。本来点了羊肉串，结果上来发现nm是小串，而且nm还是酸的，给退了。<br>吃完，WXF去看看他的另一些同学。我和ZZK一起，沿着后海逛，逛到了地铁站，聊聊天，蛮惬意的，最后分道扬镳，大概四点多，我回到了出租屋。<br>晚上要和ZYR、LF一起吃饭，所以六点开始睡觉补一补。本来预计的是九点吃海底捞，但是LF七点多就能回来，不是他之前说的大概九点，因而又变回涮肉。八点出头，我们就在西直门的聚宝源汇合了。吃涮肉的过程中，聊了聊各自的经历、心态什么的，也聊了聊八卦，还有一些不能言说的东西。吃完饭已经是十点多了，我们骑着自行车，去人民大学站的地铁站。欢乐的时光总是短暂的，唉！<br>11点多，回到了出租屋。明天又要上班了唉！  </p>
<h1 id="5-27"><a href="#5-27" class="headerlink" title="5.27"></a>5.27</h1><p>加拿大的后端实习生入职了。<br>在本地上测试了llamaindex中其他类型的index加入到多路召回里面，但是由于调用llm消耗过大、似乎与非llm的检索不能整合等原因，总之是整合失败了。<br>看官方文档的时候，突然看到，用一个index去生成不同的retriever，打开了另一个思路。于是基于vector index，又加了两个retriever。至于更多的，不是不想加，而是都需要llm或者之类的，总之是定制化支持程度不高，无法实现或是实现的价值不大。但是无论如何，多路召回的名头总算不是虚名了，目前是四路召回。<br>其中，三路召回是和embeddding模型深度绑定的，后面优化应该就是调整embedding。还有一路deep memory，似乎自己支持基于数据的训练。无论如何，再优化的话需要搞好训练数据集。<br>目前，这个多路召回还是本地，和知识库的联动没做好；因而，后端这部分的业务，还有需要做的就是把这个api整体上跑通。预计明天还是做这个东西。<br>看了leader和teacher分享的RAG方面的推文与项目，理解更深了。</p>
<blockquote>
<ul>
<li><strong>明日任务：阅读RAG相关论文&#x2F;后端api实现与修改</strong></li>
</ul>
</blockquote>
<h1 id="5-28"><a href="#5-28" class="headerlink" title="5.28"></a>5.28</h1><p>继续测试本地多路召回的效果，基本已经定型了。<br>晚上开会说，差不多可以推进在线的了。也就是说要调通api<br>刷了两道leetcode </p>
<blockquote>
<ul>
<li><strong>明日任务：调整在线api</strong></li>
</ul>
</blockquote>
<h1 id="5-29"><a href="#5-29" class="headerlink" title="5.29"></a>5.29</h1><p>记录一下查看端口进程的命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看端口进程</span></span><br><span class="line">netstat -anp | grep 端口号</span><br><span class="line"><span class="comment"># or  </span></span><br><span class="line">netstat -tnlp | grep 端口号</span><br></pre></td></tr></table></figure>

<p>尝试修复在线多路召回的API，正如之前定位的那样，问题代码应该出在这两行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chroma_collection = chromadb_client.get_collection(kb_id)</span><br><span class="line">vector_store = ChromaVectorStore(collection_name=kb_name, collection=chroma_collection, host = <span class="string">&#x27;localhost&#x27;</span>, port = <span class="number">8000</span>)</span><br></pre></td></tr></table></figure>
<p>collection显示doc是存在的，但是dict最后两个key显示的是none：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;data&#x27;</span>: None, <span class="string">&#x27;uris&#x27;</span>: None</span><br></pre></td></tr></table></figure>
<p>vectorstore读取后，显示collection为空。<br>又去GitHub issue上看了一下，终于明白了解决思路：首先，这个问题其实不是vector store的问题，而是bm25计算的问题。在llama index中，bm25计算需要的是nodes，也只能通过nodes进行bm25的构建，并没有适配从vector store转化到bm25的api，这是源码本身的缺陷。<br>那么，解决思路也就很明确了：要么自己去改源码，适配这个转化；要么就搞一个返回document的函数，然后在bm25创建前面读取这个返回的document，再构建nodes传入。出于方便和稳定性考虑，还是采取后者，当然这种做法会浪费一些时间，因为预先构建的切片不能使用，还要再构建一遍。<br>修改之后，api可以跑通了。但是时间花费很大，不知道是不是因为gpu资源不足的原因。可以看到，生成query、SYNTHESIZE、LLM三步花费时间巨多。<br><img src="%E6%97%B6%E9%97%B4%E8%8A%B1%E8%B4%B9.png" alt="时间花费" title="时间花费"><br>比较奇怪的是，在之前的步骤里面，直接从本地文档读取的话，反而不慢。明明这修改后的代码只是额外生成了一次bm25，但是时间却增加了好多。难道是因为后续一系列数值要重新计算、结构要重新生成？<br>看来后续有很大的优化空间；此外，很有意思的一点是，这个用于bm25的、由document转化而来的nodes，其数据结构和chroma直接读取的collection似乎很相似，前者似乎是List[Dict{}]，而后者似乎是Dict{List[]}；没有细看，但或许可以写一个转化函数，直接转化成nodes的数据格式导入bm25计算。  </p>
<p>晚上，和SH、LT打大乱斗捏。  </p>
<blockquote>
<ul>
<li><strong>明日任务：似乎要配合前端来展示项目总体的效果。</strong></li>
</ul>
</blockquote>
<h1 id="5-30"><a href="#5-30" class="headerlink" title="5.30"></a>5.30</h1><p>昨天的测试对话结果巨慢，但是今天，同样的东西，结果反而快了很多，直接变成了昨天测试的十几分之一？<br>后端对接了之前的前端，做了测试。发现这个前端基本都是写死的，对话、设置啥的没法调整，其实无法测试目前修改的效果。但是从fastapi和命令行结果来看，应该是没有问题的。<br>api里面document部分啥的还需要修改一下。今天修正了doc_index_create这个api。<br>下班前开会</p>
<p>下午，叶man阿姨突然打电话来，说周末可以吃饭。我一开始看到河北廊坊的号码还在好奇呢，以为是诈骗电话啥的都没想接，犹豫一下还是接了，没想到是叶阿姨！叶阿姨是我幼儿园时候，在俺爹开的小公司上过班的姐姐，后面虽然走了，但是和我妈妈关系很好，她们一直有联系。我也是大概大三？或是什么时候才加上她，但是刚加上的时候说了几句话，后面一直没联系，没啥说的感觉。这次趁我在北京这个机会，吃个饭，这很好！家里喝的茶叶似乎有不少都是叶阿姨给的。好久不见！也不知道如今能不能认出来，唉！我小时候对她的相貌记忆是一点也没有了！她的女儿在百京海淀区上学，马上高考了！我去！京✌！  </p>
<p>刷了3道题，基本都是回溯。基本不会。基本看答案。需要加强。</p>
<blockquote>
<ul>
<li><strong>明日任务：我不到啊！看一步吧！</strong></li>
</ul>
</blockquote>
<h1 id="5-31"><a href="#5-31" class="headerlink" title="5.31"></a>5.31</h1><p>今天和后端实习生一起测试了各种api，逐个排查，修正了一些bug。五部分的api，有三个部分都可以跑通了。<br>项目里面有一个小bad case就是，把id和name混淆了，函数名称啥的没改，在函数内部用查询把name转化成了id。<br>测试前端对话能否调用后端，主要是对话api。其他的前端内容基本上都是假的。对话有四类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CHAT_TYPE = [<span class="string">&quot;RAG_CHAT&quot;</span>,<span class="string">&quot;RAG_SEARCH&quot;</span>,<span class="string">&quot;LLM_CHAT&quot;</span>,<span class="string">&quot;RAG_RERANK&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>前两个对话好像是一样的返回结果。本质上这俩对话没啥区别？<br>第三、四个对话，结果和前两个不一样。快下班的时候总算是跑通了。之前是ollama 显存不够所以没跑通。</p>
<blockquote>
<ul>
<li><strong>下周任务：优质解答——我不知道</strong></li>
</ul>
</blockquote>
<h1 id="五月结束了！到北京来已经两个多月了！回首过去，是一段不太美好，但应该还是比较有价值的经历……？"><a href="#五月结束了！到北京来已经两个多月了！回首过去，是一段不太美好，但应该还是比较有价值的经历……？" class="headerlink" title="五月结束了！到北京来已经两个多月了！回首过去，是一段不太美好，但应该还是比较有价值的经历……？"></a>五月结束了！到北京来已经两个多月了！回首过去，是一段不太美好，但应该还是比较有价值的经历……？</h1>]]></content>
      <categories>
        <category>记录</category>
        <category>大模型</category>
        <category>实习</category>
      </categories>
      <tags>
        <tag>日记</tag>
        <tag>随想</tag>
        <tag>心情</tag>
      </tags>
  </entry>
</search>
